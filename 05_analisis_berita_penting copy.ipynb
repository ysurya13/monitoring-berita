{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9252cf8",
   "metadata": {},
   "source": [
    "# SETTING ENVIRONMENT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52389a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# mount the colab with google drive\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# mount the colab with google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "771e26a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set folder tempat kerja (current working directory)\n",
    "import os\n",
    "cwd = \"/Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Pekerjaan BMN/05. 2025/98_monitoring_berita/monitoring-berita\"\n",
    "#cwd = '/content/drive/MyDrive/Monitoring Berita'\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4561090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Keys - diganti dengan konfigurasi lengkap di cell selanjutnya\n",
    "DEEPSEEK_API_KEY = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75e8bd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 07:13:37,120 - INFO - ‚úÖ AI Configuration loaded: DEEPSEEK\n",
      "2025-10-02 07:13:37,121 - INFO -    Model: deepseek-chat\n",
      "2025-10-02 07:13:37,121 - INFO -    Temperature: 0.1\n",
      "2025-10-02 07:13:37,122 - INFO -    Max Tokens: 5000\n",
      "2025-10-02 07:13:37,121 - INFO -    Model: deepseek-chat\n",
      "2025-10-02 07:13:37,121 - INFO -    Temperature: 0.1\n",
      "2025-10-02 07:13:37,122 - INFO -    Max Tokens: 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== KONFIGURASI AI MODEL ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 07:13:37,464 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:13:38,257 - INFO - ‚úÖ DEEPSEEK API connection successful\n",
      "2025-10-02 07:13:38,257 - INFO - ‚úÖ DEEPSEEK API connection successful\n",
      "2025-10-02 07:13:38,421 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:13:38,421 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ AI Model: DEEPSEEK (deepseek-chat)\n",
      "üîß Temperature: 0.1\n",
      "üìù Max Tokens: 5000\n",
      "üîë API Key: ********************...8d74\n",
      "\n",
      "üß™ Testing AI connection...\n",
      "‚úÖ Test Response: AI Ready!\n",
      "\n",
      "‚úÖ AI configuration completed successfully!\n",
      "üí° Gunakan CALL_AI_MODEL(prompt) untuk memanggil AI di sel lain.\n",
      "\n",
      "üìã Environment Variables yang diset:\n",
      "   MODEL_ANALISIS = deepseek\n",
      "   AI_MODEL_NAME = deepseek-chat\n",
      "   DUMMY_MODE = 0\n",
      "‚úÖ Test Response: AI Ready!\n",
      "\n",
      "‚úÖ AI configuration completed successfully!\n",
      "üí° Gunakan CALL_AI_MODEL(prompt) untuk memanggil AI di sel lain.\n",
      "\n",
      "üìã Environment Variables yang diset:\n",
      "   MODEL_ANALISIS = deepseek\n",
      "   AI_MODEL_NAME = deepseek-chat\n",
      "   DUMMY_MODE = 0\n"
     ]
    }
   ],
   "source": [
    "# Membaca AI model yang dipilih user melalui config.json ['AI_name']\n",
    "# Kemudian mengaplikasikan pilihan tersebut (openai/deepseek) ke seluruh sel utama\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_ai_configuration():\n",
    "    \"\"\"\n",
    "    Memuat konfigurasi AI model dari config.json dan setup environment\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary berisi konfigurasi AI yang telah diload\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Baca config.json\n",
    "        with open('config.json', 'r', encoding='utf-8') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        # Ambil AI configuration\n",
    "        ai_name = config.get('AI_name', 'openai').lower()\n",
    "        \n",
    "        # Validasi AI name\n",
    "        supported_ai = ['openai', 'deepseek']\n",
    "        if ai_name not in supported_ai:\n",
    "            logger.warning(f\"AI model '{ai_name}' tidak didukung. Menggunakan 'openai' sebagai default.\")\n",
    "            ai_name = 'openai'\n",
    "        \n",
    "        # Setup environment variables berdasarkan pilihan AI\n",
    "        if ai_name == 'openai':\n",
    "            api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "            if not api_key:\n",
    "                raise ValueError(\"OPENAI_API_KEY tidak ditemukan di environment variables\")\n",
    "            \n",
    "            ai_config = {\n",
    "                'provider': 'openai',\n",
    "                'model': config.get('openai_model', 'gpt-4o-mini'),\n",
    "                'api_key': api_key,\n",
    "                'base_url': None,\n",
    "                'temperature': config.get('temperature', 0.1),\n",
    "                'max_tokens': config.get('max_tokens', 5000)\n",
    "            }\n",
    "            \n",
    "        elif ai_name == 'deepseek':\n",
    "            api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "            if not api_key:\n",
    "                raise ValueError(\"DEEPSEEK_API_KEY tidak ditemukan di environment variables\")\n",
    "                \n",
    "            ai_config = {\n",
    "                'provider': 'deepseek',\n",
    "                'model': config.get('deepseek_model', 'deepseek-chat'),\n",
    "                'api_key': api_key,\n",
    "                'base_url': 'https://api.deepseek.com/v1',\n",
    "                'temperature': config.get('temperature', 0.1),\n",
    "                'max_tokens': config.get('max_tokens', 5000)\n",
    "            }\n",
    "        \n",
    "        # Set global environment variable untuk digunakan di sel lain\n",
    "        os.environ['MODEL_ANALISIS'] = ai_name\n",
    "        os.environ['AI_MODEL_NAME'] = ai_config['model']\n",
    "        os.environ['AI_TEMPERATURE'] = str(ai_config['temperature'])\n",
    "        os.environ['AI_MAX_TOKENS'] = str(ai_config['max_tokens'])\n",
    "        \n",
    "        if ai_config['base_url']:\n",
    "            os.environ['AI_BASE_URL'] = ai_config['base_url']\n",
    "        \n",
    "        logger.info(f\"‚úÖ AI Configuration loaded: {ai_name.upper()}\")\n",
    "        logger.info(f\"   Model: {ai_config['model']}\")\n",
    "        logger.info(f\"   Temperature: {ai_config['temperature']}\")\n",
    "        logger.info(f\"   Max Tokens: {ai_config['max_tokens']}\")\n",
    "        \n",
    "        return ai_config\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        logger.error(\"‚ùå File config.json tidak ditemukan!\")\n",
    "        raise\n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"‚ùå Error parsing config.json: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error loading AI configuration: {e}\")\n",
    "        raise\n",
    "\n",
    "def setup_ai_client(ai_config):\n",
    "    \"\"\"\n",
    "    Setup AI client berdasarkan konfigurasi yang dipilih\n",
    "    \n",
    "    Args:\n",
    "        ai_config (dict): Konfigurasi AI dari load_ai_configuration()\n",
    "    \n",
    "    Returns:\n",
    "        object: AI client object (OpenAI atau Deepseek compatible)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if ai_config['provider'] == 'openai':\n",
    "            from openai import OpenAI\n",
    "            \n",
    "            client = OpenAI(\n",
    "                api_key=ai_config['api_key']\n",
    "            )\n",
    "            \n",
    "        elif ai_config['provider'] == 'deepseek':\n",
    "            from openai import OpenAI  # Deepseek menggunakan OpenAI compatible API\n",
    "            \n",
    "            client = OpenAI(\n",
    "                api_key=ai_config['api_key'],\n",
    "                base_url=ai_config['base_url']\n",
    "            )\n",
    "        \n",
    "        # Test connection dengan simple call\n",
    "        test_response = client.chat.completions.create(\n",
    "            model=ai_config['model'],\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Test connection. Respond with 'OK'.\"}],\n",
    "            max_tokens=10,\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        if test_response.choices[0].message.content:\n",
    "            logger.info(f\"‚úÖ {ai_config['provider'].upper()} API connection successful\")\n",
    "            return client\n",
    "        else:\n",
    "            raise Exception(\"API test failed - empty response\")\n",
    "            \n",
    "    except ImportError as e:\n",
    "        logger.error(f\"‚ùå Missing required library: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error setting up {ai_config['provider']} client: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_ai_call_function(ai_config, client):\n",
    "    \"\"\"\n",
    "    Mengembalikan function untuk memanggil AI yang sudah dikonfigurasi\n",
    "    \n",
    "    Args:\n",
    "        ai_config (dict): Konfigurasi AI\n",
    "        client (object): AI client object\n",
    "    \n",
    "    Returns:\n",
    "        function: Function untuk memanggil AI dengan parameter standar\n",
    "    \"\"\"\n",
    "    def call_ai_model(prompt, temperature=None, max_tokens=None):\n",
    "        \"\"\"\n",
    "        Function wrapper untuk memanggil AI model dengan konfigurasi yang sudah diset\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): Prompt untuk AI\n",
    "            temperature (float, optional): Temperature override\n",
    "            max_tokens (int, optional): Max tokens override\n",
    "            \n",
    "        Returns:\n",
    "            str: Response dari AI model\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=ai_config['model'],\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temperature or ai_config['temperature'],\n",
    "                max_tokens=max_tokens or ai_config['max_tokens']\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calling {ai_config['provider']} API: {e}\")\n",
    "            raise\n",
    "    \n",
    "    return call_ai_model\n",
    "\n",
    "# ===== EKSEKUSI KONFIGURASI AI ===== #\n",
    "print(\"=== KONFIGURASI AI MODEL ===\")\n",
    "\n",
    "try:\n",
    "    # Load AI configuration dari config.json\n",
    "    ai_config = load_ai_configuration()\n",
    "    \n",
    "    # Setup AI client\n",
    "    ai_client = setup_ai_client(ai_config)\n",
    "    \n",
    "    # Buat function wrapper untuk memanggil AI\n",
    "    call_ai_model = get_ai_call_function(ai_config, ai_client)\n",
    "    \n",
    "    # Set sebagai global variables untuk digunakan di sel lain\n",
    "    globals()['AI_CONFIG'] = ai_config\n",
    "    globals()['AI_CLIENT'] = ai_client \n",
    "    globals()['CALL_AI_MODEL'] = call_ai_model\n",
    "    \n",
    "    print(f\"üéØ AI Model: {ai_config['provider'].upper()} ({ai_config['model']})\")\n",
    "    print(f\"üîß Temperature: {ai_config['temperature']}\")\n",
    "    print(f\"üìù Max Tokens: {ai_config['max_tokens']}\")\n",
    "    print(f\"üîë API Key: {'*' * 20}...{ai_config['api_key'][-4:]}\")\n",
    "    \n",
    "    # Test simple call\n",
    "    print(f\"\\nüß™ Testing AI connection...\")\n",
    "    test_result = call_ai_model(\"Respond with 'AI Ready!'\", temperature=0, max_tokens=10)\n",
    "    print(f\"‚úÖ Test Response: {test_result}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ AI configuration completed successfully!\")\n",
    "    print(f\"üí° Gunakan CALL_AI_MODEL(prompt) untuk memanggil AI di sel lain.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error dalam setup AI configuration: {e}\")\n",
    "    print(f\"üîß Pastikan:\")\n",
    "    print(f\"   1. File config.json ada dan berisi 'AI_name': 'openai' atau 'deepseek'\")\n",
    "    print(f\"   2. Environment variable API key sudah diset (OPENAI_API_KEY atau DEEPSEEK_API_KEY)\")\n",
    "    print(f\"   3. Library openai sudah terinstall\")\n",
    "    \n",
    "    # Set fallback ke dummy mode\n",
    "    os.environ['DUMMY_MODE'] = '1'\n",
    "    globals()['AI_CONFIG'] = {'provider': 'dummy', 'model': 'dummy'}\n",
    "    globals()['CALL_AI_MODEL'] = lambda prompt, **kwargs: '{\"dummy\": \"response\"}'\n",
    "    print(f\"üîÑ Fallback ke DUMMY MODE untuk development\")\n",
    "\n",
    "print(f\"\\nüìã Environment Variables yang diset:\")\n",
    "print(f\"   MODEL_ANALISIS = {os.getenv('MODEL_ANALISIS', 'not set')}\")\n",
    "print(f\"   AI_MODEL_NAME = {os.getenv('AI_MODEL_NAME', 'not set')}\")\n",
    "print(f\"   DUMMY_MODE = {os.getenv('DUMMY_MODE', '0')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb7aa5d",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da78eb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-01 21:12:07,758 - INFO - Membaca file analisis AI: 00_hasil_analisis/seluruh_berita/analisis_ai_20251001_deepseek_default.csv\n",
      "2025-10-01 21:12:07,762 - INFO - Total berita: 44\n",
      "2025-10-01 21:12:07,762 - INFO - Berita penting (filtered): 40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berhasil memuat 40 berita penting\n",
      "\n",
      "Sample berita penting:\n",
      "                                         judul_berita topik_llm  importance  \\\n",
      "0   Purbaya Bakal Sikat Rokok Ilegal, Seberapa Par...  Kemenkeu        85.0   \n",
      "16  Dukung Kebijakan Menkeu Purbaya, Indef Singgun...  Kemenkeu        85.0   \n",
      "42  Ulama Jatim Dukung Menkeu Berantas Rokok Ilega...  Kemenkeu        85.0   \n",
      "39  Pengusaha Ungkap Kerugian Negara Akibat Rokok ...  Kemenkeu        85.0   \n",
      "37  Modus Pedagang Ecommerce, Rokok Ilegal Dijual ...  Kemenkeu        85.0   \n",
      "\n",
      "   sentimen  \n",
      "0   positif  \n",
      "16  positif  \n",
      "42  positif  \n",
      "39  positif  \n",
      "37  positif  \n"
     ]
    }
   ],
   "source": [
    "# Langkah pertama membaca file csv hasil analisis AI sebelumnya\n",
    "# file terletak di config.json \"analisis_ai_output\"\n",
    "# Filter out berita dengan topik_llm \"Lainnya\"\n",
    "# Filter out berita dengan importance < 50\n",
    "\n",
    "# CATATAN: SEL 1 akan otomatis mengupdate config.json dengan path berita penting terbaru\n",
    "# Fungsi helper tersedia: get_berita_penting_path_from_config() untuk membaca path fleksibel\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup logging untuk error handling\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_berita_penting():\n",
    "    \"\"\"\n",
    "    Memuat dan memfilter berita penting dari file hasil analisis AI\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame berisi berita yang sudah difilter\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Baca konfigurasi\n",
    "        with open('config.json', 'r', encoding='utf-8') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        # Path file analisis AI\n",
    "        analisis_file = config.get('analisis_ai_output')\n",
    "        if not analisis_file:\n",
    "            raise ValueError(\"analisis_ai_output tidak ditemukan dalam config.json\")\n",
    "        \n",
    "        # Periksa apakah file ada\n",
    "        if not Path(analisis_file).exists():\n",
    "            raise FileNotFoundError(f\"File analisis AI tidak ditemukan: {analisis_file}\")\n",
    "        \n",
    "        # Baca file CSV\n",
    "        logger.info(f\"Membaca file analisis AI: {analisis_file}\")\n",
    "        df = pd.read_csv(analisis_file)\n",
    "        \n",
    "        # Filter berita penting\n",
    "        # 1. Exclude topik_llm \"Lainnya\"\n",
    "        # 2. Include importance >= 70\n",
    "        df_filtered = df[\n",
    "            (df['topik_llm'] != 'Lainnya') & \n",
    "            (df['importance'] >= 70)\n",
    "        ].copy()\n",
    "        \n",
    "        logger.info(f\"Total berita: {len(df)}\")\n",
    "        logger.info(f\"Berita penting (filtered): {len(df_filtered)}\")\n",
    "        \n",
    "        if df_filtered.empty:\n",
    "            logger.warning(\"Tidak ada berita penting yang memenuhi kriteria!\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Urutkan berdasarkan importance (descending)\n",
    "        df_filtered = df_filtered.sort_values('importance', ascending=False)\n",
    "        \n",
    "        return df_filtered\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error dalam load_berita_penting: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Load data berita penting\n",
    "df_berita_penting = load_berita_penting()\n",
    "print(f\"Berhasil memuat {len(df_berita_penting)} berita penting\")\n",
    "if not df_berita_penting.empty:\n",
    "    print(\"\\nSample berita penting:\")\n",
    "    print(df_berita_penting[['judul_berita', 'topik_llm', 'importance', 'sentimen']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0fdf345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unkomen sel ini untuk testing.\n",
    "# df_berita_penting = df_berita_penting.sample(10, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5631a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kolom teks: artikel_berita_bersih, Kolom judul: judul_berita\n",
      "AI Model: DEEPSEEK (deepseek-chat)\n",
      "‚úÖ Using pre-configured DEEPSEEK client\n",
      "Starting analysis of 40 articles using DEEPSEEK...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]2025-10-02 07:27:17,776 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:27:17,785 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:27:17,786 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:27:17,776 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:27:17,785 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:27:17,786 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  5%|‚ñå         | 2/40 [00:09<02:24,  3.81s/it]2025-10-02 07:27:26,573 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:27:26,573 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:27:26,668 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:27:26,668 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "  8%|‚ñä         | 3/40 [00:09<01:21,  2.21s/it]2025-10-02 07:27:27,086 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:27:27,086 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 10%|‚ñà         | 4/40 [00:17<02:42,  4.52s/it]2025-10-02 07:27:35,068 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:27:35,068 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 12%|‚ñà‚ñé        | 5/40 [00:18<01:48,  3.11s/it]2025-10-02 07:27:35,655 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:27:35,655 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 15%|‚ñà‚ñå        | 6/40 [00:18<01:12,  2.12s/it]2025-10-02 07:27:35,871 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:27:35,871 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 18%|‚ñà‚ñä        | 7/40 [00:26<02:18,  4.19s/it]2025-10-02 07:27:44,340 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:27:44,340 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 20%|‚ñà‚ñà        | 8/40 [00:27<01:33,  2.93s/it]2025-10-02 07:27:44,522 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:27:44,522 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 22%|‚ñà‚ñà‚ñé       | 9/40 [00:27<01:04,  2.07s/it]2025-10-02 07:27:44,745 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:27:44,745 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 25%|‚ñà‚ñà‚ñå       | 10/40 [00:35<01:58,  3.96s/it]2025-10-02 07:27:52,979 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:27:52,979 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 28%|‚ñà‚ñà‚ñä       | 11/40 [00:36<01:26,  2.98s/it]2025-10-02 07:27:53,687 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:27:53,687 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 30%|‚ñà‚ñà‚ñà       | 12/40 [00:36<01:01,  2.20s/it]2025-10-02 07:27:54,091 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:27:54,091 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 32%|‚ñà‚ñà‚ñà‚ñé      | 13/40 [00:45<01:50,  4.08s/it]2025-10-02 07:28:02,497 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:28:02,497 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:28:02,497 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:28:02,497 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 38%|‚ñà‚ñà‚ñà‚ñä      | 15/40 [00:46<01:01,  2.45s/it]2025-10-02 07:28:03,645 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:28:03,645 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 16/40 [00:53<01:28,  3.67s/it]2025-10-02 07:28:10,977 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:28:10,977 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 42%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 17/40 [00:54<01:08,  2.99s/it]2025-10-02 07:28:12,037 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:28:12,037 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 18/40 [00:54<00:49,  2.27s/it]2025-10-02 07:28:12,402 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:28:12,402 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 19/40 [01:02<01:16,  3.65s/it]2025-10-02 07:28:19,588 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:28:19,588 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 20/40 [01:03<00:59,  2.96s/it]2025-10-02 07:28:20,808 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:28:20,808 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 21/40 [01:04<00:43,  2.31s/it]2025-10-02 07:28:21,539 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:28:21,539 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 22/40 [01:10<01:03,  3.52s/it]2025-10-02 07:28:27,987 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:28:27,987 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 23/40 [01:12<00:50,  2.99s/it]2025-10-02 07:28:29,709 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:28:29,709 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 24/40 [01:13<00:40,  2.52s/it]2025-10-02 07:28:31,086 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:28:31,086 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 25/40 [01:18<00:47,  3.19s/it]2025-10-02 07:28:35,849 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:28:35,849 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 26/40 [01:21<00:42,  3.03s/it]2025-10-02 07:28:38,637 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:28:38,637 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 27/40 [01:22<00:32,  2.52s/it]2025-10-02 07:28:39,897 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:28:39,897 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 28/40 [01:27<00:39,  3.26s/it]2025-10-02 07:28:44,893 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:28:44,893 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 29/40 [01:29<00:32,  2.93s/it]2025-10-02 07:28:47,043 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:28:47,043 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 30/40 [01:30<00:24,  2.40s/it]2025-10-02 07:28:48,180 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:28:48,180 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 31/40 [01:34<00:26,  2.94s/it]2025-10-02 07:28:52,412 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:28:52,412 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 32/40 [01:36<00:20,  2.61s/it]2025-10-02 07:28:54,220 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:28:54,220 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 33/40 [01:37<00:14,  2.04s/it]2025-10-02 07:28:54,941 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:28:54,941 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 34/40 [01:44<00:20,  3.42s/it]2025-10-02 07:29:01,602 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:29:01,602 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 35/40 [01:45<00:13,  2.67s/it]2025-10-02 07:29:02,508 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:29:02,508 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 36/40 [01:47<00:10,  2.58s/it]2025-10-02 07:29:04,904 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:29:04,904 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      " 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 37/40 [01:51<00:09,  3.09s/it]2025-10-02 07:29:09,148 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-10-02 07:29:09,148 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [02:00<00:00,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete: 40 articles in 120.9s\n",
      "Success: 40, Errors: 0\n",
      "AI Provider: DEEPSEEK\n",
      "\n",
      "Sample results (showing 3):\n",
      "1. Purbaya Bakal Sikat Rokok Ilegal, Seberapa Parah Peredaranny...\n",
      "   Resume: Menteri Keuangan Purbaya Yudhi Sadewa akan melakukan penindakan masif ...\n",
      "   Dampak: Positif\n",
      "2. Dukung Kebijakan Menkeu Purbaya, Indef Singgung Fenomena Rok...\n",
      "   Resume: Ekonom Indef Tauhid Ahmad mendukung keputusan Menkeu Purbaya yang tida...\n",
      "   Dampak: Positif\n",
      "3. Ulama Jatim Dukung Menkeu Berantas Rokok Ilegal - detikcom...\n",
      "   Resume: Gus Maksum, pengasuh Ponpes Langitan Tuban, mendukung Menkeu Purbaya m...\n",
      "   Dampak: Positif\n",
      "\n",
      "File saved: 00_hasil_analisis/berita_penting/analisis_berita_penting_deepseek_20251002_072918.csv\n",
      "üîç Debug path info:\n",
      "   Current CWD: /Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Pekerjaan BMN/05. 2025/98_monitoring_berita/monitoring-berita\n",
      "   File path: /Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Pekerjaan BMN/05. 2025/98_monitoring_berita/monitoring-berita/00_hasil_analisis/berita_penting/analisis_berita_penting_deepseek_20251002_072918.csv\n",
      "   Relative path: 00_hasil_analisis/berita_penting/analisis_berita_penting_deepseek_20251002_072918.csv\n",
      "üì¶ Config backup saved: config.json.bak\n",
      "üìù Config updated:\n",
      "   berita_penting_output: 00_hasil_analisis/berita_penting/analisis_berita_penting_deepseek_20251002_072918.csv\n",
      "   berita_penting_output_dir: 00_hasil_analisis/berita_penting\n",
      "‚úÖ Config.json berhasil diupdate dengan path berita penting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# SEL 1 - Analisis Berita Penting (Parallel) - Updated with Dynamic AI Configuration\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import Dict, Any, List\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    tqdm = lambda x, **_: x\n",
    "\n",
    "# Konfigurasi dari AI setup\n",
    "MAX_WORKERS = 3\n",
    "MODEL_PILIHAN = os.getenv(\"MODEL_ANALISIS\", \"openai\").lower()\n",
    "AI_MODEL_NAME = os.getenv(\"AI_MODEL_NAME\", \"gpt-4o-mini\")\n",
    "DUMMY_MODE = os.getenv(\"DUMMY_MODE\", \"0\") == \"1\"\n",
    "\n",
    "# Deteksi kolom\n",
    "CANDIDATE_TEXT_COLS = [\"isi_berita\", \"content\", \"artikel_berita_bersih\", \"isi\", \"full_text\", \"body\"]\n",
    "TEXT_COL = None\n",
    "for c in CANDIDATE_TEXT_COLS:\n",
    "    if c in df_berita_penting.columns:\n",
    "        TEXT_COL = c\n",
    "        break\n",
    "\n",
    "JUDUL_COL = 'judul_berita'\n",
    "print(f\"Kolom teks: {TEXT_COL}, Kolom judul: {JUDUL_COL}\")\n",
    "print(f\"AI Model: {MODEL_PILIHAN.upper()} ({AI_MODEL_NAME})\")\n",
    "\n",
    "# Setup AI client menggunakan konfigurasi global\n",
    "_ai_client = None\n",
    "_call_ai_model = None\n",
    "\n",
    "if not DUMMY_MODE:\n",
    "    try:\n",
    "        # Gunakan konfigurasi global yang sudah disetup di cell sebelumnya\n",
    "        if 'AI_CLIENT' in globals() and 'CALL_AI_MODEL' in globals():\n",
    "            _ai_client = AI_CLIENT\n",
    "            _call_ai_model = CALL_AI_MODEL\n",
    "            print(f\"‚úÖ Using pre-configured {MODEL_PILIHAN.upper()} client\")\n",
    "        else:\n",
    "            raise Exception(\"AI configuration not found. Please run AI configuration cell first.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error accessing AI client: {e}\")\n",
    "        print(\"üîÑ Switching to DUMMY mode\")\n",
    "        DUMMY_MODE = True\n",
    "\n",
    "def build_prompt(judul: str, isi: str) -> str:\n",
    "    return f\"\"\"Analisis berita ini dan buat JSON dengan format tepat:\n",
    "\n",
    "Judul: {judul[:200]}\n",
    "Isi: {isi[:3000]}\n",
    "\n",
    "Buat JSON dengan 4 field:\n",
    "- resume: ringkasan singkat (maks 60 kata)  \n",
    "- dampak_kemenkeu: Positif/Negatif/Netral (untuk Kementerian Keuangan)\n",
    "- alasan_dampak: alasan singkat (maks 40 kata)\n",
    "- hal_menarik: array 1-3 poin menarik\n",
    "\n",
    "Contoh format:\n",
    "{{\"resume\": \"Menteri melakukan sidak...\", \"dampak_kemenkeu\": \"Positif\", \"alasan_dampak\": \"Meningkatkan transparansi\", \"hal_menarik\": [\"Kunjungan mendadak\", \"Fokus kredit\"]}}\"\"\"\n",
    "\n",
    "def call_model(prompt: str) -> str:\n",
    "    if DUMMY_MODE:\n",
    "        return '{\"resume\": \"Dummy analisis berita\", \"dampak_kemenkeu\": \"Netral\", \"alasan_dampak\": \"Mode dummy testing\", \"hal_menarik\": [\"Test mode\", \"Dummy data\"]}'\n",
    "    \n",
    "    try:\n",
    "        # Gunakan wrapper function yang sudah dikonfigurasi\n",
    "        response = _call_ai_model(\n",
    "            prompt, \n",
    "            temperature=float(os.getenv('AI_TEMPERATURE', '0.2')),\n",
    "            max_tokens=int(os.getenv('AI_MAX_TOKENS', '400'))\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"API call failed: {str(e)}\")\n",
    "\n",
    "def parse_response(raw: str) -> Dict[str, Any]:\n",
    "    if not raw:\n",
    "        raise ValueError(\"Empty response\")\n",
    "    \n",
    "    # Extract JSON\n",
    "    json_match = re.search(r'\\{[^{}]*\"resume\"[^{}]*\\}', raw)\n",
    "    if json_match:\n",
    "        candidate = json_match.group()\n",
    "    else:\n",
    "        candidate = raw.strip()\n",
    "    \n",
    "    try:\n",
    "        data = json.loads(candidate)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"JSON parse error: {e}\")\n",
    "    \n",
    "    # Normalize\n",
    "    result = {\n",
    "        'resume': str(data.get('resume', '')),\n",
    "        'dampak_kemenkeu': str(data.get('dampak_kemenkeu', 'Netral')),\n",
    "        'alasan_dampak': str(data.get('alasan_dampak', '')),\n",
    "        'hal_menarik': data.get('hal_menarik', [])\n",
    "    }\n",
    "    \n",
    "    if isinstance(result['hal_menarik'], str):\n",
    "        result['hal_menarik'] = [result['hal_menarik']]\n",
    "    \n",
    "    # Standardize dampak\n",
    "    dk = result['dampak_kemenkeu'].lower()\n",
    "    if 'pos' in dk:\n",
    "        result['dampak_kemenkeu'] = 'Positif'\n",
    "    elif 'neg' in dk:\n",
    "        result['dampak_kemenkeu'] = 'Negatif'  \n",
    "    else:\n",
    "        result['dampak_kemenkeu'] = 'Netral'\n",
    "        \n",
    "    return result\n",
    "\n",
    "def analyze_row(idx: int, row: pd.Series) -> Dict[str, Any]:\n",
    "    try:\n",
    "        judul = str(row.get(JUDUL_COL, ''))[:300]\n",
    "        isi = str(row.get(TEXT_COL, ''))[:5000]\n",
    "        \n",
    "        # Clean text dari karakter bermasalah\n",
    "        judul = judul.encode('utf-8', 'ignore').decode('utf-8')\n",
    "        isi = isi.encode('utf-8', 'ignore').decode('utf-8')\n",
    "        \n",
    "        prompt = build_prompt(judul, isi)\n",
    "        raw = call_model(prompt)\n",
    "        parsed = parse_response(raw)\n",
    "        \n",
    "        parsed['__status'] = 'ok'\n",
    "        return parsed\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            '__status': 'error',\n",
    "            '__error': str(e)[:200]\n",
    "        }\n",
    "\n",
    "# Execute parallel analysis\n",
    "start_time = time.time()\n",
    "rows_df = df_berita_penting.reset_index(drop=True)\n",
    "results = [None] * len(rows_df)\n",
    "errors = 0\n",
    "\n",
    "print(f\"Starting analysis of {len(rows_df)} articles using {MODEL_PILIHAN.upper()}...\")\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = {executor.submit(analyze_row, i, row): i for i, row in rows_df.iterrows()}\n",
    "    \n",
    "    for fut in tqdm(as_completed(futures), total=len(futures)):\n",
    "        idx = futures[fut]\n",
    "        result = fut.result()\n",
    "        results[idx] = result\n",
    "        \n",
    "        if result.get('__status') != 'ok':\n",
    "            errors += 1\n",
    "\n",
    "# Compile results\n",
    "col_resume = []\n",
    "col_dampak = []\n",
    "col_alasan = []  \n",
    "col_hal = []\n",
    "col_status = []\n",
    "col_error = []\n",
    "\n",
    "for res in results:\n",
    "    if res and res.get('__status') == 'ok':\n",
    "        col_resume.append(res.get('resume', ''))\n",
    "        col_dampak.append(res.get('dampak_kemenkeu', ''))\n",
    "        col_alasan.append(res.get('alasan_dampak', ''))\n",
    "        col_hal.append(' | '.join(res.get('hal_menarik', [])))\n",
    "        col_status.append('ok')\n",
    "        col_error.append('')\n",
    "    else:\n",
    "        col_resume.append('')\n",
    "        col_dampak.append('')\n",
    "        col_alasan.append('')\n",
    "        col_hal.append('')\n",
    "        col_status.append('error')\n",
    "        col_error.append(res.get('__error', 'unknown') if res else 'unknown')\n",
    "\n",
    "# Add results to dataframe\n",
    "df_out = rows_df.copy()\n",
    "df_out['resume_ai'] = col_resume\n",
    "df_out['dampak_kemenkeu_ai'] = col_dampak  \n",
    "df_out['alasan_dampak_ai'] = col_alasan\n",
    "df_out['hal_menarik_ai'] = col_hal\n",
    "df_out['analisis_status'] = col_status\n",
    "df_out['analisis_error'] = col_error\n",
    "\n",
    "proc_time = time.time() - start_time\n",
    "success_count = len(df_out) - errors\n",
    "\n",
    "print(f\"Analysis complete: {len(df_out)} articles in {proc_time:.1f}s\")\n",
    "print(f\"Success: {success_count}, Errors: {errors}\")\n",
    "print(f\"AI Provider: {MODEL_PILIHAN.upper()}\")\n",
    "\n",
    "# Show successful samples\n",
    "success_rows = df_out[df_out['analisis_status'] == 'ok']\n",
    "if not success_rows.empty:\n",
    "    print(f\"\\nSample results (showing {min(3, len(success_rows))}):\")\n",
    "    for i, (_, row) in enumerate(success_rows.head(3).iterrows()):\n",
    "        print(f\"{i+1}. {row['judul_berita'][:60]}...\")\n",
    "        print(f\"   Resume: {row['resume_ai'][:70]}...\")\n",
    "        print(f\"   Dampak: {row['dampak_kemenkeu_ai']}\")\n",
    "\n",
    "# Save results  \n",
    "out_dir = Path('00_hasil_analisis/berita_penting')\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "outfile = out_dir / f\"analisis_berita_penting_{MODEL_PILIHAN}_{timestamp}.csv\"\n",
    "df_out.to_csv(outfile, index=False)\n",
    "print(f\"\\nFile saved: {outfile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f8abb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update config.json dengan path berita penting yang fleksibel\n",
    "def update_config_with_berita_penting_path(file_path):\n",
    "    \"\"\"\n",
    "    Update config.json dengan path berita penting yang fleksibel\n",
    "    Menggunakan path relatif agar tetap bekerja jika cwd berubah\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Baca config yang sudah ada\n",
    "        config_file = Path('config.json')\n",
    "        if config_file.exists():\n",
    "            with open(config_file, 'r', encoding='utf-8') as f:\n",
    "                config = json.load(f)\n",
    "        else:\n",
    "            config = {}\n",
    "        \n",
    "        # Convert absolute path ke relative path dari cwd dengan pendekatan yang lebih robust\n",
    "        current_cwd = Path.cwd()\n",
    "        file_path_obj = Path(file_path).resolve()  # Resolve untuk handle symlinks etc\n",
    "        \n",
    "        # Cek apakah file dalam subdirektori cwd\n",
    "        cwd_str = str(current_cwd)\n",
    "        file_str = str(file_path_obj)\n",
    "        \n",
    "        if file_str.startswith(cwd_str):\n",
    "            # File berada dalam cwd, buat relative path manual\n",
    "            relative_path_str = file_str[len(cwd_str):].lstrip('/\\\\').replace('\\\\', '/')\n",
    "        else:\n",
    "            # File berada di luar cwd, gunakan absolute path\n",
    "            relative_path_str = str(file_path_obj).replace('\\\\', '/')\n",
    "        \n",
    "        # Update config dengan beberapa format path untuk fleksibilitas\n",
    "        config['berita_penting_output'] = relative_path_str\n",
    "        config['berita_penting_output_absolute'] = str(file_path_obj)\n",
    "        \n",
    "        # Handle output directory path dengan aman\n",
    "        try:\n",
    "            dir_relative = str(out_dir.relative_to(current_cwd)).replace('\\\\', '/')\n",
    "        except ValueError:\n",
    "            # Jika tidak bisa dibuat relative, gunakan manual\n",
    "            out_dir_str = str(out_dir.resolve())\n",
    "            if out_dir_str.startswith(cwd_str):\n",
    "                dir_relative = out_dir_str[len(cwd_str):].lstrip('/\\\\').replace('\\\\', '/')\n",
    "            else:\n",
    "                dir_relative = str(out_dir)\n",
    "        \n",
    "        config['berita_penting_output_dir'] = dir_relative\n",
    "        config['berita_penting_last_updated'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # Debug info\n",
    "        print(f\"üîç Debug path info:\")\n",
    "        print(f\"   Current CWD: {current_cwd}\")\n",
    "        print(f\"   File path: {file_path_obj}\")\n",
    "        print(f\"   Relative path: {relative_path_str}\")\n",
    "        \n",
    "        # Backup config lama jika ada\n",
    "        if config_file.exists():\n",
    "            backup_file = config_file.with_suffix('.json.bak')\n",
    "            config_file.rename(backup_file)\n",
    "            print(f\"üì¶ Config backup saved: {backup_file}\")\n",
    "        \n",
    "        # Simpan config yang sudah diupdate\n",
    "        with open(config_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(config, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"üìù Config updated:\")\n",
    "        print(f\"   berita_penting_output: {relative_path_str}\")\n",
    "        print(f\"   berita_penting_output_dir: {config['berita_penting_output_dir']}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error updating config: {e}\")\n",
    "        return False\n",
    "\n",
    "# Helper function untuk mendapatkan path berita penting dari config\n",
    "def get_berita_penting_path_from_config():\n",
    "    \"\"\"\n",
    "    Fungsi helper untuk membaca path berita penting dari config\n",
    "    Otomatis resolve relative path berdasarkan cwd saat ini\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open('config.json', 'r', encoding='utf-8') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        # Coba gunakan relative path dulu\n",
    "        if 'berita_penting_output' in config:\n",
    "            relative_path = Path(config['berita_penting_output'])\n",
    "            if relative_path.exists():\n",
    "                return str(relative_path)\n",
    "        \n",
    "        # Jika relative path tidak ada, coba absolute path\n",
    "        if 'berita_penting_output_absolute' in config:\n",
    "            absolute_path = Path(config['berita_penting_output_absolute'])\n",
    "            if absolute_path.exists():\n",
    "                return str(absolute_path)\n",
    "        \n",
    "        # Jika tidak ada, return None\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Cannot read berita penting path from config: {e}\")\n",
    "        return None\n",
    "\n",
    "# Update config dengan path file yang baru disimpan\n",
    "update_success = update_config_with_berita_penting_path(outfile)\n",
    "if update_success:\n",
    "    print(f\"‚úÖ Config.json berhasil diupdate dengan path berita penting\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Config.json gagal diupdate, tapi file tetap tersimpan\")\n",
    "\n",
    "analisis_berita_penting = df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d1609e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GENERATOR DAFTAR BERITA & KONTEN ===\n",
      "üìä Data tersedia: 40 berita\n",
      "Memproses 40 berita yang berhasil dianalisis...\n",
      "‚úÖ Laporan disimpan di 00_laporan_cetak/daftar_berita_20251001_211415.txt\n",
      "üìÑ Total baris: 126\n",
      "\n",
      "üìã Preview laporan:\n",
      "------------------------------------------------------------\n",
      "Daftar Berita & Konten\n",
      "Rabu, 1 Oktober 2025\n",
      "Periode pantauan tanggal 30-1 September 2025 (pukul 14.00 s.d. 06.00 WIB)\n",
      "\n",
      "Media Online\n",
      "===========\n",
      "\n",
      "üü¢ Purbaya Bakal Sikat Rokok Ilegal, Seberapa Parah Peredarannya? - SINDOnews Ekbis\n",
      "https://ekbis.sindonews.com/read/1626051/34/purbaya-bakal-sikat-rokok-ilegal-seberapa-parah-peredarannya-1759046994\n",
      "\n",
      "üü¢ Dukung Kebijakan Menkeu Purbaya, Indef Singgung Fenomena Rokok Ilegal - republika.co.id\n",
      "https://ekonomi.republika.co.id/berita/t3ej9h484/dukung-kebijakan-menkeu-purbaya-indef-singgung-fenomena-rokok-ilegal\n",
      "...\n",
      "[114 baris lainnya]\n",
      "------------------------------------------------------------\n",
      "\n",
      "üéØ Selesai! File tersedia di folder: 00_laporan_cetak/\n"
     ]
    }
   ],
   "source": [
    "# SEL 2 - Generator Laporan \"Daftar Berita & Konten\" (Fixed Version)\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "def get_sentiment_emoji(sentimen):\n",
    "    \"\"\"Convert sentimen to emoji atau tag\"\"\"\n",
    "    if not sentimen:\n",
    "        return \"‚ö™\"\n",
    "    sentimen_lower = str(sentimen).lower()\n",
    "    if 'pos' in sentimen_lower:\n",
    "        return \"üü¢\"\n",
    "    elif 'neg' in sentimen_lower:\n",
    "        return \"üî¥\"\n",
    "    else:\n",
    "        return \"‚ö™\"\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text dari karakter bermasalah\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # Gunakan replace untuk karakter bermasalah umum\n",
    "    clean = str(text).replace('\\udcca', '').replace('\\udccb', '').replace('\\x00', '')\n",
    "    return clean.encode('utf-8', 'ignore').decode('utf-8')\n",
    "\n",
    "def format_indonesian_date():\n",
    "    \"\"\"Format tanggal hari ini dalam bahasa Indonesia\"\"\"\n",
    "    today = datetime.now()\n",
    "    days = ['Senin', 'Selasa', 'Rabu', 'Kamis', 'Jumat', 'Sabtu', 'Minggu']\n",
    "    months = ['Januari', 'Februari', 'Maret', 'April', 'Mei', 'Juni',\n",
    "              'Juli', 'Agustus', 'September', 'Oktober', 'November', 'Desember']\n",
    "    \n",
    "    day_name = days[today.weekday()]\n",
    "    day = today.day\n",
    "    month = months[today.month - 1]\n",
    "    year = today.year\n",
    "    \n",
    "    return f\"{day_name}, {day} {month} {year}\"\n",
    "\n",
    "def generate_daftar_berita_konten(df_data):\n",
    "    \"\"\"Generate laporan Daftar Berita & Konten\"\"\"\n",
    "    \n",
    "    # Header laporan\n",
    "    today = datetime.now()\n",
    "    yesterday = today - timedelta(days=1)\n",
    "    tanggal_laporan = format_indonesian_date()\n",
    "    \n",
    "    lines = []\n",
    "    lines.append(\"Daftar Berita & Konten\")\n",
    "    lines.append(tanggal_laporan)\n",
    "    lines.append(f\"Periode pantauan tanggal {yesterday.day}-{today.day} September 2025 (pukul 14.00 s.d. 06.00 WIB)\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"Media Online\")\n",
    "    lines.append(\"===========\")\n",
    "    lines.append(\"\")\n",
    "    \n",
    "    # Filter data yang berhasil dianalisis\n",
    "    success_data = df_data[df_data['analisis_status'] == 'ok'].copy()\n",
    "    \n",
    "    if success_data.empty:\n",
    "        lines.append(\"Tidak ada berita yang berhasil dianalisis.\")\n",
    "        return \"\\n\".join(lines)\n",
    "    \n",
    "    print(f\"Memproses {len(success_data)} berita yang berhasil dianalisis...\")\n",
    "    \n",
    "    # Kelompokkan berita berdasarkan sentimen: Positif -> Netral -> Negatif\n",
    "    sentimen_groups = {\n",
    "        'Positif': [],\n",
    "        'Netral': [],\n",
    "        'Negatif': []\n",
    "    }\n",
    "    \n",
    "    # Pisahkan berita berdasarkan sentimen\n",
    "    for idx, row in success_data.iterrows():\n",
    "        sentimen = str(row.get('sentimen', 'Netral')).strip()\n",
    "        \n",
    "        # Standardisasi sentimen\n",
    "        if 'positif' in sentimen.lower():\n",
    "            sentimen_groups['Positif'].append(row)\n",
    "        elif 'negatif' in sentimen.lower():\n",
    "            sentimen_groups['Negatif'].append(row)\n",
    "        else:\n",
    "            sentimen_groups['Netral'].append(row)\n",
    "    \n",
    "    # Urutkan setiap grup berdasarkan tanggal (terbaru ke terlama)\n",
    "    def sort_by_date(rows_list):\n",
    "        \"\"\"Urutkan list rows berdasarkan tanggal jika ada\"\"\"\n",
    "        if not rows_list:\n",
    "            return rows_list\n",
    "            \n",
    "        # Convert ke DataFrame untuk mudah sorting\n",
    "        temp_df = pd.DataFrame(rows_list)\n",
    "        \n",
    "        # Coba urutkan berdasarkan tanggal\n",
    "        if 'tanggal_berita' in temp_df.columns:\n",
    "            try:\n",
    "                temp_df['tanggal_sort'] = pd.to_datetime(temp_df['tanggal_berita'], errors='coerce')\n",
    "                temp_df = temp_df.sort_values('tanggal_sort', ascending=False, na_position='last')\n",
    "            except:\n",
    "                pass  # Jika gagal, biarkan urutan asli\n",
    "        elif 'waktu_publish' in temp_df.columns:\n",
    "            try:\n",
    "                temp_df['tanggal_sort'] = pd.to_datetime(temp_df['waktu_publish'], errors='coerce')\n",
    "                temp_df = temp_df.sort_values('tanggal_sort', ascending=False, na_position='last')\n",
    "            except:\n",
    "                pass  # Jika gagal, biarkan urutan asli\n",
    "        \n",
    "        return temp_df.to_dict('records')\n",
    "    \n",
    "    # Urutkan setiap grup berdasarkan tanggal\n",
    "    for sentimen_key in sentimen_groups:\n",
    "        sentimen_groups[sentimen_key] = sort_by_date(sentimen_groups[sentimen_key])\n",
    "    \n",
    "    # Generate entry berdasarkan urutan: Positif -> Netral -> Negatif\n",
    "    urutan_sentimen = ['Positif', 'Netral', 'Negatif']\n",
    "    \n",
    "    for sentimen_type in urutan_sentimen:\n",
    "        berita_list = sentimen_groups[sentimen_type]\n",
    "        \n",
    "        if berita_list:\n",
    "            # Generate entry untuk setiap berita dalam grup sentimen ini\n",
    "            for row_dict in berita_list:\n",
    "                # Ambil data\n",
    "                judul_raw = row_dict.get('judul_berita', 'Judul tidak tersedia')\n",
    "                url = row_dict.get('url_berita', row_dict.get('link', ''))\n",
    "                sentimen = row_dict.get('sentimen', 'Netral')\n",
    "                \n",
    "                # Clean text\n",
    "                judul_clean = clean_text(judul_raw)\n",
    "                if len(judul_clean.strip()) < 10:  # Jika terlalu banyak karakter hilang\n",
    "                    judul_clean = \"Berita Terkait Kementerian Keuangan\"\n",
    "                \n",
    "                # Format emoji sentimen\n",
    "                emoji = get_sentiment_emoji(sentimen)\n",
    "                \n",
    "                # Format entry\n",
    "                berita_line = f\"{emoji} {judul_clean}\"\n",
    "                lines.append(berita_line)\n",
    "                \n",
    "                if url and url.strip() and url != '#':\n",
    "                    url_clean = clean_text(url)\n",
    "                    if url_clean.strip():\n",
    "                        lines.append(url_clean)\n",
    "                lines.append(\"\")  # Baris kosong pemisah\n",
    "    \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def save_laporan_txt(content, filename_prefix=\"daftar_berita\"):\n",
    "    \"\"\"Simpan konten laporan ke file txt\"\"\"\n",
    "    # Buat direktori output\n",
    "    output_dir = Path(\"00_laporan_cetak\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Generate filename dengan timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{filename_prefix}_{timestamp}.txt\"\n",
    "    filepath = output_dir / filename\n",
    "    \n",
    "    # Tulis file dengan encoding yang aman\n",
    "    with open(filepath, 'w', encoding='utf-8', errors='ignore') as f:\n",
    "        f.write(content)\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "# ===== EKSEKUSI GENERATOR ===== #\n",
    "print(\"=== GENERATOR DAFTAR BERITA & KONTEN ===\")\n",
    "\n",
    "# Check data availability\n",
    "if 'analisis_berita_penting' not in globals():\n",
    "    print(\"‚ùå Data analisis_berita_penting tidak tersedia. Jalankan SEL 1 dulu.\")\n",
    "elif analisis_berita_penting.empty:\n",
    "    print(\"‚ùå DataFrame analisis_berita_penting kosong.\")\n",
    "else:\n",
    "    print(f\"üìä Data tersedia: {len(analisis_berita_penting)} berita\")\n",
    "    \n",
    "    # Generate laporan\n",
    "    try:\n",
    "        laporan_content = generate_daftar_berita_konten(analisis_berita_penting)\n",
    "        \n",
    "        # Simpan ke file\n",
    "        saved_file = save_laporan_txt(laporan_content, \"daftar_berita\")\n",
    "        \n",
    "        print(f\"‚úÖ Laporan disimpan di {saved_file}\")\n",
    "        print(f\"üìÑ Total baris: {len(laporan_content.splitlines())}\")\n",
    "        \n",
    "        # Preview (10 baris pertama)\n",
    "        preview_lines = laporan_content.splitlines()[:12]\n",
    "        print(\"\\nüìã Preview laporan:\")\n",
    "        print(\"-\" * 60)\n",
    "        for line in preview_lines:\n",
    "            if line.strip():\n",
    "                print(line)\n",
    "            else:\n",
    "                print(\"\")  # Baris kosong\n",
    "        if len(laporan_content.splitlines()) > 12:\n",
    "            print(\"...\")\n",
    "            print(f\"[{len(laporan_content.splitlines()) - 12} baris lainnya]\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR: {str(e)}\")\n",
    "        # Buat fallback laporan minimal\n",
    "        fallback_content = f\"\"\"Daftar Berita & Konten\n",
    "{format_indonesian_date()}\n",
    "\n",
    "Media Online\n",
    "===========\n",
    "\n",
    "Total berita: {len(analisis_berita_penting)}\n",
    "Berhasil dianalisis: {(analisis_berita_penting['analisis_status'] == 'ok').sum()}\n",
    "\n",
    "[Detail laporan tidak dapat dibuat - silakan cek file CSV]\n",
    "\"\"\"\n",
    "        \n",
    "        saved_file = save_laporan_txt(fallback_content, \"daftar_berita_fallback\")\n",
    "        print(f\"üìÑ Laporan fallback disimpan di {saved_file}\")\n",
    "\n",
    "print(\"\\nüéØ Selesai! File tersedia di folder: 00_laporan_cetak/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97842265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using DEEPSEEK for headline generation\n",
      "=== GENERATOR NEWS UPDATE ===\n",
      "üìä Data tersedia: 40 berita\n",
      "\n",
      "üìù Gabungan resume (11383 karakter):\n",
      "Preview: Menteri Keuangan Purbaya Yudhi Sadewa akan melakukan penindakan masif terhadap rokok ilegal untuk mengamankan penerimaan negara yang tergerus triliunan rupiah, tanpa perlu menaikkan cukai rokok. Langk...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 07:07:07,901 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ News Update disimpan di: 00_laporan_cetak/news_update_general_20251002_070717.txt\n",
      "üìÑ Total baris: 67\n",
      "\n",
      "üìã Preview News Update:\n",
      "------------------------------------------------------------\n",
      "News Update\n",
      "Kebijakan Cukai Tembakau\n",
      "Jakarta, Kamis, 2 Oktober 2025 (Pukul 07.00 WIB)\n",
      "\n",
      "Pemberitaan terkait kebijakan cukai tembakau hari ini tercatat terdapat 40 berita (37 positif, 1 netral, 2 negatif) di media online.\n",
      "\n",
      "Sorotan Media Online\n",
      "‚Ä¢ Menkeu Purbaya memastikan tarif cukai rokok 2026 tidak naik untuk jaga lapangan kerja.\n",
      "‚Ä¢ Fokus utama beralih ke penindakan masif rokok ilegal yang rugikan negara triliunan rupiah.\n",
      "‚Ä¢ Bea Cukai ungkap modus rokok ilegal online disamarkan sebagai kaos dan pakaian dalam.\n",
      "‚Ä¢ Pemerintah larang penjualan rokok ilegal di semua saluran, termasuk warung dan e-commerce.\n",
      "‚Ä¢ Kerugian negara akibat rokok ilegal di Jawa Barat saja capai Rp 124,8 miliar.\n",
      "‚Ä¢ Operasi pengamanan berhasil sita 745 juta batang rokok ilegal dalam 12.041 penindakan.\n",
      "‚Ä¢ Ekonom dan Kemenperin dukung kebijakan, namun ingatkan penanganan rokok ilegal yang masif.\n",
      "‚Ä¢ Komnas PT ancam laporkan Menkeu ke Ombudsman karena tarif cukai dinilai belum optimal.\n",
      "...\n",
      "[52 baris lainnya]\n",
      "------------------------------------------------------------\n",
      "\n",
      "üéØ Selesai! File tersedia di: 00_laporan_cetak/\n"
     ]
    }
   ],
   "source": [
    "# SEL 3 - Generator News Update dengan AI Analysis\n",
    "# Gabungkan ringkasan berita dari proses di sel menjadi satu paragraf panjang. \n",
    "# Buat prompt untuk membuat maksimal 10 headline untuk mengisi sorotan_media_online\n",
    "# Tambahkan Tautan media online yang sesuai headline tersebut dari database.\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup AI client untuk headline generation - menggunakan konfigurasi dinamis\n",
    "_client_ai = None\n",
    "_call_ai_model = None\n",
    "\n",
    "if 'AI_CLIENT' in globals() and 'CALL_AI_MODEL' in globals():\n",
    "    _client_ai = AI_CLIENT\n",
    "    _call_ai_model = CALL_AI_MODEL\n",
    "    ai_provider = os.getenv('MODEL_ANALISIS', 'openai').upper()\n",
    "    print(f\"‚úÖ Using {ai_provider} for headline generation\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è AI configuration not found. Headlines will use fallback method.\")\n",
    "\n",
    "def load_config():\n",
    "    \"\"\"Load config.json untuk mendapat topic keywords\"\"\"\n",
    "    try:\n",
    "        with open('config.json', 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Tidak dapat memuat config.json: {e}\")\n",
    "        return {}\n",
    "\n",
    "def clean_text_safe(text):\n",
    "    \"\"\"Safe text cleaning\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    return str(text).encode('utf-8', 'ignore').decode('utf-8').strip()\n",
    "\n",
    "def combine_resumes(df_data):\n",
    "    \"\"\"Gabungkan semua resume berita menjadi satu paragraf panjang\"\"\"\n",
    "    success_data = df_data[df_data['analisis_status'] == 'ok']\n",
    "    \n",
    "    if success_data.empty:\n",
    "        return \"Tidak ada resume berita yang tersedia.\"\n",
    "    \n",
    "    # Gabungkan semua resume\n",
    "    all_resumes = []\n",
    "    for _, row in success_data.iterrows():\n",
    "        resume = clean_text_safe(row.get('resume_ai', ''))\n",
    "        if resume and len(resume) > 10:\n",
    "            all_resumes.append(resume)\n",
    "    \n",
    "    if not all_resumes:\n",
    "        return \"Resume berita tidak tersedia.\"\n",
    "    \n",
    "    # Gabungkan dengan connector yang natural\n",
    "    combined = \". \".join(all_resumes)\n",
    "    return combined\n",
    "\n",
    "def count_sentiment_stats(df_data):\n",
    "    \"\"\"Hitung statistik sentimen dengan akurasi tinggi\"\"\"\n",
    "    success_data = df_data[df_data['analisis_status'] == 'ok']\n",
    "    \n",
    "    if success_data.empty:\n",
    "        return 0, 0, 0, 0\n",
    "    \n",
    "    total = len(success_data)\n",
    "    \n",
    "    # Hitung sentimen dengan standardisasi yang lebih akurat\n",
    "    positif = 0\n",
    "    negatif = 0 \n",
    "    netral = 0\n",
    "    \n",
    "    for _, row in success_data.iterrows():\n",
    "        sentimen = str(row.get('sentimen', 'netral')).strip().lower()\n",
    "        \n",
    "        if 'positif' in sentimen:\n",
    "            positif += 1\n",
    "        elif 'negatif' in sentimen:\n",
    "            negatif += 1\n",
    "        else:\n",
    "            netral += 1\n",
    "    \n",
    "    # Validasi total\n",
    "    if positif + negatif + netral != total:\n",
    "        print(f\"Warning: Sentimen count mismatch. Total: {total}, Sum: {positif + negatif + netral}\")\n",
    "    \n",
    "    return total, positif, negatif, netral\n",
    "\n",
    "def generate_headlines_with_ai(combined_resumes, topic_keywords, df_data):\n",
    "    \"\"\"Generate headlines menggunakan AI\"\"\"\n",
    "    try:\n",
    "        if not _call_ai_model:\n",
    "            ai_provider = os.getenv('MODEL_ANALISIS', 'unknown').upper()\n",
    "            print(f\"Warning: {ai_provider} tidak tersedia, menggunakan fallback headlines\")\n",
    "            return generate_fallback_headlines(df_data)\n",
    "        \n",
    "        # Build prompt untuk AI\n",
    "        keywords_str = \", \".join(topic_keywords) if topic_keywords else \"Kementerian Keuangan, ekonomi, fiskal\"\n",
    "        \n",
    "        prompt = f\"\"\"Berdasarkan ringkasan berita berikut, buatlah maksimal 8 poin sorotan media online yang menarik dan informatif.\n",
    "\n",
    "RINGKASAN GABUNGAN BERITA:\n",
    "{combined_resumes[:5000]}\n",
    "\n",
    "TOPIK YANG DIPANTAU: {keywords_str}\n",
    "\n",
    "TUGAS:\n",
    "1. Buat 5-10 poin sorotan yang merangkum isu-isu utama yang diangkat pada ringkasan gabungan berita.\n",
    "2. Setiap poin sorotan harus berbeda satu dengan yang lain serta mencakup seluruh aspek dalam ringkasan gabungan berita.\n",
    "3. Fokus pada aspek Kementerian Keuangan, ekonomi, dan kebijakan fiskal\n",
    "4. Setiap poin maksimal 25 kata\n",
    "5. Gunakan bahasa Indonesia yang profesional\n",
    "6. Format: satu poin per baris, dimulai dengan \"‚Ä¢ \"\n",
    "\n",
    "CONTOH FORMAT:\n",
    "‚Ä¢ Menkeu melakukan inspeksi mendadak ke kantor pusat BNI untuk memantau penyaluran kredit perbankan.\n",
    "‚Ä¢ Kebijakan cukai tembakau 2026 tidak mengalami kenaikan untuk melindungi industri dan pekerja.\n",
    "\n",
    "Buat poin sorotan sekarang:\"\"\"\n",
    "\n",
    "        ai_response = _call_ai_model(prompt, temperature=0.3, max_tokens=600)\n",
    "        \n",
    "        # Parse response menjadi list headlines\n",
    "        headlines = []\n",
    "        for line in ai_response.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if line.startswith('‚Ä¢'):\n",
    "                headline = line[1:].strip()\n",
    "                if len(headline) > 10:  # Filter headline yang terlalu pendek\n",
    "                    headlines.append(headline)\n",
    "        \n",
    "        return headlines[:8]  # Maksimal 8 headlines\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating AI headlines: {e}\")\n",
    "        return generate_fallback_headlines(df_data)\n",
    "\n",
    "def generate_fallback_headlines(df_data):\n",
    "    \"\"\"Generate headlines fallback tanpa AI\"\"\"\n",
    "    success_data = df_data[df_data['analisis_status'] == 'ok']\n",
    "    \n",
    "    headlines = []\n",
    "    for _, row in success_data.head(6).iterrows():\n",
    "        resume = clean_text_safe(row.get('resume_ai', ''))\n",
    "        if resume and len(resume) > 15:\n",
    "            # Potong di titik atau koma pertama untuk jadi headline\n",
    "            headline = resume.split('.')[0].split(',')[0]\n",
    "            if len(headline) > 20 and len(headline) < 100:\n",
    "                headlines.append(headline.strip())\n",
    "    \n",
    "    return headlines\n",
    "\n",
    "def get_main_topic_from_data(df_data, config):\n",
    "    \"\"\"Tentukan topik utama berdasarkan data berita\"\"\"\n",
    "    success_data = df_data[df_data['analisis_status'] == 'ok']\n",
    "    \n",
    "    if success_data.empty:\n",
    "        return \"Monitoring Berita Kementerian Keuangan\"\n",
    "    \n",
    "    # Ambil kata kunci dari judul-judul berita\n",
    "    all_titles = \" \".join([clean_text_safe(row.get('judul_berita', '')) for _, row in success_data.iterrows()])\n",
    "    \n",
    "    # Cari keyword yang sering muncul\n",
    "    common_words = ['Menkeu', 'Purbaya', 'BNI', 'Sidak', 'Cukai', 'Bank', 'Kredit', 'Ekonomi']\n",
    "    word_counts = {word: all_titles.upper().count(word.upper()) for word in common_words}\n",
    "    \n",
    "    # Ambil kata dengan frekuensi tertinggi\n",
    "    most_common = max(word_counts.items(), key=lambda x: x[1])\n",
    "    if most_common[1] > 0:\n",
    "        if 'SIDAK' in all_titles.upper() and 'BNI' in all_titles.upper():\n",
    "            return \"Menkeu Sidak BNI\"\n",
    "        elif 'CUKAI' in all_titles.upper():\n",
    "            return \"Kebijakan Cukai Tembakau\"\n",
    "        elif 'MENKEU' in all_titles.upper() or 'PURBAYA' in all_titles.upper():\n",
    "            return \"Aktivitas Menteri Keuangan\"\n",
    "    \n",
    "    return \"Monitoring Berita Kementerian Keuangan\"\n",
    "\n",
    "def get_related_links_for_headlines(df_data, headlines, min_score_threshold=1):\n",
    "    \"\"\"Ambil link berita yang relevan berdasarkan headline yang dihasilkan AI dengan informasi sentimen\"\"\"\n",
    "    success_data = df_data[df_data['analisis_status'] == 'ok']\n",
    "    \n",
    "    if success_data.empty or not headlines:\n",
    "        return []\n",
    "    \n",
    "    # Mapping headline ke berita yang paling relevan\n",
    "    headline_matches = []\n",
    "    used_urls = set()\n",
    "    \n",
    "    for headline in headlines:\n",
    "        best_matches = []  # Simpan beberapa match terbaik per headline\n",
    "        \n",
    "        # Cari berita yang paling cocok dengan headline\n",
    "        for _, row in success_data.iterrows():\n",
    "            judul = clean_text_safe(row.get('judul_berita', '')).upper()\n",
    "            resume = clean_text_safe(row.get('resume_ai', '')).upper()\n",
    "            url = clean_text_safe(row.get('url_berita', row.get('link', '')))\n",
    "            sentimen = clean_text_safe(row.get('sentimen', 'Netral'))\n",
    "            \n",
    "            if not url or url == '#' or url in used_urls:\n",
    "                continue\n",
    "            \n",
    "            # Hitung skor kecocokan berdasarkan kata kunci dalam headline\n",
    "            headline_words = headline.upper().split()\n",
    "            score = 0\n",
    "            \n",
    "            # Skor berdasarkan kecocokan kata dalam judul (bobot lebih tinggi)\n",
    "            for word in headline_words:\n",
    "                if len(word) > 3:  # Skip kata pendek\n",
    "                    if word in judul:\n",
    "                        score += 3\n",
    "                    elif word in resume:\n",
    "                        score += 1\n",
    "            \n",
    "            # Tambahkan ke best matches jika skor memenuhi threshold\n",
    "            if score >= min_score_threshold:\n",
    "                best_matches.append({\n",
    "                    'headline': headline,\n",
    "                    'judul': clean_text_safe(row.get('judul_berita', '')),\n",
    "                    'url': url,\n",
    "                    'sentimen': sentimen,\n",
    "                    'score': score,\n",
    "                    'row_data': row\n",
    "                })\n",
    "        \n",
    "        # Urutkan berdasarkan skor dan ambil yang terbaik\n",
    "        best_matches.sort(key=lambda x: x['score'], reverse=True)\n",
    "        \n",
    "        # Ambil maksimal 2-3 berita terbaik per headline untuk variasi\n",
    "        for match in best_matches[:2]:\n",
    "            if match['url'] not in used_urls:\n",
    "                headline_matches.append(match)\n",
    "                used_urls.add(match['url'])\n",
    "    \n",
    "    # Jika masih sedikit, tambahkan berita terpenting lainnya (tanpa threshold ketat)\n",
    "    if len(headline_matches) < 3:\n",
    "        for _, row in success_data.iterrows():\n",
    "            if len(headline_matches) >= 12:  # Batas maksimal fleksibel\n",
    "                break\n",
    "                \n",
    "            url = clean_text_safe(row.get('url_berita', row.get('link', '')))\n",
    "            judul = clean_text_safe(row.get('judul_berita', ''))\n",
    "            sentimen = clean_text_safe(row.get('sentimen', 'Netral'))\n",
    "            \n",
    "            if url and url != '#' and url not in used_urls:\n",
    "                headline_matches.append({\n",
    "                    'headline': 'Berita Terkait Lainnya',\n",
    "                    'judul': judul,\n",
    "                    'url': url,\n",
    "                    'sentimen': sentimen,\n",
    "                    'score': 0,\n",
    "                    'row_data': row\n",
    "                })\n",
    "                used_urls.add(url)\n",
    "    \n",
    "    # Format hasil untuk output dengan informasi sentimen\n",
    "    formatted_links = []\n",
    "    for match in headline_matches:\n",
    "        judul = match['judul']\n",
    "        # Potong judul jika terlalu panjang\n",
    "        if len(judul) > 75:\n",
    "            judul = judul[:75] + \"...\"\n",
    "        \n",
    "        # Tentukan emoji sentimen\n",
    "        sentimen = match['sentimen'].lower()\n",
    "        if 'positif' in sentimen:\n",
    "            sentimen_emoji = \"üü¢\"\n",
    "            sentimen_text = \"Positif\"\n",
    "        elif 'negatif' in sentimen:\n",
    "            sentimen_emoji = \"üî¥\"  \n",
    "            sentimen_text = \"Negatif\"\n",
    "        else:\n",
    "            sentimen_emoji = \"‚ö™\"\n",
    "            sentimen_text = \"Netral\"\n",
    "        \n",
    "        formatted_links.append({\n",
    "            'judul': judul,\n",
    "            'url': match['url'],\n",
    "            'headline_ref': match['headline'],\n",
    "            'sentimen': sentimen_text,\n",
    "            'sentimen_emoji': sentimen_emoji,\n",
    "            'score': match['score']\n",
    "        })\n",
    "    \n",
    "    # Urutkan berdasarkan sentimen: Positif -> Netral -> Negatif\n",
    "    def sentimen_sort_key(link):\n",
    "        sentimen = link['sentimen']\n",
    "        if sentimen == 'Positif':\n",
    "            return 0\n",
    "        elif sentimen == 'Netral':\n",
    "            return 1\n",
    "        elif sentimen == 'Negatif':\n",
    "            return 2\n",
    "        else:\n",
    "            return 3  # fallback\n",
    "    \n",
    "    # Urutkan berdasarkan sentimen, kemudian berdasarkan skor dalam setiap grup sentimen\n",
    "    formatted_links.sort(key=lambda x: (sentimen_sort_key(x), -x['score']))\n",
    "    \n",
    "    return formatted_links\n",
    "\n",
    "def get_related_links(df_data, max_links=8):\n",
    "    \"\"\"Fallback function untuk compatibility - ambil link berita secara umum\"\"\"\n",
    "    success_data = df_data[df_data['analisis_status'] == 'ok']\n",
    "    \n",
    "    if success_data.empty:\n",
    "        return []\n",
    "    \n",
    "    links = []\n",
    "    for idx, row in success_data.head(max_links).iterrows():\n",
    "        judul = clean_text_safe(row.get('judul_berita', ''))\n",
    "        url = clean_text_safe(row.get('url_berita', row.get('link', '')))\n",
    "        \n",
    "        if judul and url and url != '#':\n",
    "            # Potong judul jika terlalu panjang\n",
    "            if len(judul) > 80:\n",
    "                judul = judul[:80] + \"...\"\n",
    "            \n",
    "            links.append({\n",
    "                'judul': judul,\n",
    "                'url': url\n",
    "            })\n",
    "    \n",
    "    return links\n",
    "\n",
    "def generate_news_update(df_data, config=None):\n",
    "    \"\"\"Generate News Update format lengkap\"\"\"\n",
    "    \n",
    "    if config is None:\n",
    "        config = {}\n",
    "    \n",
    "    # Ambil data statistik\n",
    "    total, positif, negatif, netral = count_sentiment_stats(df_data)\n",
    "    \n",
    "    if total == 0:\n",
    "        return \"News Update tidak dapat dibuat: tidak ada berita yang berhasil dianalisis.\"\n",
    "    \n",
    "    # Header informasi\n",
    "    today = datetime.now()\n",
    "    hari_indo = ['Senin', 'Selasa', 'Rabu', 'Kamis', 'Jumat', 'Sabtu', 'Minggu'][today.weekday()]\n",
    "    tanggal_indo = f\"{today.day} Oktober {today.year}\"\n",
    "    waktu_laporan = f\"{hari_indo}, {tanggal_indo} (Pukul {today.hour:02d}.00 WIB)\"\n",
    "    \n",
    "    # Tentukan topik utama\n",
    "    main_topic = get_main_topic_from_data(df_data, config)\n",
    "    \n",
    "    # Gabungkan resume\n",
    "    combined_resumes = combine_resumes(df_data)\n",
    "    \n",
    "    # Generate headlines dengan AI\n",
    "    topic_keywords = config.get('topic_keywords', [])\n",
    "    headlines = generate_headlines_with_ai(combined_resumes, topic_keywords, df_data)\n",
    "    \n",
    "    # Ambil tautan terkait yang sesuai dengan headlines (jumlah fleksibel)\n",
    "    related_links = get_related_links_for_headlines(df_data, headlines)\n",
    "    \n",
    "    # Build content\n",
    "    lines = []\n",
    "    lines.append(\"News Update\")\n",
    "    lines.append(main_topic)\n",
    "    lines.append(f\"Jakarta, {waktu_laporan}\")\n",
    "    lines.append(\"\")\n",
    "    \n",
    "    # Statistik berita dengan data akurat\n",
    "    stats_text = f\"Pemberitaan terkait {main_topic.lower()} hari ini tercatat terdapat {total} berita\"\n",
    "    \n",
    "    # Hanya tampilkan sentimen yang ada (> 0)\n",
    "    detail_stats = []\n",
    "    if positif > 0:\n",
    "        detail_stats.append(f\"{positif} positif\")\n",
    "    if netral > 0:\n",
    "        detail_stats.append(f\"{netral} netral\")\n",
    "    if negatif > 0:\n",
    "        detail_stats.append(f\"{negatif} negatif\")\n",
    "    \n",
    "    if detail_stats:\n",
    "        stats_text += f\" ({', '.join(detail_stats)})\"\n",
    "    \n",
    "    stats_text += \" di media online.\"\n",
    "    lines.append(stats_text)\n",
    "    lines.append(\"\")\n",
    "    \n",
    "    # Sorotan Media Online\n",
    "    lines.append(\"Sorotan Media Online\")\n",
    "    if headlines:\n",
    "        for headline in headlines:\n",
    "            lines.append(f\"‚Ä¢ {headline}\")\n",
    "    else:\n",
    "        lines.append(\"‚Ä¢ Tidak ada sorotan khusus tersedia.\")\n",
    "    \n",
    "    lines.append(\"\")\n",
    "    \n",
    "    # Tautan Media Online\n",
    "    lines.append(\"Tautan Media Online:\")\n",
    "    if related_links:\n",
    "        for i, link in enumerate(related_links, 1):\n",
    "            # Format dengan emoji sentimen\n",
    "            lines.append(f\" {i}. {link['sentimen_emoji']} {link['judul']} ({link['sentimen']})\")\n",
    "            lines.append(f\"    {link['url']}\")\n",
    "            # Tambahkan referensi ke headline jika ada dan relevan\n",
    "            if link.get('headline_ref') and link['headline_ref'] != 'Berita Terkait Lainnya' and link.get('score', 0) > 0:\n",
    "                lines.append(f\"    (Terkait sorotan: {link['headline_ref'][:50]}...)\")\n",
    "    else:\n",
    "        lines.append(\" 1. Tidak ada tautan tersedia\")\n",
    "    \n",
    "    # Tambahkan informasi statistik tautan\n",
    "    if related_links:\n",
    "        lines.append(\"\")\n",
    "        total_links = len(related_links)\n",
    "        sentimen_counts = {}\n",
    "        for link in related_links:\n",
    "            sent = link['sentimen']\n",
    "            sentimen_counts[sent] = sentimen_counts.get(sent, 0) + 1\n",
    "        \n",
    "        stats_parts = []\n",
    "        if sentimen_counts.get('Positif', 0) > 0:\n",
    "            stats_parts.append(f\"{sentimen_counts['Positif']} positif\")\n",
    "        if sentimen_counts.get('Netral', 0) > 0:\n",
    "            stats_parts.append(f\"{sentimen_counts['Netral']} netral\") \n",
    "        if sentimen_counts.get('Negatif', 0) > 0:\n",
    "            stats_parts.append(f\"{sentimen_counts['Negatif']} negatif\")\n",
    "        \n",
    "        if stats_parts:\n",
    "            lines.append(f\"Total {total_links} tautan ({', '.join(stats_parts)})\")\n",
    "    \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def save_news_update(content, filename_prefix=\"news_update\"):\n",
    "    \"\"\"Simpan News Update ke file txt\"\"\"\n",
    "    output_dir = Path(\"00_laporan_cetak\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{filename_prefix}_{timestamp}.txt\"\n",
    "    filepath = output_dir / filename\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8', errors='ignore') as f:\n",
    "        f.write(content)\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "# ===== EKSEKUSI SEL 3 ===== #\n",
    "print(\"=== GENERATOR NEWS UPDATE ===\")\n",
    "\n",
    "# Check data\n",
    "if 'analisis_berita_penting' not in globals() or analisis_berita_penting.empty:\n",
    "    print(\"‚ùå Data analisis_berita_penting tidak tersedia. Jalankan SEL 1 terlebih dahulu.\")\n",
    "else:\n",
    "    print(f\"üìä Data tersedia: {len(analisis_berita_penting)} berita\")\n",
    "    \n",
    "    # Load config\n",
    "    config = load_config()\n",
    "    \n",
    "    # Preview gabungan resume\n",
    "    combined_resumes = combine_resumes(analisis_berita_penting)\n",
    "    print(f\"\\nüìù Gabungan resume ({len(combined_resumes)} karakter):\")\n",
    "    print(f\"Preview: {combined_resumes[:200]}...\")\n",
    "    \n",
    "    # Generate news update\n",
    "    try:\n",
    "        news_update_content = generate_news_update(analisis_berita_penting, config)\n",
    "        \n",
    "        # Simpan file\n",
    "        saved_file = save_news_update(news_update_content, \"news_update_general\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ News Update disimpan di: {saved_file}\")\n",
    "        print(f\"üìÑ Total baris: {len(news_update_content.splitlines())}\")\n",
    "        \n",
    "        # Preview hasil\n",
    "        preview_lines = news_update_content.splitlines()[:15]\n",
    "        print(f\"\\nüìã Preview News Update:\")\n",
    "        print(\"-\" * 60)\n",
    "        for line in preview_lines:\n",
    "            print(line)\n",
    "        if len(news_update_content.splitlines()) > 15:\n",
    "            print(\"...\")\n",
    "            print(f\"[{len(news_update_content.splitlines()) - 15} baris lainnya]\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating news update: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(f\"\\nüéØ Selesai! File tersedia di: 00_laporan_cetak/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e026da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using DEEPSEEK for topic analysis\n",
      "=== GENERATOR LAPORAN ANALISIS MEDIA (FIXED VERSION) ===\n",
      "üìä Data tersedia: 40 berita\n",
      "üìã Topic keywords dari config: ['rokok ilegal', 'makan bergizi gratis', 'tax amnesty', 'sidak BNI', 'lainnya']\n",
      "üè∑Ô∏è  Berita dikelompokkan dalam 1 topik:\n",
      "   - rokok ilegal: 38 berita\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 07:16:06,114 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Laporan Analisis Media disimpan di: 00_laporan_cetak/laporan_analisis_media_20251002_071617.txt\n",
      "üìÑ Total baris: 146\n",
      "\n",
      "üìã Preview Laporan Analisis Media:\n",
      "----------------------------------------------------------------------\n",
      "===== Page 1 =====\n",
      "\n",
      "**Laporan Analisis Media Online dan Media Sosial**\n",
      "Kamis, 2 Oktober 2025\n",
      "\n",
      "**EXECUTIVE SUMMARY**\n",
      "==================================================\n",
      "Periode pemantauan media online menunjukkan 40 berita penting yang berhasil dianalisis.\n",
      "Dari jumlah tersebut, 38 berita sesuai dengan topik yang dipantau: rokok ilegal.\n",
      "Fokus pemerintah terutama pada transparansi dan pengawasan sektor keuangan.\n",
      "Pernyataan dan kebijakan penting dari pejabat terkait terus dipantau secara intensif.\n",
      "\n",
      "**MEDIA ONLINE**\n",
      "\n",
      "**Topik Berita:** rokok ilegal\n",
      "**Tonasi Berita:** positif\n",
      "\n",
      "**Pesan Kunci dan Analisis:**\n",
      "\n",
      "**ISU KEMENKEU**\n",
      "...\n",
      "[126 baris lainnya]\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üéØ Selesai! File tersedia di: 00_laporan_cetak/\n"
     ]
    }
   ],
   "source": [
    "# SEL 4 - Generator Laporan Analisis Media Online dan Media Sosial (Fixed Version)\n",
    "# Format Laporan Analisis Berita\n",
    "# Lengkapi sel ini dengan cara untuk memanggil open AI dengan feeding data berupa:\n",
    "# 1. Gabungan dari resume berita yang ada dikelompokkan dalam topik terpisah sesuai config.json 'topic_keywords'\n",
    "# 2. Topik Berita di dokumen diambil dari config.json 'topic_keywords', masing-masing topic harus ada resume berita singkat. \n",
    "# 3. Masing-masing topic harus diberikan poin 2-3 poin penjelasan.\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Setup AI client - menggunakan konfigurasi dinamis\n",
    "_client_ai = None\n",
    "_call_ai_model = None\n",
    "\n",
    "if 'AI_CLIENT' in globals() and 'CALL_AI_MODEL' in globals():\n",
    "    _client_ai = AI_CLIENT\n",
    "    _call_ai_model = CALL_AI_MODEL\n",
    "    ai_provider = os.getenv('MODEL_ANALISIS', 'openai').upper()\n",
    "    print(f\"‚úÖ Using {ai_provider} for topic analysis\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è AI configuration not found. Analysis will use fallback method.\")\n",
    "\n",
    "def load_config_keywords():\n",
    "    \"\"\"Load config.json untuk mendapat topic keywords\"\"\"\n",
    "    try:\n",
    "        with open('config.json', 'r', encoding='utf-8') as f:\n",
    "            config = json.load(f)\n",
    "        return config.get('topic_keywords', []), config\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Tidak dapat memuat config.json: {e}\")\n",
    "        return [], {}\n",
    "\n",
    "def clean_text_safe(text):\n",
    "    \"\"\"Safe text cleaning\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    return str(text).encode('utf-8', 'ignore').decode('utf-8').strip()\n",
    "\n",
    "def format_indonesian_datetime():\n",
    "    \"\"\"Format tanggal dan waktu dalam bahasa Indonesia\"\"\"\n",
    "    today = datetime.now()\n",
    "    days = ['Senin', 'Selasa', 'Rabu', 'Kamis', 'Jumat', 'Sabtu', 'Minggu']\n",
    "    months = ['Januari', 'Februari', 'Maret', 'April', 'Mei', 'Juni',\n",
    "              'Juli', 'Agustus', 'September', 'Oktober', 'November', 'Desember']\n",
    "    \n",
    "    day_name = days[today.weekday()]\n",
    "    day = today.day\n",
    "    month = months[today.month - 1]\n",
    "    year = today.year\n",
    "    \n",
    "    return f\"{day_name}, {day} {month} {year}\"\n",
    "\n",
    "def group_news_by_topics(df_data, topic_keywords):\n",
    "    \"\"\"Kelompokkan berita berdasarkan topic keywords dari config.json - FIXED VERSION\"\"\"\n",
    "    success_data = df_data[df_data['analisis_status'] == 'ok'].copy()\n",
    "    \n",
    "    if success_data.empty:\n",
    "        return {}\n",
    "    \n",
    "    # Kelompokkan berita berdasarkan topik\n",
    "    topic_groups = defaultdict(list)\n",
    "    \n",
    "    # Jika tidak ada topic_keywords, gunakan topik_llm yang ada\n",
    "    if not topic_keywords:\n",
    "        for _, row in success_data.iterrows():\n",
    "            topik = clean_text_safe(row.get('topik_llm', 'Lainnya'))\n",
    "            if topik != 'Lainnya':\n",
    "                topic_groups[topik].append(row)\n",
    "    else:\n",
    "        # Gunakan topic_keywords dari config - HANYA yang cocok dengan keywords\n",
    "        for _, row in success_data.iterrows():\n",
    "            judul = clean_text_safe(row.get('judul_berita', '')).upper()\n",
    "            resume = clean_text_safe(row.get('resume_ai', '')).upper()\n",
    "            \n",
    "            # Cek apakah berita cocok dengan keywords\n",
    "            for topic in topic_keywords:\n",
    "                topic_upper = topic.upper()\n",
    "                if topic_upper in judul or topic_upper in resume:\n",
    "                    topic_groups[topic].append(row)\n",
    "                    break  # Berita hanya masuk ke satu topik\n",
    "        \n",
    "        # TIDAK membuat kategori \"Isu Lainnya\" otomatis\n",
    "        # Hanya topik yang ada beritanya yang akan ditampilkan\n",
    "    \n",
    "    return dict(topic_groups)\n",
    "\n",
    "def analyze_sentiment_by_topic(topic_groups):\n",
    "    \"\"\"Analisis sentimen per topik\"\"\"\n",
    "    topic_sentiments = {}\n",
    "    \n",
    "    for topic, news_list in topic_groups.items():\n",
    "        sentiments = []\n",
    "        for news in news_list:\n",
    "            sentiment = clean_text_safe(news.get('sentimen', 'Netral'))\n",
    "            sentiments.append(sentiment)\n",
    "        \n",
    "        # Hitung distribusi sentimen\n",
    "        sentiment_counts = Counter(sentiments)\n",
    "        total = len(sentiments)\n",
    "        \n",
    "        # Tentukan sentimen dominan\n",
    "        if sentiment_counts.get('Positif', 0) > total * 0.5:\n",
    "            dominant = 'Positif'\n",
    "        elif sentiment_counts.get('Negatif', 0) > total * 0.3:\n",
    "            dominant = 'Negatif'\n",
    "        else:\n",
    "            dominant = 'Netral'\n",
    "        \n",
    "        topic_sentiments[topic] = {\n",
    "            'dominant': dominant,\n",
    "            'distribution': dict(sentiment_counts),\n",
    "            'total': total\n",
    "        }\n",
    "    \n",
    "    return topic_sentiments\n",
    "\n",
    "def generate_topic_analysis_with_ai(topic, news_list, topic_keywords):\n",
    "    \"\"\"Generate analisis untuk satu topik menggunakan AI\"\"\"\n",
    "    \n",
    "    if not _call_ai_model:\n",
    "        return generate_fallback_topic_analysis(topic, news_list)\n",
    "    \n",
    "    # Gabungkan semua resume untuk topik ini\n",
    "    resumes = []\n",
    "    juduls = []\n",
    "    dampaks = []\n",
    "    \n",
    "    for news in news_list[:5]:  # Maksimal 5 berita per topik\n",
    "        resume = clean_text_safe(news.get('resume_ai', ''))\n",
    "        judul = clean_text_safe(news.get('judul_berita', ''))\n",
    "        dampak = clean_text_safe(news.get('dampak_kemenkeu_ai', ''))\n",
    "        \n",
    "        if resume:\n",
    "            resumes.append(resume)\n",
    "        if judul:\n",
    "            juduls.append(judul[:100])  # Potong judul yang terlalu panjang\n",
    "        if dampak:\n",
    "            dampaks.append(dampak)\n",
    "    \n",
    "    combined_resumes = \". \".join(resumes)\n",
    "    combined_juduls = \". \".join(juduls)\n",
    "    \n",
    "    # Build prompt untuk AI yang lebih spesifik\n",
    "    prompt = f\"\"\"Analisis komprehensif topik berita Kementerian Keuangan berikut:\n",
    "\n",
    "TOPIK: {topic}\n",
    "\n",
    "JUDUL-JUDUL BERITA:\n",
    "{combined_juduls[:800]}\n",
    "\n",
    "RINGKASAN DETAIL:\n",
    "{combined_resumes[:2000]}\n",
    "\n",
    "TUGAS ANALISIS:\n",
    "1. Buat RINGKASAN EKSEKUTIF yang mencakup:\n",
    "   - Poin utama dari topik ini (2-3 kalimat)\n",
    "   - Dampak langsung terhadap Kementerian Keuangan\n",
    "   - Signifikansi kebijakan atau tindakan yang diambil\n",
    "\n",
    "2. Buat 3-5 POIN ANALISIS yang mencakup:\n",
    "   - Implikasi kebijakan fiskal\n",
    "   - Dampak terhadap penerimaan/pengeluaran negara\n",
    "   - Respon stakeholder dan masyarakat\n",
    "   - Tindak lanjut yang diperlukan\n",
    "\n",
    "ATURAN:\n",
    "- Ringkasan maksimal 80 kata, padat dan informatif\n",
    "- Setiap poin analisis maksimal 30 kata\n",
    "- Fokus pada aspek Kemenkeu dan kebijakan fiskal\n",
    "- Gunakan bahasa profesional dan objektif\n",
    "\n",
    "FORMAT OUTPUT:\n",
    "RINGKASAN: [ringkasan eksekutif yang substansial dan informatif]\n",
    "POIN 1: [analisis dampak kebijakan]\n",
    "POIN 2: [analisis implikasi fiskal]\n",
    "POIN 3: [analisis respon stakeholder]\n",
    "POIN 4: [analisis tindak lanjut jika ada]\n",
    "\n",
    "CONTOH:\n",
    "RINGKASAN: Menkeu Purbaya memutuskan tidak menaikkan cukai rokok 2026 untuk melindungi industri dan pekerja, sambil fokus pada pemberantasan rokok ilegal yang merugikan negara triliunan rupiah melalui operasi masif dan pengawasan ketat\n",
    "POIN 1: Kebijakan cukai rokok tetap stabil menjaga stabilitas industri dan lapangan kerja\n",
    "POIN 2: Fokus beralih ke penindakan rokok ilegal untuk mengamankan penerimaan negara\n",
    "POIN 3: Operasi Bea Cukai intensif sita ratusan juta batang rokok ilegal\n",
    "POIN 4: Dukungan ekonom dan Kemenperin terhadap kebijakan yang seimbang\"\"\"\n",
    "\n",
    "    try:\n",
    "        ai_response = _call_ai_model(prompt, temperature=0.3, max_tokens=500)\n",
    "        \n",
    "        # Parse response\n",
    "        lines = ai_response.strip().split('\\n')\n",
    "        result = {\n",
    "            'ringkasan': '',\n",
    "            'poin_analisis': []\n",
    "        }\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith('RINGKASAN:'):\n",
    "                result['ringkasan'] = line.replace('RINGKASAN:', '').strip()\n",
    "            elif line.startswith('POIN'):\n",
    "                poin_text = re.sub(r'^POIN \\d+:', '', line).strip()\n",
    "                if poin_text:\n",
    "                    result['poin_analisis'].append(poin_text)\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating AI analysis for topic {topic}: {e}\")\n",
    "        return generate_fallback_topic_analysis(topic, news_list)\n",
    "\n",
    "def generate_fallback_topic_analysis(topic, news_list):\n",
    "    \"\"\"Generate analisis fallback tanpa AI dengan ringkasan yang lebih substansial\"\"\"\n",
    "    \n",
    "    # Gabungkan beberapa resume untuk ringkasan yang lebih kaya\n",
    "    all_resumes = []\n",
    "    all_dampaks = []\n",
    "    \n",
    "    for news in news_list[:3]:  # Ambil 3 berita pertama\n",
    "        resume = clean_text_safe(news.get('resume_ai', ''))\n",
    "        dampak = clean_text_safe(news.get('dampak_kemenkeu_ai', ''))\n",
    "        \n",
    "        if resume:\n",
    "            all_resumes.append(resume)\n",
    "        if dampak:\n",
    "            all_dampaks.append(dampak)\n",
    "    \n",
    "    # Buat ringkasan dari resume yang ada\n",
    "    if all_resumes:\n",
    "        combined_resume = \". \".join(all_resumes)\n",
    "        if len(combined_resume) > 150:\n",
    "            # Ambil 2 kalimat pertama\n",
    "            sentences = combined_resume.split('. ')[:2]\n",
    "            ringkasan = '. '.join(sentences)\n",
    "            if not ringkasan.endswith('.'):\n",
    "                ringkasan += '.'\n",
    "        else:\n",
    "            ringkasan = combined_resume\n",
    "    else:\n",
    "        ringkasan = f\"Pemantauan berita terkait {topic} menunjukkan perkembangan penting yang memerlukan perhatian Kementerian Keuangan.\"\n",
    "    \n",
    "    # Buat poin analisis yang lebih spesifik\n",
    "    poin_analisis = []\n",
    "    \n",
    "    # Analisis berdasarkan jumlah berita\n",
    "    if len(news_list) >= 5:\n",
    "        poin_analisis.append(f\"Topik {topic} mendapat perhatian media tinggi dengan {len(news_list)} berita\")\n",
    "    else:\n",
    "        poin_analisis.append(f\"Terdapat {len(news_list)} berita terkait {topic} yang memerlukan monitoring\")\n",
    "    \n",
    "    # Analisis dampak Kemenkeu\n",
    "    if all_dampaks:\n",
    "        dampak_counts = {}\n",
    "        for dampak in all_dampaks:\n",
    "            dampak_counts[dampak] = dampak_counts.get(dampak, 0) + 1\n",
    "        \n",
    "        most_common_dampak = max(dampak_counts.items(), key=lambda x: x[1])\n",
    "        if most_common_dampak[0].lower() == 'positif':\n",
    "            poin_analisis.append(\"Mayoritas berita menunjukkan dampak positif terhadap Kementerian Keuangan\")\n",
    "        elif most_common_dampak[0].lower() == 'negatif':\n",
    "            poin_analisis.append(\"Terdapat dampak negatif yang perlu diperhatikan Kementerian Keuangan\")\n",
    "        else:\n",
    "            poin_analisis.append(\"Dampak terhadap Kementerian Keuangan memerlukan evaluasi lebih lanjut\")\n",
    "    \n",
    "    # Tambahkan poin umum\n",
    "    poin_analisis.append(\"Memerlukan pemantauan berkelanjutan dari perspektif kebijakan fiskal\")\n",
    "    \n",
    "    # Pastikan maksimal 4 poin\n",
    "    poin_analisis = poin_analisis[:4]\n",
    "    \n",
    "    return {\n",
    "        'ringkasan': ringkasan,\n",
    "        'poin_analisis': poin_analisis\n",
    "    }\n",
    "\n",
    "def categorize_topics(topic_groups, topic_keywords):\n",
    "    \"\"\"Kategorikan topik menjadi ISU KEMENKEU vs ISU NASIONAL/INTERNASIONAL\"\"\"\n",
    "    kemenkeu_keywords = [\n",
    "        'Menkeu', 'Kementerian Keuangan', 'Pajak', 'Cukai', 'APBN', 'Fiskal', \n",
    "        'Bea Cukai', 'DJP', 'Purbaya', 'Sidak', 'Bank', 'Kredit', 'Ekonomi'\n",
    "    ]\n",
    "    \n",
    "    isu_kemenkeu = {}\n",
    "    isu_nasional = {}\n",
    "    \n",
    "    for topic, news_list in topic_groups.items():\n",
    "        # Cek apakah topik terkait Kemenkeu\n",
    "        is_kemenkeu = False\n",
    "        topic_upper = topic.upper()\n",
    "        \n",
    "        for keyword in kemenkeu_keywords:\n",
    "            if keyword.upper() in topic_upper:\n",
    "                is_kemenkeu = True\n",
    "                break\n",
    "        \n",
    "        # Jika tidak jelas dari nama topik, cek dari isi berita\n",
    "        if not is_kemenkeu and news_list:\n",
    "            sample_text = \" \".join([\n",
    "                clean_text_safe(news.get('judul_berita', '')).upper() + \" \" +\n",
    "                clean_text_safe(news.get('resume_ai', '')).upper()\n",
    "                for news in news_list[:3]\n",
    "            ])\n",
    "            \n",
    "            for keyword in kemenkeu_keywords:\n",
    "                if keyword.upper() in sample_text:\n",
    "                    is_kemenkeu = True\n",
    "                    break\n",
    "        \n",
    "        if is_kemenkeu:\n",
    "            isu_kemenkeu[topic] = news_list\n",
    "        else:\n",
    "            isu_nasional[topic] = news_list\n",
    "    \n",
    "    return isu_kemenkeu, isu_nasional\n",
    "\n",
    "def extract_narasumber(df_data):\n",
    "    \"\"\"Extract narasumber utama dari berita\"\"\"\n",
    "    success_data = df_data[df_data['analisis_status'] == 'ok']\n",
    "    \n",
    "    # Cari nama-nama yang sering muncul (kemungkinan narasumber)\n",
    "    all_text = \"\"\n",
    "    \n",
    "    for _, row in success_data.iterrows():\n",
    "        judul = clean_text_safe(row.get('judul_berita', ''))\n",
    "        resume = clean_text_safe(row.get('resume_ai', ''))\n",
    "        all_text += f\" {judul} {resume}\"\n",
    "    \n",
    "    # Cari nama-nama pejabat yang umum\n",
    "    known_officials = [\n",
    "        'Purbaya Yudhi Sadewa', 'Menkeu Purbaya', 'Menteri Keuangan',\n",
    "        'Dirjen Pajak', 'Dirjen Bea Cukai', 'Kepala Bappenas',\n",
    "        'Gubernur BI', 'Presiden Prabowo'\n",
    "    ]\n",
    "    \n",
    "    found_narasumber = []\n",
    "    for official in known_officials:\n",
    "        if official.upper() in all_text.upper():\n",
    "            found_narasumber.append(official)\n",
    "    \n",
    "    return found_narasumber[:3] if found_narasumber else [\"Belum ada narasumber\"]\n",
    "\n",
    "def generate_laporan_analisis_media(df_data, config):\n",
    "    \"\"\"Generate laporan analisis media lengkap\"\"\"\n",
    "    \n",
    "    # Load topic keywords\n",
    "    topic_keywords, _ = load_config_keywords()\n",
    "    \n",
    "    # Kelompokkan berita berdasarkan topik - HANYA yang cocok dengan keywords\n",
    "    topic_groups = group_news_by_topics(df_data, topic_keywords)\n",
    "    \n",
    "    # Jika tidak ada yang cocok dengan keywords, buat pesan informasi\n",
    "    if not topic_groups:\n",
    "        return f\"\"\"**Laporan Analisis Media Online dan Media Sosial**\n",
    "{format_indonesian_datetime()}\n",
    "\n",
    "**EXECUTIVE SUMMARY**\n",
    "==================================================\n",
    "Periode pemantauan ini tidak menemukan berita yang sesuai dengan topic keywords yang telah ditentukan dalam config.json.\n",
    "Mungkin perlu review atau penyesuaian keywords untuk menangkap lebih banyak berita yang relevan.\n",
    "\n",
    "**MEDIA ONLINE**\n",
    "**Topik Berita:** Tidak ada topik yang cocok dengan keywords\n",
    "**Tonasi Berita:** -\n",
    "\n",
    "**Kegiatan yang dirujuk:** Pemantauan Berkelanjutan\n",
    "**Narasumber utama yang dirujuk:** Belum ada narasumber\n",
    "\n",
    "Silakan periksa kembali topic_keywords di config.json atau data berita yang tersedia.\"\"\"\n",
    "    \n",
    "    # Analisis sentimen per topik\n",
    "    topic_sentiments = analyze_sentiment_by_topic(topic_groups)\n",
    "    \n",
    "    # Kategorikan topik\n",
    "    isu_kemenkeu, isu_nasional = categorize_topics(topic_groups, topic_keywords)\n",
    "    \n",
    "    # Extract narasumber\n",
    "    narasumber_list = extract_narasumber(df_data)\n",
    "    \n",
    "    # Build laporan\n",
    "    lines = []\n",
    "    \n",
    "    # ===== HEADER ===== #\n",
    "    lines.append(\"===== Page 1 =====\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"**Laporan Analisis Media Online dan Media Sosial**\")\n",
    "    lines.append(format_indonesian_datetime())\n",
    "    lines.append(\"\")\n",
    "    \n",
    "    # ===== EXECUTIVE SUMMARY ===== #\n",
    "    lines.append(\"**EXECUTIVE SUMMARY**\")\n",
    "    lines.append(\"=\" * 50)\n",
    "    \n",
    "    # Generate executive summary dengan AI jika tersedia\n",
    "    total_berita = len(df_data[df_data['analisis_status'] == 'ok'])\n",
    "    total_relevan = sum(len(news_list) for news_list in topic_groups.values())\n",
    "    main_topics = list(topic_groups.keys())[:3]\n",
    "    \n",
    "    lines.append(f\"Periode pemantauan media online menunjukkan {total_berita} berita penting yang berhasil dianalisis.\")\n",
    "    lines.append(f\"Dari jumlah tersebut, {total_relevan} berita sesuai dengan topik yang dipantau: {', '.join(main_topics)}.\")\n",
    "    \n",
    "    if isu_kemenkeu:\n",
    "        lines.append(\"Fokus pemerintah terutama pada transparansi dan pengawasan sektor keuangan.\")\n",
    "    \n",
    "    lines.append(\"Pernyataan dan kebijakan penting dari pejabat terkait terus dipantau secara intensif.\")\n",
    "    lines.append(\"\")\n",
    "    \n",
    "    # ===== MEDIA ONLINE ===== #\n",
    "    lines.append(\"**MEDIA ONLINE**\")\n",
    "    lines.append(\"\")\n",
    "    \n",
    "    # Topik Berita\n",
    "    lines.append(f\"**Topik Berita:** {', '.join(topic_groups.keys())}\")\n",
    "    \n",
    "    # Tonasi Berita Overall - FIXED VERSION\n",
    "    all_sentiments = []\n",
    "    for sentiment_data in topic_sentiments.values():\n",
    "        for sentiment, count in sentiment_data['distribution'].items():\n",
    "            # Extend dengan string sentimen sebanyak count-nya\n",
    "            all_sentiments.extend([sentiment] * count)\n",
    "            \n",
    "    sentiment_counter = Counter(all_sentiments) if all_sentiments else Counter(['Netral'])\n",
    "    dominant_sentiment = sentiment_counter.most_common(1)[0][0] if sentiment_counter else 'Netral'\n",
    "    lines.append(f\"**Tonasi Berita:** {dominant_sentiment}\")\n",
    "    lines.append(\"\")\n",
    "    \n",
    "    # ===== PESAN KUNCI DAN ANALISIS ===== #\n",
    "    lines.append(\"**Pesan Kunci dan Analisis:**\")\n",
    "    lines.append(\"\")\n",
    "    \n",
    "    # ISU KEMENKEU\n",
    "    if isu_kemenkeu:\n",
    "        lines.append(\"**ISU KEMENKEU**\")\n",
    "        for i, (topic, news_list) in enumerate(isu_kemenkeu.items(), 1):\n",
    "            # Generate AI analysis untuk topik ini\n",
    "            analysis = generate_topic_analysis_with_ai(topic, news_list, topic_keywords)\n",
    "            \n",
    "            lines.append(f\"{i}. **{topic}**\")\n",
    "            lines.append(f\"   Ringkasan: {analysis['ringkasan']}\")\n",
    "            for j, poin in enumerate(analysis['poin_analisis'], 1):\n",
    "                lines.append(f\"   - {poin}\")\n",
    "            lines.append(\"\")\n",
    "    \n",
    "    # ISU NASIONAL DAN INTERNASIONAL\n",
    "    if isu_nasional:\n",
    "        lines.append(\"**ISU NASIONAL DAN INTERNASIONAL**\")\n",
    "        for i, (topic, news_list) in enumerate(isu_nasional.items(), 1):\n",
    "            # Generate AI analysis untuk topik ini\n",
    "            analysis = generate_topic_analysis_with_ai(topic, news_list, topic_keywords)\n",
    "            \n",
    "            lines.append(f\"{i}. **{topic}**\")\n",
    "            lines.append(f\"   Ringkasan: {analysis['ringkasan']}\")\n",
    "            for j, poin in enumerate(analysis['poin_analisis'], 1):\n",
    "                lines.append(f\"   - {poin}\")\n",
    "            lines.append(\"\")\n",
    "    \n",
    "    # Jika tidak ada isu kemenkeu atau nasional, beri informasi\n",
    "    if not isu_kemenkeu and not isu_nasional:\n",
    "        lines.append(\"**ISU KEMENKEU**\")\n",
    "        lines.append(\"Tidak ada berita yang cocok dengan kategori isu Kemenkeu pada periode ini.\")\n",
    "        lines.append(\"\")\n",
    "        lines.append(\"**ISU NASIONAL DAN INTERNASIONAL**\")\n",
    "        lines.append(\"Tidak ada berita yang cocok dengan kategori isu nasional/internasional pada periode ini.\")\n",
    "        lines.append(\"\")\n",
    "    \n",
    "    # ===== KEGIATAN & NARASUMBER ===== #\n",
    "    lines.append(\"**Kegiatan yang dirujuk:** Kegiatan Baru, Pemantauan Berkelanjutan\")\n",
    "    lines.append(f\"**Narasumber utama yang dirujuk:** {', '.join(narasumber_list)}\")\n",
    "    lines.append(\"\")\n",
    "    \n",
    "    # ===== DAFTAR BERITA ===== #\n",
    "    lines.append(\"===== Page 2 =====\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"**Daftar Berita:**\")\n",
    "    \n",
    "    # Hanya tampilkan berita yang masuk dalam topic groups\n",
    "    displayed_count = 0\n",
    "    for topic, news_list in topic_groups.items():\n",
    "        for news in news_list:\n",
    "            displayed_count += 1\n",
    "            judul = clean_text_safe(news.get('judul_berita', 'Judul tidak tersedia'))\n",
    "            url = clean_text_safe(news.get('url_berita', news.get('link', '#')))\n",
    "            \n",
    "            lines.append(f\"{displayed_count}. {judul}\")\n",
    "            if url and url != '#':\n",
    "                lines.append(f\"[{url}]\")\n",
    "            lines.append(\"\")\n",
    "    \n",
    "    if displayed_count == 0:\n",
    "        lines.append(\"Tidak ada berita yang sesuai dengan topic keywords untuk ditampilkan.\")\n",
    "    \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def save_laporan_analisis(content, filename_prefix=\"laporan_analisis_media\"):\n",
    "    \"\"\"Simpan laporan analisis ke file txt\"\"\"\n",
    "    output_dir = Path(\"00_laporan_cetak\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{filename_prefix}_{timestamp}.txt\"\n",
    "    filepath = output_dir / filename\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8', errors='ignore') as f:\n",
    "        f.write(content)\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "# ===== EKSEKUSI SEL 4 ===== #\n",
    "print(\"=== GENERATOR LAPORAN ANALISIS MEDIA (FIXED VERSION) ===\")\n",
    "\n",
    "# Check data\n",
    "if 'analisis_berita_penting' not in globals() or analisis_berita_penting.empty:\n",
    "    print(\"‚ùå Data analisis_berita_penting tidak tersedia. Jalankan SEL 1 terlebih dahulu.\")\n",
    "else:\n",
    "    print(f\"üìä Data tersedia: {len(analisis_berita_penting)} berita\")\n",
    "    \n",
    "    # Load config dan analisis topik\n",
    "    topic_keywords, config = load_config_keywords()\n",
    "    print(f\"üìã Topic keywords dari config: {topic_keywords}\")\n",
    "    \n",
    "    # Preview pengelompokan topik\n",
    "    topic_groups = group_news_by_topics(analisis_berita_penting, topic_keywords)\n",
    "\n",
    "    if topic_groups:\n",
    "        print(f\"üè∑Ô∏è  Berita dikelompokkan dalam {len(topic_groups)} topik:\")\n",
    "        for topic, news_list in topic_groups.items():\n",
    "            print(f\"   - {topic}: {len(news_list)} berita\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Tidak ada berita yang cocok dengan topic keywords dari config.json\")\n",
    "        print(\"   Laporan akan dibuat dengan informasi bahwa tidak ada topik yang cocok\")\n",
    "    \n",
    "    # Generate laporan lengkap\n",
    "    try:\n",
    "        laporan_content = generate_laporan_analisis_media(analisis_berita_penting, config)\n",
    "        \n",
    "        # Simpan file\n",
    "        saved_file = save_laporan_analisis(laporan_content, \"laporan_analisis_media\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Laporan Analisis Media disimpan di: {saved_file}\")\n",
    "        print(f\"üìÑ Total baris: {len(laporan_content.splitlines())}\")\n",
    "        \n",
    "        # Preview hasil (20 baris pertama)\n",
    "        preview_lines = laporan_content.splitlines()[:20]\n",
    "        print(f\"\\nüìã Preview Laporan Analisis Media:\")\n",
    "        print(\"-\" * 70)\n",
    "        for line in preview_lines:\n",
    "            print(line)\n",
    "        if len(laporan_content.splitlines()) > 20:\n",
    "            print(\"...\")\n",
    "            print(f\"[{len(laporan_content.splitlines()) - 20} baris lainnya]\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating laporan analisis: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(f\"\\nüéØ Selesai! File tersedia di: 00_laporan_cetak/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3c5421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output path /Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Pekerjaan BMN/05. 2025/98_monitoring_berita/monitoring-berita/00_laporan_cetak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ccca302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEMO FUNGSI HELPER CONFIG PATH ===\n",
      "‚úÖ Path berita penting ditemukan: 00_hasil_analisis/berita_penting/analisis_berita_penting_deepseek_20251002_072918.csv\n",
      "üìä File berhasil dibaca: 40 baris\n",
      "üìã Kolom tersedia: ['judul_berita', 'source_domain', 'url_berita', 'tanggal_berita_norm', 'kategori_isu']...\n",
      "‚úÖ Berita berhasil dianalisis: 40\n",
      "\n",
      "üìã Info config berita penting:\n",
      "   Relative path: 00_hasil_analisis/berita_penting/analisis_berita_penting_deepseek_20251002_072918.csv\n",
      "   Absolute path: /Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Pekerjaan BMN/05. 2025/98_monitoring_berita/monitoring-berita/00_hasil_analisis/berita_penting/analisis_berita_penting_deepseek_20251002_072918.csv\n",
      "   Output dir: 00_hasil_analisis/berita_penting\n",
      "   Last updated: 2025-10-02 07:29:18\n",
      "\n",
      "üí° Keunggulan sistem path fleksibel:\n",
      "   1. Otomatis update setiap kali SEL 1 dijalankan\n",
      "   2. Path relatif tetap bekerja jika folder dipindah\n",
      "   3. Fallback ke absolute path jika diperlukan\n",
      "   4. Backup config otomatis sebelum update\n",
      "   5. Compatible dengan berbagai sistem operasi\n"
     ]
    }
   ],
   "source": [
    "# DEMO: Fungsi Helper untuk Membaca Path Berita Penting dari Config\n",
    "# Contoh penggunaan fungsi get_berita_penting_path_from_config()\n",
    "\n",
    "print(\"=== DEMO FUNGSI HELPER CONFIG PATH ===\")\n",
    "\n",
    "# Fungsi helper yang telah dibuat di SEL 1\n",
    "def get_berita_penting_path_from_config():\n",
    "    \"\"\"\n",
    "    Fungsi helper untuk membaca path berita penting dari config\n",
    "    Otomatis resolve relative path berdasarkan cwd saat ini\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open('config.json', 'r', encoding='utf-8') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        # Coba gunakan relative path dulu\n",
    "        if 'berita_penting_output' in config:\n",
    "            relative_path = Path(config['berita_penting_output'])\n",
    "            if relative_path.exists():\n",
    "                return str(relative_path)\n",
    "        \n",
    "        # Jika relative path tidak ada, coba absolute path\n",
    "        if 'berita_penting_output_absolute' in config:\n",
    "            absolute_path = Path(config['berita_penting_output_absolute'])\n",
    "            if absolute_path.exists():\n",
    "                return str(absolute_path)\n",
    "        \n",
    "        # Jika tidak ada, return None\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Cannot read berita penting path from config: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test fungsi helper\n",
    "berita_path = get_berita_penting_path_from_config()\n",
    "\n",
    "if berita_path:\n",
    "    print(f\"‚úÖ Path berita penting ditemukan: {berita_path}\")\n",
    "    \n",
    "    # Coba baca file untuk verifikasi\n",
    "    try:\n",
    "        df_test = pd.read_csv(berita_path)\n",
    "        print(f\"üìä File berhasil dibaca: {len(df_test)} baris\")\n",
    "        print(f\"üìã Kolom tersedia: {list(df_test.columns)[:5]}...\")  # Show first 5 columns\n",
    "        \n",
    "        # Cek data yang berhasil dianalisis\n",
    "        success_count = (df_test['analisis_status'] == 'ok').sum() if 'analisis_status' in df_test.columns else 0\n",
    "        print(f\"‚úÖ Berita berhasil dianalisis: {success_count}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error membaca file: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Path berita penting tidak ditemukan di config\")\n",
    "\n",
    "# Tampilkan informasi config terkait\n",
    "try:\n",
    "    with open('config.json', 'r', encoding='utf-8') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    print(f\"\\nüìã Info config berita penting:\")\n",
    "    print(f\"   Relative path: {config.get('berita_penting_output', 'Not set')}\")\n",
    "    print(f\"   Absolute path: {config.get('berita_penting_output_absolute', 'Not set')}\")\n",
    "    print(f\"   Output dir: {config.get('berita_penting_output_dir', 'Not set')}\")\n",
    "    print(f\"   Last updated: {config.get('berita_penting_last_updated', 'Never')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error reading config: {e}\")\n",
    "\n",
    "print(f\"\\nüí° Keunggulan sistem path fleksibel:\")\n",
    "print(f\"   1. Otomatis update setiap kali SEL 1 dijalankan\")\n",
    "print(f\"   2. Path relatif tetap bekerja jika folder dipindah\") \n",
    "print(f\"   3. Fallback ke absolute path jika diperlukan\")\n",
    "print(f\"   4. Backup config otomatis sebelum update\")\n",
    "print(f\"   5. Compatible dengan berbagai sistem operasi\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
