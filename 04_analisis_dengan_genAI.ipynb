{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nl15yqDXsRcB"
   },
   "source": [
    "# SETTING ENVIRONMENT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1535,
     "status": "ok",
     "timestamp": 1758531769359,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "f8BqutX0SOd1",
    "outputId": "03f6a941-b429-4c40-f4b5-bb99b01094e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# mount the colab with google drive\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# mount the colab with google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1758531769362,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "eLOguPRQSRLN"
   },
   "outputs": [],
   "source": [
    "# set folder tempat kerja (current working directory)\n",
    "import os\n",
    "cwd = \"/Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Pekerjaan BMN/05. 2025/98_monitoring_berita\"\n",
    "#cwd = '/content/drive/MyDrive/Monitoring Berita'\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryLq4OOjcbct"
   },
   "source": [
    "# PROSES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuBGZ0SWcdze"
   },
   "source": [
    "## Baca Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 404,
     "status": "ok",
     "timestamp": 1758531769767,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "ixpesHsMcft1"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "cfg_path = f\"{cwd}/config.json\"\n",
    "CONFIG_PATH = Path(cfg_path)\n",
    "\n",
    "with open(CONFIG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            cfg = json.load(f)\n",
    "\n",
    "INPUT_EXCEL_PATH = cfg[\"labelled_data_xlsx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 621,
     "status": "ok",
     "timestamp": 1758531770390,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "-frgVDKndDIP",
    "outputId": "4dd7778d-dbfa-4d0b-814b-109450f849cc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_input = pd.read_excel(INPUT_EXCEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1758531770402,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "2szjpaAadl1k"
   },
   "outputs": [],
   "source": [
    "df = df_input[['judul_berita', 'source_domain', 'url_berita', 'tanggal_berita_norm', 'kategori_isu',\n",
    "               'artikel_berita_bersih']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1758531770449,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "L7C6wVGSjaWU"
   },
   "outputs": [],
   "source": [
    "df = df[df['kategori_isu'] != \"Isu Lainnya\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test untuk 10 data\n",
    "df = df.sample(10, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPK0SVVpx833"
   },
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1758531770451,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "3SRsmnoox8mS"
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 1. Import & setup\n",
    "# ==============================\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# Setup logging agar progress terlihat\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    ")\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()  # memungkinkan .progress_apply di pandas jika diperlukan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1088,
     "status": "ok",
     "timestamp": 1758531771539,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "EAURwUPSzTb1",
    "outputId": "d5dfddcd-57ca-42ca-9b1d-bffd42ab18b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API keys - Deepseek: OK | OpenAI: OK\n"
     ]
    }
   ],
   "source": [
    "# Load API keys from mac environment variables (and ensure they are available to HTTP clients)\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "DEEPSEEK_API_KEY = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Optional: try to read from config.json if env vars are missing\n",
    "if not DEEPSEEK_API_KEY or not OPENAI_API_KEY:\n",
    "    cfg_file = Path(\"config.json\")\n",
    "    if cfg_file.exists():\n",
    "        try:\n",
    "            with open(cfg_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                cfg_local = json.load(f)\n",
    "            DEEPSEEK_API_KEY = DEEPSEEK_API_KEY or cfg_local.get(\"deepseek_api_key\") or cfg_local.get(\"deepseek_api\")\n",
    "            OPENAI_API_KEY = OPENAI_API_KEY or cfg_local.get(\"openai_api_key\") or cfg_local.get(\"openai_api\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# Ensure os.environ is populated for downstream HTTP calls\n",
    "if DEEPSEEK_API_KEY:\n",
    "    os.environ[\"DEEPSEEK_API_KEY\"] = DEEPSEEK_API_KEY\n",
    "if OPENAI_API_KEY:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "print(\n",
    "    \"API keys - Deepseek:\", \"OK\" if os.getenv(\"DEEPSEEK_API_KEY\") else \"MISSING\",\n",
    "    \"| OpenAI:\", \"OK\" if os.getenv(\"OPENAI_API_KEY\") else \"MISSING\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1758531771602,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "4uaCaOUUyANt"
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 2. Fungsi cleaning teks\n",
    "# ==============================\n",
    "\n",
    "def normalize_whitespace(text: str) -> str:\n",
    "    \"\"\"Normalisasi whitespace: hapus spasi ganda, spasi di pinggir, newline berlebih.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def strip_non_text_noise(text: str) -> str:\n",
    "    \"\"\"Hilangkan pola umum yang bukan isi artikel (disclaimer, navigasi, dsb).\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = re.sub(r\"^\\s*[-•\\u2022]\\s*\", \"\", text, flags=re.M)\n",
    "    noise_patterns = [\n",
    "        r\"Bagikan:\\s*Facebook.*?$\",\n",
    "        r\"Follow\\s+kami.*?$\",\n",
    "        r\"Editor:\\s.*?$\",\n",
    "        r\"Penulis:\\s.*?$\",\n",
    "    ]\n",
    "    for pat in noise_patterns:\n",
    "        text = re.sub(pat, \"\", text, flags=re.I | re.M)\n",
    "    return text\n",
    "\n",
    "def final_cleanup(text: str, min_line_len: int = 3) -> str:\n",
    "    \"\"\"Pembersihan akhir: normalisasi whitespace, buang baris terlalu pendek.\"\"\"\n",
    "    text = normalize_whitespace(text)\n",
    "    lines = [ln for ln in text.splitlines() if len(ln.strip()) >= min_line_len]\n",
    "    return \"\\n\".join(lines).strip()\n",
    "\n",
    "def clean_article(text: str) -> str:\n",
    "    \"\"\"Pipeline cleaning lengkap.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = strip_non_text_noise(text)\n",
    "    text = final_cleanup(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 3. Prompt & parsing JSON\n",
    "# ==============================\n",
    "\n",
    "# System prompt untuk membatasi gaya output LLM agar selalu JSON valid\n",
    "SYSTEM_PROMPT = (\n",
    "\"Anda adalah asisten analisis berita.  \"\n",
    "\"Keluarkan **hanya** JSON valid sesuai skema yang ditentukan, **tanpa penjelasan tambahan**.\"  \n",
    "\"Jika ada bagian yang tidak dapat ditentukan, isilah dengan string kosong `\"\"`.\"\n",
    ")\n",
    "\n",
    "USER_TEMPLATE = \"\"\"TUGAS:\n",
    "TUGAS:\n",
    "1) Tentukan nilai **\"topik\"** (pilih satu dari kategori berikut):\n",
    "   - Kemenkeu  \n",
    "   - Nasional  \n",
    "   - Internasional  \n",
    "   - Lainnya  \n",
    "\n",
    "   “Kemenkeu” digunakan jika berita berkaitan langsung dengan tugas/fungsi Kementerian Keuangan RI.  \n",
    "   “Nasional” untuk kebijakan pusat selain Kemenkeu.  \n",
    "   “Internasional” untuk hubungan luar negeri atau isu global.  \n",
    "   “Lainnya” untuk kategori yang tidak sesuai ketiganya (misalnya olahraga, hiburan, dsb.).\n",
    "\n",
    "2) Tentukan **\"subtopik\"**: frasa singkat yang menggambarkan isi pokok berita (misalnya “reformasi pajak”, “utang luar negeri”, “kerjasama bilateral”, “skandal korupsi”, dsb.).\n",
    "\n",
    "3) Tentukan **\"sentimen\"** (pilih salah satu):  \n",
    "   - positif  \n",
    "   - netral  \n",
    "   - negatif  \n",
    "\n",
    "4) Beri **\"alasan_sentimen\"**: penjelasan singkat (maksimal 2 kalimat) kenapa sentimen tersebut sesuai.\n",
    "\n",
    "5) Temukan **\"poin_of_interest\"**: nama pejabat / public figure (dengan jabatan dan gelar jika ada) yang menjadi fokus dalam berita (jika ada).\n",
    "\n",
    "6) Ambil **\"statement_pejabat\"**: **semua kutipan** langsung dari berita, persis seperti tertulis (jangan diubah atau ringkas).\n",
    "\n",
    "7) Beri **\"confidence\"**: angka antara 0.0 hingga 1.0 yang menunjukkan tingkat kepastianmu terhadap hasil tersebut.\n",
    "\n",
    "ATURAN-SENTIMEN TAMBAHAN (jika relevan):\n",
    "- Kata “utang” atau “defisit” dalam konteks pembiayaan → **positif**  \n",
    "- “Anjing pelacak” → **positif** (jika digunakan dalam konteks penegakan hukum)  \n",
    "- “Penyelundupan” + konteks Bea Cukai / DJBC → **positif**  \n",
    "- “Penggelapan pajak” + DJP / Direktorat Jenderal Pajak → **positif**  \n",
    "- “Pajak” atau “cukai” tanpa konteks penindakan → **netral**  \n",
    "- “Korupsi” + KPK / Komisi Pemberantasan Korupsi → **positif**  \n",
    "- “Strategi penyerapan anggaran yang efektif” → **positif**  \n",
    "\n",
    "FORMAT OUTPUT HARUS JSON (tanpa tambahan teks apa pun), contoh:\n",
    "\n",
    "\n",
    "Output HARUS JSON:\n",
    "{{\n",
    "  \"topik\": \"Kemenkeu|Nasional|Internasional|Lainnya\",\n",
    "  \"subtopik\": \"string\",\n",
    "  \"sentimen\": \"positif|netral|negatif\",\n",
    "  \"alasan_sentimen\": \"string\",\n",
    "  \"poin_of_interest\": \"string\",\n",
    "  \"statement_pejabat\": \"string\",\n",
    "  \"confidence\": 0.0\n",
    "}}\n",
    "\n",
    "TEKS_BERITA:\n",
    "\"{text}\"\n",
    "\"\"\"\n",
    "\n",
    "def parse_first_json_block(s: str) -> Dict[str, Any]:\n",
    "    \"\"\"Ambil JSON {...} pertama dari string dan parse jadi dict.\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        raise ValueError(\"Response bukan string.\")\n",
    "    m = re.search(r\"\\{.*\\}\", s, flags=re.S)\n",
    "    if not m:\n",
    "        raise ValueError(\"Tidak ditemukan blok JSON dalam respons.\")\n",
    "    snippet = m.group(0)\n",
    "    try:\n",
    "        return json.loads(snippet)\n",
    "    except json.JSONDecodeError:\n",
    "        fixed = snippet.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"'\")\n",
    "        return json.loads(fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1758531771606,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "uFWooQPvzyO0"
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 4. Provider: Mock, OpenAI, DeepSeek\n",
    "# ==============================\n",
    "\n",
    "# Optimasi DeepSeek:\n",
    "# - Reuse HTTP session (keep-alive)\n",
    "# - Turunkan max_tokens karena output JSON kecil\n",
    "# - Siapkan fungsi helper agar mudah ganti parameter\n",
    "\n",
    "_DEEPSEEK_SESSION = None\n",
    "\n",
    "def _get_deepseek_session():\n",
    "    global _DEEPSEEK_SESSION\n",
    "    if _DEEPSEEK_SESSION is None:\n",
    "        import requests\n",
    "        _DEEPSEEK_SESSION = requests.Session()\n",
    "    return _DEEPSEEK_SESSION\n",
    "\n",
    "\n",
    "def call_mock_classifier(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"Classifier dummy untuk demo/offline.\"\"\"\n",
    "    t = (text or \"\").lower()\n",
    "    if any(k in t for k in [\"apbn\", \"pajak\", \"kemenkeu\", \"sri mulyani\", \"bea cukai\"]):\n",
    "        topik = \"Kemenkeu\"\n",
    "    elif any(k in t for k in [\"presiden\", \"dpr\", \"kpk\", \"polri\", \"politik\"]):\n",
    "        topik = \"Nasional\"\n",
    "    else:\n",
    "        topik = \"Lainnya\"\n",
    "\n",
    "    if any(k in t for k in [\"skandal\", \"korupsi\", \"anjlok\", \"utang\"]):\n",
    "        sentimen = \"negatif\"; alasan = \"Berita bernuansa kritik atau masalah.\"\n",
    "    elif any(k in t for k in [\"subsidi\", \"bantuan\", \"pemulihan\", \"peningkatan\"]):\n",
    "        sentimen = \"positif\"; alasan = \"Berita menekankan capaian atau dukungan.\"\n",
    "    else:\n",
    "        sentimen = \"netral\"; alasan = \"Berita informatif tanpa penilaian.\"\n",
    "\n",
    "    return {\n",
    "        \"topik\": topik,\n",
    "        \"subtopik\": \"\",\n",
    "        \"sentimen\": sentimen,\n",
    "        \"alasan_sentimen\": alasan,\n",
    "        \"confidence\": 0.6,\n",
    "    }\n",
    "\n",
    "\n",
    "def call_openai(messages: List[Dict[str, str]], model: str = \"gpt-5-mini\") -> str:\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "    except ImportError as e:\n",
    "        raise RuntimeError(\"Library openai belum terpasang. Install dengan: pip install openai\") from e\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"OPENAI_API_KEY tidak ditemukan di environment.\")\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.0,\n",
    "        max_completion_tokens=400,  # cukup untuk JSON kecil\n",
    "        top_p=1,\n",
    "    )\n",
    "    return resp.choices[0].message.content or \"\"\n",
    "\n",
    "\n",
    "def call_deepseek(messages: List[Dict[str, str]], model: str = \"deepseek-chat\") -> str:\n",
    "    import requests\n",
    "    import math\n",
    "    api_key = os.getenv('DEEPSEEK_API_KEY','')\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"DEEPSEEK_API_KEY tidak ditemukan di environment.\")\n",
    "\n",
    "    # Heuristik: output JSON pendek, jadi max_tokens tidak perlu besar (hemat & cepat)\n",
    "    # Jika prompt panjang (>5k chars) naikkan sedikit.\n",
    "    user_content = \"\".join(m.get(\"content\",\"\") for m in messages if m.get(\"role\")=='user')\n",
    "    base_max = 600\n",
    "    if len(user_content) > 6000:\n",
    "        base_max = 800\n",
    "    elif len(user_content) > 3000:\n",
    "        base_max = 450\n",
    "\n",
    "    url = \"https://api.deepseek.com/chat/completions\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": base_max,\n",
    "        \"top_p\": 1,\n",
    "    }\n",
    "    session = _get_deepseek_session()\n",
    "    # Timeout lebih agresif agar cepat gagal & bisa retry di wrapper\n",
    "    r = session.post(url, headers=headers, data=json.dumps(payload), timeout=45)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    try:\n",
    "        return data[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except Exception:\n",
    "        raise RuntimeError(f\"Format respons DeepSeek tidak terduga: {data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1758531771609,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "aPJtApuAyDYD"
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 5. Klasifikasi wrapper (retry & fallback) + Model Registry\n",
    "# ==============================\n",
    "\n",
    "# Registry untuk memudahkan penambahan provider baru\n",
    "MODEL_REGISTRY = {\n",
    "    \"mock\": {\n",
    "        \"default_model\": \"mock-local\",\n",
    "        \"caller\": lambda messages, model: json.dumps(call_mock_classifier(messages[-1]['content']) if messages else {})\n",
    "    },\n",
    "    \"openai\": {\n",
    "        \"default_model\": \"gpt-4o-mini\",  # mudah diganti nanti\n",
    "        \"caller\": lambda messages, model: call_openai(messages, model=model)\n",
    "    },\n",
    "    \"deepseek\": {\n",
    "        \"default_model\": \"deepseek-chat\",\n",
    "        \"caller\": lambda messages, model: call_deepseek(messages, model=model)\n",
    "    },\n",
    "}\n",
    "\n",
    "def get_provider_config(provider: str):\n",
    "    prov = (provider or \"mock\").lower()\n",
    "    if prov not in MODEL_REGISTRY:\n",
    "        logging.warning(f\"Provider '{provider}' tidak dikenal. Fallback ke 'mock'.\")\n",
    "        prov = \"mock\"\n",
    "    return prov, MODEL_REGISTRY[prov]\n",
    "\n",
    "\n",
    "def classify_with_llm(\n",
    "    text: str,\n",
    "    provider: str = \"mock\",\n",
    "    model: str | None = None,\n",
    "    max_chars: int = 6000,\n",
    "    retries: int = 2,\n",
    "    backoff: float = 1.0,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Wrapper klasifikasi teks -> dict hasil klasifikasi.\n",
    "\n",
    "    Parameter:\n",
    "    - provider: kunci provider (mock|openai|deepseek|... di masa depan)\n",
    "    - model: override nama model; jika None pakai default_model dari registry\n",
    "    \"\"\"\n",
    "    cleaned = (text or \"\").strip()\n",
    "    if not cleaned:\n",
    "        return {\"topik\":\"Lainnya\",\"subtopik\":\"\",\"sentimen\":\"netral\",\"alasan_sentimen\":\"Teks kosong.\",\"confidence\":0.0}\n",
    "\n",
    "    cleaned = cleaned[:max_chars]\n",
    "    user_msg = USER_TEMPLATE.format(text=cleaned)\n",
    "    messages = [\n",
    "        {\"role\":\"system\",\"content\":SYSTEM_PROMPT},\n",
    "        {\"role\":\"user\",\"content\":user_msg},\n",
    "    ]\n",
    "\n",
    "    prov_key, prov_cfg = get_provider_config(provider)\n",
    "    model_name = model or prov_cfg[\"default_model\"]\n",
    "\n",
    "    if prov_key == \"mock\":\n",
    "        # langsung pakai fungsi mock -> hasil dict\n",
    "        return call_mock_classifier(cleaned)\n",
    "\n",
    "    caller = prov_cfg[\"caller\"]\n",
    "    last_err = None\n",
    "    for attempt in range(retries + 1):\n",
    "        try:\n",
    "            raw = caller(messages, model_name)\n",
    "            # Jika provider bukan mock, raw biasanya string; pastikan parse\n",
    "            if isinstance(raw, dict):  # kalau ada provider lain mengembalikan dict langsung\n",
    "                parsed = raw\n",
    "            else:\n",
    "                parsed = parse_first_json_block(str(raw))\n",
    "            return {\n",
    "                \"topik\": parsed.get(\"topik\",\"Lainnya\"),\n",
    "                \"subtopik\": parsed.get(\"subtopik\",\"\"),\n",
    "                \"sentimen\": parsed.get(\"sentimen\",\"netral\"),\n",
    "                \"alasan_sentimen\": parsed.get(\"alasan_sentimen\",\"\"),\n",
    "                \"poin_of_interest\": parsed.get(\"poin_of_interest\",\"\"),\n",
    "                \"statement_pejabat\": parsed.get(\"statement_pejabat\",\"\"),\n",
    "                \"confidence\": float(parsed.get(\"confidence\",0.0)),\n",
    "            }\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            logging.warning(f\"Attempt {attempt+1} provider={prov_key} gagal: {e}\")\n",
    "            time.sleep(backoff * (2 ** attempt))\n",
    "\n",
    "    # fallback ke mock kalau semua gagal\n",
    "    fb = call_mock_classifier(cleaned)\n",
    "    fb[\"alasan_sentimen\"] += f\" (fallback error: {last_err})\"\n",
    "    return fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1758531771623,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "5Xsv14wLyF_R"
   },
   "outputs": [],
   "source": [
    "def run_pipeline_on_df(df: pd.DataFrame,\n",
    "                       text_col: str = \"artikel_berita\",\n",
    "                       title_col: str = \"judul_berita\",\n",
    "                       provider: str = \"mock\",\n",
    "                       sleep_sec: float = 0.0,\n",
    "                       show_progress: bool = True,\n",
    "                       classifier=None,\n",
    "                       parallel: bool = True,\n",
    "                       max_workers: int = 4,\n",
    "                       cache_enabled: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pipeline lengkap: bersihkan teks -> klasifikasi (LLM/mock) -> tambah kolom hasil.\n",
    "\n",
    "    Optimasi tambahan:\n",
    "    - parallel: jika True dan provider deepseek/openai maka gunakan ThreadPoolExecutor\n",
    "      (I/O bound HTTP) dengan jumlah worker terbatas.\n",
    "    - cache_enabled: aktifkan memoization berdasarkan hash teks untuk hindari panggilan ulang\n",
    "      pada artikel identik (misal duplikasi). Hash memakai first 300 chars + length.\n",
    "    - sleep_sec: jeda antar batch (bukan antar item) jika parallel.\n",
    "    \"\"\"\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "    import hashlib\n",
    "\n",
    "    # Siapkan classifier default\n",
    "    if classifier is None:\n",
    "        def _default_classifier(text: str):\n",
    "            try:\n",
    "                return classify_with_llm(text, provider=provider)\n",
    "            except NameError:\n",
    "                try:\n",
    "                    return call_mock_classifier(text)\n",
    "                except NameError:\n",
    "                    t = (text or \"\").lower()\n",
    "                    if any(k in t for k in [\"apbn\", \"pajak\", \"kemenkeu\", \"sri mulyani\", \"bea cukai\"]):\n",
    "                        topik = \"Kemenkeu\"\n",
    "                    elif any(k in t for k in [\"presiden\", \"dpr\", \"kpk\", \"polri\", \"politik\"]):\n",
    "                        topik = \"Nasional\"\n",
    "                    else:\n",
    "                        topik = \"Lainnya\"\n",
    "                    return {\n",
    "                        \"topik\": topik,\n",
    "                        \"subtopik\": \"\",\n",
    "                        \"sentimen\": \"netral\",\n",
    "                        \"alasan_sentimen\": \"Fallback inline classifier.\",\n",
    "                        \"confidence\": 0.0,\n",
    "                    }\n",
    "        classifier = _default_classifier\n",
    "\n",
    "    # Cache sederhana in-memory\n",
    "    cache: dict[str, dict] = {}\n",
    "\n",
    "    def _hash_text(s: str) -> str:\n",
    "        # cukup sebagian awal untuk speed, plus panjang untuk beda\n",
    "        head = s[:300]\n",
    "        return hashlib.sha1((head + str(len(s))).encode('utf-8')).hexdigest()\n",
    "\n",
    "    # Precompute cleaned text list\n",
    "    cleaned_rows: list[tuple[int, str]] = []\n",
    "    for idx, row in df.iterrows():\n",
    "        raw_text = str(row.get(text_col, \"\") or \"\")\n",
    "        if len(raw_text) < 100:\n",
    "            raw_text = str(row.get(title_col, \"\") or \"\")\n",
    "        cleaned = clean_article(raw_text)\n",
    "        cleaned_rows.append((idx, cleaned))\n",
    "\n",
    "    results_map: dict[int, dict] = {}\n",
    "\n",
    "    # Fungsi eksekusi satu item dengan cache\n",
    "    def _process(idx_cleaned: tuple[int, str]):\n",
    "        idx_i, cleaned_i = idx_cleaned\n",
    "        if cache_enabled:\n",
    "            h = _hash_text(cleaned_i)\n",
    "            if h in cache:\n",
    "                return idx_i, cache[h]\n",
    "        try:\n",
    "            res = classifier(cleaned_i)\n",
    "        except Exception as e:\n",
    "            res = {\n",
    "                \"topik\": \"Lainnya\",\n",
    "                \"subtopik\": \"\",\n",
    "                \"sentimen\": \"netral\",\n",
    "                \"alasan_sentimen\": f\"error: {str(e)[:200]}\",\n",
    "                \"confidence\": 0.0,\n",
    "            }\n",
    "        if cache_enabled:\n",
    "            cache[h] = res\n",
    "        return idx_i, res\n",
    "\n",
    "    use_parallel = parallel and provider.lower() in {\"deepseek\", \"openai\"} and len(cleaned_rows) > 1\n",
    "\n",
    "    if use_parallel:\n",
    "        iterator = cleaned_rows\n",
    "        if show_progress:\n",
    "            iterator = tqdm(cleaned_rows, total=len(cleaned_rows), desc=f\"Analisis berita (parallel {provider})\", unit=\"berita\", leave=True)\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "            future_map = {ex.submit(_process, item): item[0] for item in iterator}\n",
    "            for fut in as_completed(future_map):\n",
    "                idx_i, res = fut.result()\n",
    "                results_map[idx_i] = res\n",
    "        if sleep_sec > 0:\n",
    "            time.sleep(sleep_sec)\n",
    "    else:\n",
    "        iterator = cleaned_rows\n",
    "        if show_progress:\n",
    "            iterator = tqdm(cleaned_rows, total=len(cleaned_rows), desc=f\"Analisis berita ({provider})\", unit=\"berita\", leave=True)\n",
    "        for item in iterator:\n",
    "            idx_i, res = _process(item)\n",
    "            results_map[idx_i] = res\n",
    "            if sleep_sec > 0:\n",
    "                time.sleep(sleep_sec)\n",
    "\n",
    "    # Susun output DF ternormalisasi\n",
    "    output_records = []\n",
    "    for idx in df.index:\n",
    "        res = results_map.get(idx, {})\n",
    "        original_cleaned = next((c for i,c in cleaned_rows if i==idx), \"\")\n",
    "        output_records.append({\n",
    "            \"topik_llm\": res.get(\"topik\", \"Lainnya\"),\n",
    "            \"subtopik_llm\": res.get(\"subtopik\", \"\"),\n",
    "            \"sentimen\": res.get(\"sentimen\", \"netral\"),\n",
    "            \"alasan_sentimen\": res.get(\"alasan_sentimen\", \"\"),\n",
    "            \"poin_of_interest\": res.get(\"poin_of_interest\",\"\"),\n",
    "            \"statement_pejabat\": res.get(\"statement_pejabat\",\"\"),\n",
    "            \"confidence\": res.get(\"confidence\", 0.0),\n",
    "            \"text_len\": len(original_cleaned),\n",
    "        })\n",
    "\n",
    "    out_df = pd.DataFrame(output_records, index=df.index)\n",
    "    return pd.concat([df, out_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QIztsUe5wq8"
   },
   "source": [
    "# *OUTPUT*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcfPKqk9M0rH"
   },
   "source": [
    "## Eksekusi\n",
    "\n",
    "<!-- Catatan Optimasi -->\n",
    "Jika menggunakan provider DeepSeek, pipeline sekarang otomatis:\n",
    "- Menggunakan parallel threads (I/O bound) default 4 worker.\n",
    "- Reuse HTTP session (keep-alive) untuk mengurangi overhead koneksi.\n",
    "- Mengurangi max_tokens output (JSON kecil) agar respons lebih cepat.\n",
    "- Cache in-memory untuk teks identik (hindari panggilan ulang).\n",
    "\n",
    "Parameter baru di run_pipeline_on_df:\n",
    "- parallel (bool)\n",
    "- max_workers (int)\n",
    "- cache_enabled (bool)\n",
    "\n",
    "Bisa override misalnya:\n",
    "result_df = run_pipeline_on_df(df, provider=provider_selected, parallel=True, max_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'openai'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ambil parameter AI_name dari config\n",
    "provider_selected = cfg[\"AI_name\"]\n",
    "provider_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372,
     "referenced_widgets": [
      "e8666d4da8c0458483dfb292d701325a",
      "5ea695cb8e3e47918b3fe78ae400ebb9",
      "91d8d0fa421644af93abd8d5fe535210",
      "266b550acfda4378816101a6395c99f9",
      "62ae927c096545f99ec8a67a01b91bc2",
      "e09521dca283485fa61f19ccace17719",
      "7927dfe6ec564f59a7a4567b805b1f91",
      "c2c1449ac5e44696aef7b60ef3b91d1c",
      "8ec77c93f3da410b92a2884fcf3047d2",
      "a11482dc58194ba0b7d97941f399025b",
      "7ccd4b6485b54af483dc9911e6fb93b8"
     ]
    },
    "executionInfo": {
     "elapsed": 459177,
     "status": "error",
     "timestamp": 1758532230801,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "_HKLagSsM0dT",
    "outputId": "d4d05ab8-2885-4268-c2a3-2d0e51669ed7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provider yang dipakai: openai\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "820c6c422c5e42a791085c4f5709c4e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analisis berita (parallel openai):   0%|          | 0/10 [00:00<?, ?berita/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 16:34:09,545 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-28 16:34:10,711 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-28 16:34:11,193 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-28 16:34:11,577 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-28 16:34:13,550 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-28 16:34:14,714 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-28 16:34:14,986 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-28 16:34:18,157 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-28 16:34:18,159 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-28 16:34:18,160 | INFO | HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline selesai. Hasil disimpan di analisis_ai_20250928_openai_default.csv\n"
     ]
    }
   ],
   "source": [
    "# 1. Load data berita & jalankan pipeline\n",
    "\n",
    "# Ambil preferensi model jika ada di config\n",
    "custom_model = cfg.get(\"AI_model_override\") if isinstance(cfg, dict) else None\n",
    "print(\"Provider yang dipakai:\", provider_selected)\n",
    "\n",
    "# Set parameter paralel khusus deepseek (I/O lebih lambat) -> percepat total waktu\n",
    "parallel_args = {}\n",
    "if provider_selected.lower() == 'deepseek':\n",
    "    parallel_args = {\"parallel\": True, \"max_workers\": 6, \"cache_enabled\": True}\n",
    "else:\n",
    "    # untuk model cepat, masih bisa pakai parallel tapi default cukup\n",
    "    parallel_args = {\"parallel\": True, \"max_workers\": 4, \"cache_enabled\": True}\n",
    "\n",
    "# Jalankan pipeline untuk bersihkan teks & analisis topik/sentimen\n",
    "result_df = run_pipeline_on_df(\n",
    "    df,\n",
    "    text_col=\"artikel_berita_bersih\",\n",
    "    title_col=\"judul_berita\",\n",
    "    provider=provider_selected,   # ganti dengan \"openai\" atau \"deepseek\" kalau pakai API\n",
    "    show_progress=True, # tampilkan progress bar\n",
    "    **parallel_args\n",
    ")\n",
    "\n",
    "if custom_model:\n",
    "    print(\"Model override:\", custom_model)\n",
    "\n",
    "# Simpan hasil analisis full\n",
    "today = pd.Timestamp.today().strftime(\"%Y%m%d\")\n",
    "model_suffix = custom_model or 'default'\n",
    "output_path = f\"analisis_ai_{today}_{provider_selected}_{model_suffix}.csv\"\n",
    "result_df.to_csv(output_path, index=False)\n",
    "print(f\"Pipeline selesai. Hasil disimpan di {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 0,
     "status": "aborted",
     "timestamp": 1758532230809,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "Bk07r89bjG8w"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-28 16:31:20,415 | INFO | Berhasil update config.json di config.json\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import json\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Gunakan path lokal default (bukan path Colab) agar tidak error di Mac\n",
    "CONFIG_PATH = Path(\"config.json\")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def update_config(path: Path, new_values: dict):\n",
    "    \"\"\"Update config.json hanya pada key tertentu tanpa menimpa keseluruhan isi.\"\"\"\n",
    "    data = {}\n",
    "    if path.exists():\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Gagal membaca config lama: {e}\")\n",
    "            data = {}\n",
    "\n",
    "    # update hanya key yang diberikan\n",
    "    data.update(new_values)\n",
    "\n",
    "    try:\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "        logger.info(f\"Berhasil update config.json di {path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Gagal menyimpan config.json: {e}\")\n",
    "\n",
    "# Simpan OUTPUT ke config dengan nama yang lebih jelas\n",
    "update_payload = {\n",
    "    \"analisis_ai_output\": output_path,\n",
    "    \"AI_name\": provider_selected,\n",
    "}\n",
    "if cfg.get(\"AI_model_override\"):\n",
    "    update_payload[\"AI_model_override\"] = cfg.get(\"AI_model_override\")\n",
    "update_config(CONFIG_PATH, update_payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil uji LLM (deepseek): {'topik': 'Kemenkeu', 'subtopik': 'kebijakan fiskal pajak APBN', 'sentimen': 'netral', 'alasan_sentimen': 'Berita menyampaikan informasi kebijakan tanpa konteks penindakan atau kata kunci yang mengarah ke sentimen tertentu.', 'poin_of_interest': 'Menteri Keuangan', 'statement_pejabat': '', 'confidence': 0.9}\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: panggil LLM langsung untuk verifikasi (jika bukan mock)\n",
    "sample_text = \"Menteri Keuangan menyampaikan kebijakan fiskal terbaru terkait pajak dan APBN.\"\n",
    "try:\n",
    "    if provider_selected != 'mock':\n",
    "        test_res = classify_with_llm(sample_text, provider=provider_selected, model=cfg.get(\"AI_model_override\"))\n",
    "        print(f\"Hasil uji LLM ({provider_selected}):\", test_res)\n",
    "    else:\n",
    "        print(\"Provider mock: skip panggilan API langsung.\")\n",
    "except Exception as e:\n",
    "    print(\"Gagal memanggil LLM:\", e)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOIPcwvsGMaABbPNui3v/Dn",
   "collapsed_sections": [
    "nl15yqDXsRcB"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "266b550acfda4378816101a6395c99f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a11482dc58194ba0b7d97941f399025b",
      "placeholder": "​",
      "style": "IPY_MODEL_7ccd4b6485b54af483dc9911e6fb93b8",
      "value": " 39/310 [07:39&lt;50:21, 11.15s/berita, sentimen_terakhir=negatif]"
     }
    },
    "5ea695cb8e3e47918b3fe78ae400ebb9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e09521dca283485fa61f19ccace17719",
      "placeholder": "​",
      "style": "IPY_MODEL_7927dfe6ec564f59a7a4567b805b1f91",
      "value": "Analisis berita:  13%"
     }
    },
    "62ae927c096545f99ec8a67a01b91bc2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7927dfe6ec564f59a7a4567b805b1f91": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7ccd4b6485b54af483dc9911e6fb93b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8ec77c93f3da410b92a2884fcf3047d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "91d8d0fa421644af93abd8d5fe535210": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c2c1449ac5e44696aef7b60ef3b91d1c",
      "max": 310,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8ec77c93f3da410b92a2884fcf3047d2",
      "value": 39
     }
    },
    "a11482dc58194ba0b7d97941f399025b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2c1449ac5e44696aef7b60ef3b91d1c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e09521dca283485fa61f19ccace17719": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8666d4da8c0458483dfb292d701325a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5ea695cb8e3e47918b3fe78ae400ebb9",
       "IPY_MODEL_91d8d0fa421644af93abd8d5fe535210",
       "IPY_MODEL_266b550acfda4378816101a6395c99f9"
      ],
      "layout": "IPY_MODEL_62ae927c096545f99ec8a67a01b91bc2"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
