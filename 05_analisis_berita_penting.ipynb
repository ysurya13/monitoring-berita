{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9252cf8",
   "metadata": {},
   "source": [
    "# SETTING ENVIRONMENT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "52389a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# mount the colab with google drive\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# mount the colab with google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "771e26a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set folder tempat kerja (current working directory)\n",
    "import os\n",
    "cwd = \"/Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Pekerjaan BMN/05. 2025/98_monitoring_berita/monitoring-berita\"\n",
    "#cwd = '/content/drive/MyDrive/Monitoring Berita'\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb7aa5d",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "da78eb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 15:25:27,922 - INFO - Membaca file analisis AI: 00_hasil_analisis/seluruh_berita/analisis_ai_20250930_deepseek_default.csv\n",
      "2025-09-30 15:25:27,931 - INFO - Total berita: 225\n",
      "2025-09-30 15:25:27,931 - INFO - Berita penting (filtered): 107\n",
      "2025-09-30 15:25:27,931 - INFO - Total berita: 225\n",
      "2025-09-30 15:25:27,931 - INFO - Berita penting (filtered): 107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berhasil memuat 107 berita penting\n",
      "\n",
      "Sample berita penting:\n",
      "                                          judul_berita topik_llm  importance  \\\n",
      "94   Menkeu Purbaya Sidak ke Kantor Pusat BNI, Ada ...  Kemenkeu        85.0   \n",
      "118    Cukai Rokok Tak Naik, Penerimaan Turun - KONTAN  Kemenkeu        85.0   \n",
      "116  Pemerhati Sayangkan Penundaan Kenaikan Cukai R...  Kemenkeu        85.0   \n",
      "114  Prabowo Perintahkan Bea Cukai Gandeng Ahli Kim...  Kemenkeu        85.0   \n",
      "113  Saham WIIM, HMSP, GGRM Rontok Usai Menkeu Purb...  Kemenkeu        85.0   \n",
      "\n",
      "    sentimen  \n",
      "94   positif  \n",
      "118  positif  \n",
      "116  negatif  \n",
      "114  positif  \n",
      "113  positif  \n"
     ]
    }
   ],
   "source": [
    "# Langkah pertama membaca file csv hasil analisis AI sebelumnya\n",
    "# file terletak di config.json \"analisis_ai_output\"\n",
    "# Filter out berita dengan topik_llm \"Lainnya\"\n",
    "# Filter out berita dengan importance < 50\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup logging untuk error handling\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_berita_penting():\n",
    "    \"\"\"\n",
    "    Memuat dan memfilter berita penting dari file hasil analisis AI\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame berisi berita yang sudah difilter\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Baca konfigurasi\n",
    "        with open('config.json', 'r', encoding='utf-8') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        # Path file analisis AI\n",
    "        analisis_file = config.get('analisis_ai_output')\n",
    "        if not analisis_file:\n",
    "            raise ValueError(\"analisis_ai_output tidak ditemukan dalam config.json\")\n",
    "        \n",
    "        # Periksa apakah file ada\n",
    "        if not Path(analisis_file).exists():\n",
    "            raise FileNotFoundError(f\"File analisis AI tidak ditemukan: {analisis_file}\")\n",
    "        \n",
    "        # Baca file CSV\n",
    "        logger.info(f\"Membaca file analisis AI: {analisis_file}\")\n",
    "        df = pd.read_csv(analisis_file)\n",
    "        \n",
    "        # Filter berita penting\n",
    "        # 1. Exclude topik_llm \"Lainnya\"\n",
    "        # 2. Include importance >= 70\n",
    "        df_filtered = df[\n",
    "            (df['topik_llm'] != 'Lainnya') & \n",
    "            (df['importance'] >= 70)\n",
    "        ].copy()\n",
    "        \n",
    "        logger.info(f\"Total berita: {len(df)}\")\n",
    "        logger.info(f\"Berita penting (filtered): {len(df_filtered)}\")\n",
    "        \n",
    "        if df_filtered.empty:\n",
    "            logger.warning(\"Tidak ada berita penting yang memenuhi kriteria!\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Urutkan berdasarkan importance (descending)\n",
    "        df_filtered = df_filtered.sort_values('importance', ascending=False)\n",
    "        \n",
    "        return df_filtered\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error dalam load_berita_penting: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Load data berita penting\n",
    "df_berita_penting = load_berita_penting()\n",
    "print(f\"Berhasil memuat {len(df_berita_penting)} berita penting\")\n",
    "if not df_berita_penting.empty:\n",
    "    print(\"\\nSample berita penting:\")\n",
    "    print(df_berita_penting[['judul_berita', 'topik_llm', 'importance', 'sentimen']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0e5631a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fungsi analisis AI telah disiapkan (Updated dengan JSON parsing yang diperbaiki).\n",
      "Untuk melakukan analisis, gunakan: analyze_berita_batch(df_berita_penting, 'openai', 'YOUR_API_KEY')\n",
      "Atau: analyze_berita_batch(df_berita_penting, 'deepseek', 'YOUR_API_KEY')\n"
     ]
    }
   ],
   "source": [
    "# buat format prompt baru untuk menganalisis berita\n",
    "# tanya ke AI untuk mengetahui\n",
    "# 1. Resume \n",
    "# 2. Dampak ke Kementerian Keuangan (positif, negatif, netral)\n",
    "# 3. Alasan dampak\n",
    "# 4. Hal menarik dari berita ini\n",
    "\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "def create_analysis_prompt(judul, artikel, source_domain):\n",
    "    \"\"\"\n",
    "    Membuat prompt untuk analisis berita penting\n",
    "    \n",
    "    Args:\n",
    "        judul (str): Judul berita\n",
    "        artikel (str): Isi artikel berita\n",
    "        source_domain (str): Domain sumber berita\n",
    "    \n",
    "    Returns:\n",
    "        str: Prompt untuk AI\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "Analisis berita berikut dengan detail:\n",
    "\n",
    "JUDUL: {judul}\n",
    "SUMBER: {source_domain}\n",
    "ARTIKEL: {artikel}\n",
    "\n",
    "Berikan analisis dalam format JSON dengan struktur berikut:\n",
    "{{\n",
    "    \"resume\": \"Ringkasan singkat dan jelas dari berita dalam 2-3 kalimat\",\n",
    "    \"dampak_kemenkeu\": \"positif/negatif/netral\",\n",
    "    \"alasan_dampak\": \"Penjelasan detail mengapa berita ini berdampak positif/negatif/netral terhadap Kementerian Keuangan. Jelaskan kaitan dengan kebijakan fiskal, perpajakan, kepabeanan, keuangan negara, atau fungsi lain Kemenkeu\",\n",
    "    \"hal_menarik\": \"Poin-poin menarik atau insights penting dari berita ini yang perlu mendapat perhatian khusus\"\n",
    "}}\n",
    "\n",
    "Pastikan analisis objektif dan berdasarkan fakta yang ada dalam berita.\n",
    "Berikan response HANYA dalam format JSON, tanpa teks tambahan.\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def parse_ai_response(raw_response):\n",
    "    \"\"\"\n",
    "    Parse response dari AI untuk extract JSON\n",
    "    \n",
    "    Args:\n",
    "        raw_response (str): Raw response dari AI\n",
    "        \n",
    "    Returns:\n",
    "        dict: Parsed JSON atau None jika gagal\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Remove markdown code blocks jika ada\n",
    "        cleaned_response = raw_response.strip()\n",
    "        \n",
    "        # Remove ```json dan ``` jika ada\n",
    "        if cleaned_response.startswith('```json'):\n",
    "            cleaned_response = cleaned_response[7:]\n",
    "        if cleaned_response.startswith('```'):\n",
    "            cleaned_response = cleaned_response[3:]\n",
    "        if cleaned_response.endswith('```'):\n",
    "            cleaned_response = cleaned_response[:-3]\n",
    "        \n",
    "        cleaned_response = cleaned_response.strip()\n",
    "        \n",
    "        # Try parsing as JSON directly\n",
    "        try:\n",
    "            return json.loads(cleaned_response)\n",
    "        except json.JSONDecodeError:\n",
    "            # Fallback: extract JSON pattern\n",
    "            json_match = re.search(r'\\{.*\\}', cleaned_response, re.DOTALL)\n",
    "            if json_match:\n",
    "                json_str = json_match.group()\n",
    "                return json.loads(json_str)\n",
    "            else:\n",
    "                raise json.JSONDecodeError(\"No JSON found\", cleaned_response, 0)\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error parsing AI response: {e}\")\n",
    "        return None\n",
    "\n",
    "def analyze_with_openai(prompt, api_key, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    Analisis menggunakan OpenAI API (versi 1.0+)\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): Prompt untuk analisis\n",
    "        api_key (str): OpenAI API key\n",
    "        model (str): Model OpenAI yang digunakan\n",
    "    \n",
    "    Returns:\n",
    "        dict: Hasil analisis\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize OpenAI client dengan API key\n",
    "        client = OpenAI(api_key=api_key)\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Anda adalah analis berita ahli yang fokus pada dampak berita terhadap Kementerian Keuangan Indonesia. Selalu berikan response dalam format JSON yang valid.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=1000\n",
    "        )\n",
    "        \n",
    "        raw_content = response.choices[0].message.content\n",
    "        result = parse_ai_response(raw_content)\n",
    "        \n",
    "        if result is None:\n",
    "            logger.error(f\"Failed to parse OpenAI response: {raw_content[:200]}...\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error OpenAI analysis: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def analyze_with_deepseek(prompt, api_key, base_url=\"https://api.deepseek.com/v1\"):\n",
    "    \"\"\"\n",
    "    Analisis menggunakan DeepSeek API\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): Prompt untuk analisis\n",
    "        api_key (str): DeepSeek API key\n",
    "        base_url (str): Base URL DeepSeek API\n",
    "    \n",
    "    Returns:\n",
    "        dict: Hasil analisis\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        data = {\n",
    "            \"model\": \"deepseek-chat\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"Anda adalah analis berita ahli yang fokus pada dampak berita terhadap Kementerian Keuangan Indonesia. Selalu berikan response dalam format JSON yang valid.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            \"temperature\": 0.3,\n",
    "            \"max_tokens\": 1000\n",
    "        }\n",
    "        \n",
    "        response = requests.post(f\"{base_url}/chat/completions\", headers=headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        raw_content = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "        result = parse_ai_response(raw_content)\n",
    "        \n",
    "        if result is None:\n",
    "            logger.error(f\"Failed to parse DeepSeek response: {raw_content[:200]}...\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error DeepSeek analysis: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def analyze_berita_batch(df_berita, ai_provider=\"openai\", api_key=None):\n",
    "    \"\"\"\n",
    "    Analisis batch berita menggunakan AI\n",
    "    \n",
    "    Args:\n",
    "        df_berita (pd.DataFrame): DataFrame berita\n",
    "        ai_provider (str): Provider AI (\"openai\" atau \"deepseek\")\n",
    "        api_key (str): API key untuk AI provider\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame dengan kolom analisis tambahan\n",
    "    \"\"\"\n",
    "    if api_key is None or api_key.strip() == \"\":\n",
    "        logger.warning(\"API key tidak tersedia, skip analisis AI\")\n",
    "        return df_berita\n",
    "    \n",
    "    # Create a copy to avoid modifying original DataFrame\n",
    "    df_result = df_berita.copy()\n",
    "    results = []\n",
    "    \n",
    "    for i, (idx, row) in enumerate(df_berita.iterrows()):\n",
    "        try:\n",
    "            logger.info(f\"Menganalisis berita {i+1}/{len(df_berita)}: {row['judul_berita'][:50]}...\")\n",
    "            \n",
    "            # Buat prompt\n",
    "            prompt = create_analysis_prompt(\n",
    "                row['judul_berita'], \n",
    "                row['artikel_berita_bersih'], \n",
    "                row['source_domain']\n",
    "            )\n",
    "            \n",
    "            # Analisis dengan AI\n",
    "            if ai_provider == \"openai\":\n",
    "                analysis = analyze_with_openai(prompt, api_key)\n",
    "            elif ai_provider == \"deepseek\":\n",
    "                analysis = analyze_with_deepseek(prompt, api_key)\n",
    "            else:\n",
    "                raise ValueError(f\"AI provider tidak dikenali: {ai_provider}\")\n",
    "            \n",
    "            if analysis and isinstance(analysis, dict):\n",
    "                results.append(analysis)\n",
    "                logger.info(f\"✅ Berhasil menganalisis berita {i+1}\")\n",
    "            else:\n",
    "                # Default jika analisis gagal\n",
    "                results.append({\n",
    "                    \"resume\": \"Analisis tidak tersedia\",\n",
    "                    \"dampak_kemenkeu\": \"netral\",\n",
    "                    \"alasan_dampak\": \"Tidak dapat dianalisis\",\n",
    "                    \"hal_menarik\": \"Tidak dapat dianalisis\"\n",
    "                })\n",
    "                logger.warning(f\"⚠️  Analisis gagal untuk berita {i+1}, menggunakan default\")\n",
    "            \n",
    "            # Delay untuk menghindari rate limiting\n",
    "            time.sleep(1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error analyzing berita {i+1}: {str(e)}\")\n",
    "            results.append({\n",
    "                \"resume\": \"Error dalam analisis\",\n",
    "                \"dampak_kemenkeu\": \"netral\", \n",
    "                \"alasan_dampak\": f\"Error: {str(e)}\",\n",
    "                \"hal_menarik\": \"Tidak dapat dianalisis\"\n",
    "            })\n",
    "    \n",
    "    # Tambahkan hasil analisis ke DataFrame\n",
    "    for i, result in enumerate(results):\n",
    "        original_idx = df_result.index[i]\n",
    "        for key, value in result.items():\n",
    "            df_result.loc[original_idx, f\"ai_{key}\"] = value\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "print(\"✅ Fungsi analisis AI telah disiapkan (Updated dengan JSON parsing yang diperbaiki).\")\n",
    "print(\"Untuk melakukan analisis, gunakan: analyze_berita_batch(df_berita_penting, 'openai', 'YOUR_API_KEY')\")\n",
    "print(\"Atau: analyze_berita_batch(df_berita_penting, 'deepseek', 'YOUR_API_KEY')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9d1609e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PREVIEW FORMAT DAFTAR BERITA ===\n",
      "Daftar Berita & Konten\n",
      "Selasa, 30 September 2025\n",
      "Periode pantauan tanggal 29-30 September 2025 (pukul 14.00 s.d. 06.00 WIB)\n",
      "\n",
      "Media Online\n",
      "===========\n",
      "\n",
      "🔴 [Negatif] Pemerhati Sayangkan Penundaan Kenaikan Cukai Rokok pada 2026 - RRI.co.id\n",
      "https://rri.co.id/nasional/1866071/pemerhati-sayangkan-penundaan-kenaikan-cukai-rokok-pada-2026\n",
      "\n",
      "🟢 [Positif] Menkeu Purbaya Sidak ke Kantor Pusat BNI, Ada apa? - Liputan6.com\n",
      "https://www.liputan6.com/amp/6171242/menkeu-purbaya-sidak-ke-kantor-pusat-bni-ada-apa\n",
      "\n",
      "🟢 ...\n"
     ]
    }
   ],
   "source": [
    "# Setelah mendapat informasi terkait berita penting\n",
    "# Format cetakan sehingga sesuai dengan format ini:\n",
    "\n",
    "\"\"\"Daftar Berita & Konten [Judul]\n",
    "Selasa, 30 September 2025 [Tanggal Laporan]\n",
    "Periode pantauan tanggal 29-30 September 2025 (pukul 14.00 s.d. 06.00 WIB) [Waktu Pemantauan]\n",
    "\t\n",
    "Media Online [Judul Bagian]\n",
    "===========\n",
    "\n",
    "\n",
    "🟢 [Sentimen] Purbaya Yakin Kredit Bank Capai 11 Persen Usai Suntikan Dana Rp200 Triliun : Okezone Economy [Judul Berita]\n",
    "https://economy.okezone.com/read/2025/09/29/320/3173364/purbaya-yakin-kredit-bank-capai-11-persen-usai-suntikan-dana-rp200-triliun [url]\n",
    "\n",
    "\n",
    "🟢 [Sentimen] Purbaya Targetkan Ekonomi Indonesia Kuartal IV Tumbuh di Atas 5,5% [Judul Berita]\n",
    "https://ekbis.sindonews.com/read/1626607/33/purbaya-targetkan-ekonomi-indonesia-kuartal-iv-tumbuh-di-atas-55-1759158548 [url]\n",
    "\n",
    "🟢 [Sentimen] Indef Sikap Purbaya Soal Cukai Lindungi Pekerja Industri Rokok [Judul Berita]\n",
    "https://mediaindonesia.com/ekonomi/815843/indef-sikap-purbaya-soal-cukai-lindungi-pekerja-industri-rokok [url]\"\"\"\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import locale\n",
    "\n",
    "def get_sentiment_emoji(sentimen):\n",
    "    \"\"\"\n",
    "    Mengkonversi sentimen ke emoji\n",
    "    \n",
    "    Args:\n",
    "        sentimen (str): Sentimen berita (positif, negatif, netral)\n",
    "    \n",
    "    Returns:\n",
    "        str: Emoji yang sesuai\n",
    "    \"\"\"\n",
    "    sentiment_map = {\n",
    "        'positif': '🟢',\n",
    "        'negatif': '🔴', \n",
    "        'netral': '🟡'\n",
    "    }\n",
    "    return sentiment_map.get(sentimen.lower(), '🟡')\n",
    "\n",
    "def format_tanggal_indonesia(date_obj):\n",
    "    \"\"\"\n",
    "    Format tanggal dalam bahasa Indonesia\n",
    "    \n",
    "    Args:\n",
    "        date_obj (datetime): Objek datetime\n",
    "    \n",
    "    Returns:\n",
    "        str: Tanggal dalam format Indonesia\n",
    "    \"\"\"\n",
    "    hari_indo = {\n",
    "        'Monday': 'Senin',\n",
    "        'Tuesday': 'Selasa', \n",
    "        'Wednesday': 'Rabu',\n",
    "        'Thursday': 'Kamis',\n",
    "        'Friday': 'Jumat',\n",
    "        'Saturday': 'Sabtu',\n",
    "        'Sunday': 'Minggu'\n",
    "    }\n",
    "    \n",
    "    bulan_indo = {\n",
    "        'January': 'Januari', 'February': 'Februari', 'March': 'Maret',\n",
    "        'April': 'April', 'May': 'Mei', 'June': 'Juni',\n",
    "        'July': 'Juli', 'August': 'Agustus', 'September': 'September',\n",
    "        'October': 'Oktober', 'November': 'November', 'December': 'Desember'\n",
    "    }\n",
    "    \n",
    "    hari_eng = date_obj.strftime('%A')\n",
    "    bulan_eng = date_obj.strftime('%B')\n",
    "    \n",
    "    hari_id = hari_indo.get(hari_eng, hari_eng)\n",
    "    bulan_id = bulan_indo.get(bulan_eng, bulan_eng)\n",
    "    \n",
    "    return f\"{hari_id}, {date_obj.day} {bulan_id} {date_obj.year}\"\n",
    "\n",
    "def generate_daftar_berita_format(df_analyzed, periode_start=None, periode_end=None):\n",
    "    \"\"\"\n",
    "    Generate format daftar berita sesuai template\n",
    "    \n",
    "    Args:\n",
    "        df_analyzed (pd.DataFrame): DataFrame berita yang sudah dianalisis\n",
    "        periode_start (str): Tanggal mulai periode (YYYY-MM-DD)\n",
    "        periode_end (str): Tanggal akhir periode (YYYY-MM-DD)\n",
    "    \n",
    "    Returns:\n",
    "        str: Laporan dalam format yang diinginkan\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if df_analyzed.empty:\n",
    "            return \"Tidak ada berita penting untuk dilaporkan.\"\n",
    "        \n",
    "        # Tanggal laporan (hari ini)\n",
    "        tanggal_laporan = format_tanggal_indonesia(datetime.now())\n",
    "        \n",
    "        # Periode pemantauan\n",
    "        if periode_start and periode_end:\n",
    "            start_date = datetime.strptime(periode_start, '%Y-%m-%d')\n",
    "            end_date = datetime.strptime(periode_end, '%Y-%m-%d')\n",
    "            periode_text = f\"tanggal {start_date.day}-{end_date.day} {format_tanggal_indonesia(end_date).split(', ')[1].split(' ')[1]} {end_date.year}\"\n",
    "        else:\n",
    "            # Default ke kemarin-hari ini\n",
    "            hari_ini = datetime.now()\n",
    "            kemarin = hari_ini - timedelta(days=1)\n",
    "            periode_text = f\"tanggal {kemarin.day}-{hari_ini.day} September 2025\"\n",
    "        \n",
    "        # Header laporan\n",
    "        laporan = f\"\"\"Daftar Berita & Konten\n",
    "{tanggal_laporan}\n",
    "Periode pantauan {periode_text} (pukul 14.00 s.d. 06.00 WIB)\n",
    "\n",
    "Media Online\n",
    "===========\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        # Group berita berdasarkan sentimen untuk urutan yang baik\n",
    "        df_sorted = df_analyzed.sort_values(['sentimen', 'importance'], ascending=[True, False])\n",
    "        \n",
    "        # Generate entry untuk setiap berita\n",
    "        for idx, row in df_sorted.iterrows():\n",
    "            emoji = get_sentiment_emoji(row['sentimen'])\n",
    "            judul_clean = row['judul_berita'].replace('\\n', ' ').strip()\n",
    "            \n",
    "            # Format entry berita\n",
    "            berita_entry = f\"{emoji} [{row['sentimen'].title()}] {judul_clean}\\n{row['url_berita']}\\n\\n\"\n",
    "            laporan += berita_entry\n",
    "        \n",
    "        return laporan\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating daftar berita format: {str(e)}\")\n",
    "        return f\"Error dalam generate format: {str(e)}\"\n",
    "\n",
    "def save_daftar_berita(laporan_text, output_dir=\"00_laporan_cetak\"):\n",
    "    \"\"\"\n",
    "    Simpan laporan daftar berita ke file\n",
    "    \n",
    "    Args:\n",
    "        laporan_text (str): Teks laporan\n",
    "        output_dir (str): Directory output\n",
    "    \n",
    "    Returns:\n",
    "        str: Path file yang disimpan\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Buat directory jika belum ada\n",
    "        Path(output_dir).mkdir(exist_ok=True)\n",
    "        \n",
    "        # Nama file dengan timestamp\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"daftar_berita_{timestamp}.txt\"\n",
    "        filepath = Path(output_dir) / filename\n",
    "        \n",
    "        # Simpan file\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(laporan_text)\n",
    "        \n",
    "        logger.info(f\"Laporan daftar berita disimpan: {filepath}\")\n",
    "        return str(filepath)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving daftar berita: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Test format dengan data dummy jika ada data\n",
    "if 'df_berita_penting' in locals() and not df_berita_penting.empty:\n",
    "    print(\"=== PREVIEW FORMAT DAFTAR BERITA ===\")\n",
    "    sample_format = generate_daftar_berita_format(df_berita_penting.head(3))\n",
    "    print(sample_format[:500] + \"...\" if len(sample_format) > 500 else sample_format)\n",
    "else:\n",
    "    print(\"Fungsi format daftar berita telah disiapkan.\")\n",
    "    print(\"Gunakan: generate_daftar_berita_format(df_analyzed) untuk generate laporan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "410cb2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fungsi parallel processing telah disiapkan!\n",
      "\n",
      "🚀 Optimizations tersedia:\n",
      "1. analyze_berita_parallel() - Multi-threading dengan ThreadPoolExecutor\n",
      "2. analyze_berita_batch_optimized() - Kombinasi batching + parallel processing\n",
      "\n",
      "📊 Perkiraan percepatan:\n",
      "- Sequential: ~1-2 detik per berita\n",
      "- Parallel (3 workers): ~3-5x lebih cepat\n",
      "- Batch + Parallel: Optimal untuk dataset besar\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PARALLEL PROCESSING UNTUK MEMPERCEPAT ANALISIS AI\n",
    "# =============================================================================\n",
    "\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "def analyze_single_berita(row_data, ai_provider=\"openai\", api_key=None, delay=0.5):\n",
    "    \"\"\"\n",
    "    Analisis single berita untuk parallel processing\n",
    "    \n",
    "    Args:\n",
    "        row_data (tuple): (index, row) dari iterrows()\n",
    "        ai_provider (str): Provider AI\n",
    "        api_key (str): API key\n",
    "        delay (float): Delay antar request (detik)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (index, result_dict)\n",
    "    \"\"\"\n",
    "    idx, row = row_data\n",
    "    \n",
    "    try:\n",
    "        # Buat prompt\n",
    "        prompt = create_analysis_prompt(\n",
    "            row['judul_berita'], \n",
    "            row['artikel_berita_bersih'], \n",
    "            row['source_domain']\n",
    "        )\n",
    "        \n",
    "        # Analisis dengan AI\n",
    "        if ai_provider == \"openai\":\n",
    "            analysis = analyze_with_openai(prompt, api_key)\n",
    "        elif ai_provider == \"deepseek\":\n",
    "            analysis = analyze_with_deepseek(prompt, api_key)\n",
    "        else:\n",
    "            raise ValueError(f\"AI provider tidak dikenali: {ai_provider}\")\n",
    "        \n",
    "        # Delay untuk rate limiting\n",
    "        if delay > 0:\n",
    "            time.sleep(delay)\n",
    "        \n",
    "        if analysis and isinstance(analysis, dict):\n",
    "            return (idx, analysis)\n",
    "        else:\n",
    "            return (idx, {\n",
    "                \"resume\": \"Analisis tidak tersedia\",\n",
    "                \"dampak_kemenkeu\": \"netral\",\n",
    "                \"alasan_dampak\": \"Tidak dapat dianalisis\",\n",
    "                \"hal_menarik\": \"Tidak dapat dianalisis\"\n",
    "            })\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error analyzing berita {idx}: {str(e)}\")\n",
    "        return (idx, {\n",
    "            \"resume\": \"Error dalam analisis\",\n",
    "            \"dampak_kemenkeu\": \"netral\", \n",
    "            \"alasan_dampak\": f\"Error: {str(e)}\",\n",
    "            \"hal_menarik\": \"Tidak dapat dianalisis\"\n",
    "        })\n",
    "\n",
    "def analyze_berita_parallel(df_berita, ai_provider=\"openai\", api_key=None, max_workers=3, delay=0.5):\n",
    "    \"\"\"\n",
    "    Analisis berita menggunakan parallel processing dengan ThreadPoolExecutor\n",
    "    \n",
    "    Args:\n",
    "        df_berita (pd.DataFrame): DataFrame berita\n",
    "        ai_provider (str): Provider AI (\"openai\" atau \"deepseek\")\n",
    "        api_key (str): API key untuk AI provider\n",
    "        max_workers (int): Jumlah maksimum thread (default: 3 untuk menghindari rate limit)\n",
    "        delay (float): Delay antar request dalam detik (default: 0.5)\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame dengan kolom analisis tambahan\n",
    "    \"\"\"\n",
    "    if api_key is None or api_key.strip() == \"\":\n",
    "        logger.warning(\"API key tidak tersedia, skip analisis AI\")\n",
    "        return df_berita\n",
    "    \n",
    "    logger.info(f\"🚀 Memulai analisis parallel dengan {max_workers} workers...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    df_result = df_berita.copy()\n",
    "    \n",
    "    # Persiapkan data untuk parallel processing\n",
    "    row_data = list(df_berita.iterrows())\n",
    "    \n",
    "    # Fungsi partial untuk menyederhanakan parameter\n",
    "    analyze_func = partial(\n",
    "        analyze_single_berita, \n",
    "        ai_provider=ai_provider, \n",
    "        api_key=api_key, \n",
    "        delay=delay\n",
    "    )\n",
    "    \n",
    "    # Parallel processing dengan ThreadPoolExecutor\n",
    "    results = {}\n",
    "    completed = 0\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit semua tugas\n",
    "        future_to_idx = {executor.submit(analyze_func, row): row[0] for row in row_data}\n",
    "        \n",
    "        # Collect hasil secara bertahap\n",
    "        for future in as_completed(future_to_idx):\n",
    "            try:\n",
    "                idx, analysis = future.result()\n",
    "                results[idx] = analysis\n",
    "                completed += 1\n",
    "                \n",
    "                # Progress update\n",
    "                progress = (completed / len(df_berita)) * 100\n",
    "                logger.info(f\"✅ Progress: {completed}/{len(df_berita)} ({progress:.1f}%)\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                original_idx = future_to_idx[future]\n",
    "                logger.error(f\"❌ Error processing berita {original_idx}: {str(e)}\")\n",
    "                results[original_idx] = {\n",
    "                    \"resume\": \"Error dalam parallel processing\",\n",
    "                    \"dampak_kemenkeu\": \"netral\",\n",
    "                    \"alasan_dampak\": f\"Parallel error: {str(e)}\",\n",
    "                    \"hal_menarik\": \"Tidak dapat dianalisis\"\n",
    "                }\n",
    "    \n",
    "    # Tambahkan hasil ke DataFrame\n",
    "    for idx, result in results.items():\n",
    "        if isinstance(result, dict):\n",
    "            for key, value in result.items():\n",
    "                df_result.loc[idx, f\"ai_{key}\"] = str(value) if value is not None else \"\"\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    logger.info(f\"🎉 Analisis parallel selesai dalam {elapsed:.2f} detik\")\n",
    "    logger.info(f\"📊 Rata-rata: {elapsed/len(df_berita):.2f} detik per berita\")\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "def analyze_berita_batch_optimized(df_berita, ai_provider=\"openai\", api_key=None, batch_size=5, max_workers=3):\n",
    "    \"\"\"\n",
    "    Analisis berita dengan kombinasi batching dan parallel processing\n",
    "    \n",
    "    Args:\n",
    "        df_berita (pd.DataFrame): DataFrame berita\n",
    "        ai_provider (str): Provider AI\n",
    "        api_key (str): API key\n",
    "        batch_size (int): Ukuran batch untuk processing\n",
    "        max_workers (int): Jumlah thread per batch\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame dengan analisis\n",
    "    \"\"\"\n",
    "    if api_key is None or api_key.strip() == \"\":\n",
    "        logger.warning(\"API key tidak tersedia, skip analisis AI\")\n",
    "        return df_berita\n",
    "    \n",
    "    logger.info(f\"🔄 Memulai batch processing dengan ukuran batch: {batch_size}\")\n",
    "    \n",
    "    df_result = df_berita.copy()\n",
    "    total_batches = (len(df_berita) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for batch_num in range(total_batches):\n",
    "        start_idx = batch_num * batch_size\n",
    "        end_idx = min((batch_num + 1) * batch_size, len(df_berita))\n",
    "        \n",
    "        logger.info(f\"📦 Processing batch {batch_num + 1}/{total_batches} (rows {start_idx}-{end_idx-1})\")\n",
    "        \n",
    "        # Ambil batch data\n",
    "        batch_df = df_berita.iloc[start_idx:end_idx]\n",
    "        \n",
    "        # Analisis batch dengan parallel processing\n",
    "        batch_result = analyze_berita_parallel(\n",
    "            batch_df, \n",
    "            ai_provider=ai_provider, \n",
    "            api_key=api_key, \n",
    "            max_workers=max_workers,\n",
    "            delay=0.3  # Delay lebih kecil karena batch lebih kecil\n",
    "        )\n",
    "        \n",
    "        # Update hasil - perbaiki assignment dengan memastikan compatibility\n",
    "        for col in batch_result.columns:\n",
    "            if col.startswith('ai_'):\n",
    "                # Ensure we're updating existing rows properly\n",
    "                for idx in batch_result.index:\n",
    "                    if idx in df_result.index:\n",
    "                        df_result.loc[idx, col] = batch_result.loc[idx, col]\n",
    "        \n",
    "        # Delay antar batch untuk menghindari rate limit\n",
    "        if batch_num < total_batches - 1:\n",
    "            logger.info(\"⏱️  Waiting between batches...\")\n",
    "            time.sleep(2)\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "print(\"✅ Fungsi parallel processing telah disiapkan!\")\n",
    "print(\"\\n🚀 Optimizations tersedia:\")\n",
    "print(\"1. analyze_berita_parallel() - Multi-threading dengan ThreadPoolExecutor\")\n",
    "print(\"2. analyze_berita_batch_optimized() - Kombinasi batching + parallel processing\")\n",
    "print(\"\\n📊 Perkiraan percepatan:\")\n",
    "print(\"- Sequential: ~1-2 detik per berita\")\n",
    "print(\"- Parallel (3 workers): ~3-5x lebih cepat\")\n",
    "print(\"- Batch + Parallel: Optimal untuk dataset besar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "97842265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PREVIEW FORMAT NEWS UPDATE ===\n",
      "News Update\n",
      "Rokok Ilegal\n",
      "Jakarta, Selasa, 30 September 2025 (Pukul 15.25 WIB)\n",
      "\n",
      "Pemberitaan mengenai rokok ilegal hari ini tercatat terdapat 5 berita (4 positif dan 1 negatif) di media online.\n",
      "\n",
      "Sorotan Media Online\n",
      "• Menteri Keuangan Purbaya Yudhi Sadewa, Direktur Utama BNI Putrama Wahju Setyawan, Wakil Direktur Utama BNI Alexandra Askandar\n",
      "• Menteri Keuangan Purbaya Yudhi Sadewa\n",
      "• Menteri Keuangan Purbaya Yudhi Sadewa, Ketua FKBI Tulus Abadi, Pengamat Hananto Wibisono\n",
      "• Presiden Prabowo Subianto\n",
      "• Menteri Keuangan Purbaya Yudhi Sadewa, Direktur Komunikasi dan Bimbingan Pengguna Jasa Direktorat Bea Cukai Nirwala Dwi Heryanto\n",
      "\n",
      "Tautan Media Online:\n",
      "1. Menkeu Purbaya Sidak ke Kantor Pusat BNI, Ada apa? - Liputan6.com\n",
      "https://www.liputan6.com/amp/6171242/menkeu-purbaya-sidak-ke-kantor-pusat-bni...\n"
     ]
    }
   ],
   "source": [
    "# Format kedua terkait dengan news update\n",
    "\n",
    "\"\"\"News Update [Judul] \n",
    "Menkeu Sidak BNI [Topik yang Dipantau diambil dari config.json \"topic_keywords]     \n",
    "Jakarta, 29 September 2025 (Pukul 19.00 WIB) [Periode Pemantauan]\n",
    "\n",
    "Pemberitaan mengenai inspeksi mendadak (sidak) ke kantor pusat PT Bank Negara Indonesia (Persero) Tbk atau BNI hari ini tercatat terdapat 33 berita (30 positif dan 3 netral) di media online. \n",
    "\n",
    "Sorotan Media Online \n",
    "•⁠  ⁠Menkeu melakukan inspeksi mendadak ke kantor pusat BNI untuk melihat bagaimana kerja BNI pada saat rapat direksi berlangsung.  \n",
    "•⁠  ⁠Kontroversi kenaikan suku bunga deposito valuta asing (valas) dolar AS menjadi 4% yang dilakukan oleh bank-bank Himbara diduga menjadi latar belakang sidak Menkeu tersebut.  \n",
    "•⁠  ⁠Sebelumnya Menkeu telah menegaskan bahwa isu kenaikan bunga deposito valas bukan instruksinya dan menolak tuduhan bahwa dirinya mendikte kebijakan perbankan. \n",
    "•⁠  ⁠Chief Economist Permata Bank, Josua Pardede, menjelaskan risiko yang lebih luas dari kebijakan menaikkan valas dolar AS adalah menguatnya kecenderungan menyimpan kekayaan dalam bentuk dolar. \n",
    "•⁠  ⁠Menkeu menyebut kedatangannya hanya untuk mengecek langsung penyaluran kredit dari perbankan, khususnya bank-bank yang menerima penempatan dana negara sebesar Rp200 triliun. \n",
    " \n",
    "Tautan Media Online: \n",
    " 1.⁠ ⁠Purbaya Tiba-tiba Sidak Kantor BNI, Nimbrung Rapat Direksi \n",
    "https://www.cnnindonesia.com/ekonomi/20250929134914-532-1278863/purbaya-tiba-tiba-sidak-kantor-bni-nimbrung-rapat-direksi  \n",
    " 2.⁠ ⁠Mengapa Menteri Purbaya Inspeksi Mendadak BNI? \n",
    "https://www.tempo.co/ekonomi/mengapa-menteri-purbaya-inspeksi-mendadak-bni--2074388 \n",
    " 3.⁠ ⁠Mendadak Sidak ke Kantor BNI, Menkeu Purbaya: Boleh Masuk Enggak Ya \n",
    "https://www.beritasatu.com/ekonomi/2926647/mendadak-sidak-ke-kantor-bni-menkeu-purbaya-boleh-masuk-enggak-ya#goog_rewarded \n",
    " 4.⁠ ⁠Purbaya Sidak Kantor BNI Saat Direksi Lagi Rapat, Ada Apa? \n",
    "https://economy.okezone.com/amp/2025/09/29/320/3173222/purbaya-sidak-kantor-bni-saat-direksi-lagi-rapat-ada-apa \n",
    " 5.⁠ ⁠Purbaya Sidak Kantor BNI: Saya Mau Lihat Bagaimana Kerja Mereka \n",
    "https://ekbis.sindonews.com/read/1626387/33/purbaya-sidak-kantor-bni-saya-mau-lihat-bagaimana-kerja-mereka-1759129777/5  \n",
    " 6.⁠ ⁠Menkeu Purbaya Sidak ke Kantor BNI, Ada Apa? \n",
    "https://www.idxchannel.com/amp/economics/menkeu-purbaya-sidak-ke-kantor-bni-ada-apa  \n",
    "\"\"\"\n",
    "\n",
    "def identify_main_topic(df_berita, config_topics):\n",
    "    \"\"\"\n",
    "    Identifikasi topik utama dari berita berdasarkan frequency dan importance\n",
    "    \n",
    "    Args:\n",
    "        df_berita (pd.DataFrame): DataFrame berita\n",
    "        config_topics (list): Daftar topik keywords dari config\n",
    "    \n",
    "    Returns:\n",
    "        str: Topik utama yang teridentifikasi\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if df_berita.empty:\n",
    "            return \"Berita Umum\"\n",
    "        \n",
    "        # Analisis topik berdasarkan subtopik_llm dan importance\n",
    "        topic_analysis = df_berita.groupby('subtopik_llm').agg({\n",
    "            'importance': ['mean', 'count'],\n",
    "            'judul_berita': 'first'\n",
    "        }).round(2)\n",
    "        \n",
    "        # Flatten kolom\n",
    "        topic_analysis.columns = ['avg_importance', 'count_berita', 'sample_judul']\n",
    "        \n",
    "        # Hitung skor gabungan (weighted importance)\n",
    "        topic_analysis['weighted_score'] = (\n",
    "            topic_analysis['avg_importance'] * topic_analysis['count_berita']\n",
    "        )\n",
    "        \n",
    "        # Dapatkan topik utama\n",
    "        main_topic = topic_analysis.sort_values('weighted_score', ascending=False).index[0]\n",
    "        \n",
    "        # Cek apakah topik utama ada dalam config topics\n",
    "        for config_topic in config_topics:\n",
    "            if config_topic.lower() in main_topic.lower() or main_topic.lower() in config_topic.lower():\n",
    "                return config_topic.title()\n",
    "        \n",
    "        return main_topic.title()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error identifying main topic: {str(e)}\")\n",
    "        return \"Berita Umum\"\n",
    "\n",
    "def generate_sentiment_summary(df_berita):\n",
    "    \"\"\"\n",
    "    Generate ringkasan sentimen berita\n",
    "    \n",
    "    Args:\n",
    "        df_berita (pd.DataFrame): DataFrame berita\n",
    "    \n",
    "    Returns:\n",
    "        dict: Summary sentimen\n",
    "    \"\"\"\n",
    "    sentiment_count = df_berita['sentimen'].value_counts().to_dict()\n",
    "    total = len(df_berita)\n",
    "    \n",
    "    return {\n",
    "        'total': total,\n",
    "        'positif': sentiment_count.get('positif', 0),\n",
    "        'negatif': sentiment_count.get('negatif', 0), \n",
    "        'netral': sentiment_count.get('netral', 0)\n",
    "    }\n",
    "\n",
    "def extract_key_points(df_berita, max_points=5):\n",
    "    \"\"\"\n",
    "    Extract poin-poin kunci dari berita untuk sorotan media online\n",
    "    \n",
    "    Args:\n",
    "        df_berita (pd.DataFrame): DataFrame berita\n",
    "        max_points (int): Maksimal poin yang akan diambil\n",
    "    \n",
    "    Returns:\n",
    "        list: Daftar poin kunci\n",
    "    \"\"\"\n",
    "    try:\n",
    "        points = []\n",
    "        \n",
    "        # Prioritaskan berita dengan importance tinggi\n",
    "        df_sorted = df_berita.sort_values('importance', ascending=False)\n",
    "        \n",
    "        for idx, row in df_sorted.head(max_points).iterrows():\n",
    "            # Gunakan AI analysis jika ada, kalau tidak gunakan poin_of_interest\n",
    "            if 'ai_hal_menarik' in row and pd.notna(row['ai_hal_menarik']):\n",
    "                point = row['ai_hal_menarik']\n",
    "            elif pd.notna(row['poin_of_interest']):\n",
    "                point = row['poin_of_interest']\n",
    "            else:\n",
    "                # Fallback ke statement pejabat atau excerpt dari artikel\n",
    "                if pd.notna(row['statement_pejabat']):\n",
    "                    point = row['statement_pejabat'][:150] + \"...\"\n",
    "                else:\n",
    "                    point = row['artikel_berita_bersih'][:150] + \"...\"\n",
    "            \n",
    "            points.append(point)\n",
    "        \n",
    "        return points\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting key points: {str(e)}\")\n",
    "        return [\"Tidak dapat mengekstrak poin kunci dari berita\"]\n",
    "\n",
    "def generate_news_update_format(df_berita, main_topic=None):\n",
    "    \"\"\"\n",
    "    Generate format News Update sesuai template\n",
    "    \n",
    "    Args:\n",
    "        df_berita (pd.DataFrame): DataFrame berita yang sudah dianalisis\n",
    "        main_topic (str): Topik utama (optional)\n",
    "    \n",
    "    Returns:\n",
    "        str: News Update dalam format yang diinginkan\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if df_berita.empty:\n",
    "            return \"Tidak ada berita untuk news update.\"\n",
    "        \n",
    "        # Load config untuk topic keywords\n",
    "        with open('config.json', 'r', encoding='utf-8') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        # Identifikasi topik utama\n",
    "        if not main_topic:\n",
    "            main_topic = identify_main_topic(df_berita, config.get('topic_keywords', []))\n",
    "        \n",
    "        # Tanggal dan waktu\n",
    "        tanggal_laporan = format_tanggal_indonesia(datetime.now())\n",
    "        waktu_laporan = datetime.now().strftime(\"%H.%M\")\n",
    "        \n",
    "        # Summary sentimen\n",
    "        sentiment_summary = generate_sentiment_summary(df_berita)\n",
    "        \n",
    "        # Generate sentiment text\n",
    "        sentiment_text = []\n",
    "        if sentiment_summary['positif'] > 0:\n",
    "            sentiment_text.append(f\"{sentiment_summary['positif']} positif\")\n",
    "        if sentiment_summary['negatif'] > 0:\n",
    "            sentiment_text.append(f\"{sentiment_summary['negatif']} negatif\")\n",
    "        if sentiment_summary['netral'] > 0:\n",
    "            sentiment_text.append(f\"{sentiment_summary['netral']} netral\")\n",
    "        \n",
    "        sentiment_string = \" dan \".join(sentiment_text) if sentiment_text else \"beragam sentimen\"\n",
    "        \n",
    "        # Extract key points\n",
    "        key_points = extract_key_points(df_berita)\n",
    "        \n",
    "        # Header news update\n",
    "        news_update = f\"\"\"News Update\n",
    "{main_topic}\n",
    "Jakarta, {tanggal_laporan} (Pukul {waktu_laporan} WIB)\n",
    "\n",
    "Pemberitaan mengenai {main_topic.lower()} hari ini tercatat terdapat {sentiment_summary['total']} berita ({sentiment_string}) di media online.\n",
    "\n",
    "Sorotan Media Online\"\"\"\n",
    "        \n",
    "        # Tambahkan key points\n",
    "        for i, point in enumerate(key_points, 1):\n",
    "            news_update += f\"\\n• {point}\"\n",
    "        \n",
    "        news_update += \"\\n\\nTautan Media Online:\"\n",
    "        \n",
    "        # Tambahkan daftar berita dengan link\n",
    "        for i, (idx, row) in enumerate(df_berita.head(10).iterrows(), 1):\n",
    "            judul_clean = row['judul_berita'].replace('\\n', ' ').strip()\n",
    "            news_update += f\"\\n{i}. {judul_clean}\\n{row['url_berita']}\"\n",
    "        \n",
    "        return news_update\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating news update format: {str(e)}\")\n",
    "        return f\"Error dalam generate news update: {str(e)}\"\n",
    "\n",
    "def save_news_update(news_update_text, topic=\"general\", output_dir=\"00_laporan_cetak\"):\n",
    "    \"\"\"\n",
    "    Simpan news update ke file\n",
    "    \n",
    "    Args:\n",
    "        news_update_text (str): Teks news update\n",
    "        topic (str): Topik untuk nama file\n",
    "        output_dir (str): Directory output\n",
    "    \n",
    "    Returns:\n",
    "        str: Path file yang disimpan\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Buat directory jika belum ada\n",
    "        Path(output_dir).mkdir(exist_ok=True)\n",
    "        \n",
    "        # Nama file dengan timestamp dan topik\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        topic_clean = topic.replace(\" \", \"_\").lower()\n",
    "        filename = f\"news_update_{topic_clean}_{timestamp}.txt\"\n",
    "        filepath = Path(output_dir) / filename\n",
    "        \n",
    "        # Simpan file\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(news_update_text)\n",
    "        \n",
    "        logger.info(f\"News update disimpan: {filepath}\")\n",
    "        return str(filepath)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving news update: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Test format dengan data dummy jika ada data\n",
    "if 'df_berita_penting' in locals() and not df_berita_penting.empty:\n",
    "    print(\"=== PREVIEW FORMAT NEWS UPDATE ===\")\n",
    "    sample_news_update = generate_news_update_format(df_berita_penting.head(5))\n",
    "    print(sample_news_update[:800] + \"...\" if len(sample_news_update) > 800 else sample_news_update)\n",
    "else:\n",
    "    print(\"Fungsi format news update telah disiapkan.\")\n",
    "    print(\"Gunakan: generate_news_update_format(df_analyzed) untuk generate news update\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e026da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PREVIEW FORMAT LAPORAN ANALISIS LENGKAP ===\n",
      "**Laporan Analisis Media Online dan Media Sosial**\n",
      "Selasa, 30 September 2025\n",
      "\n",
      "=========================================================\n",
      "\n",
      "**EXECUTIVE SUMMARY**\n",
      "=========================================================\n",
      "Periode pemantauan ini mencatat 3 berita penting yang memenuhi kriteria analisis. \n",
      "Dari total tersebut, 3 berita terkait langsung dengan Kementerian Keuangan dan 0 berita terkait isu nasional/internasional.\n",
      "\n",
      "Isu utama yang mendominasi pemberitaan Kemenkeu adalah sidak BNI. \n",
      "Secara keseluruhan, tonasi pemberitaan menunjukkan 2 berita positif, \n",
      "1 berita negatif, dan 0 berita netral.\n",
      "\n",
      "Fokus pemerintah pada periode ini terkonsentrasi pada implementasi kebijakan fiskal dan \n",
      "monitoring pelaksanaan program prioritas nasional.\n",
      "\n",
      "=========================================================\n",
      "\n",
      "**MEDIA ONLINE**\n",
      "\n",
      "**Topik Berita:** rokok ilegal, makan bergizi gratis, tax amnesty, sidak BNI\n",
      "\n",
      "**Tonasi Berita:** Positif\n",
      "\n",
      "**Pesan Kunci dan Analisis:**\n",
      "\n",
      "**ISU KEMENKEU**\n",
      "1. Liputan6.com, Jakarta M...\n"
     ]
    }
   ],
   "source": [
    "# Format ketiga \n",
    "# Prompt untuk laporan analisis berita\n",
    "\n",
    "\"\"\"Buatlah sebuah dokumen laporan analisis media online dan media sosial dengan struktur dan format sebagai berikut:\n",
    "\n",
    "1. JUDUL DAN TANGGAL\n",
    "\n",
    "Judul utama: \"Laporan Analisis Media Online dan Media Sosial\"\n",
    "Cantumkan hari, tanggal, dan tahun (contoh: Senin, 29 September 2025)\n",
    "\n",
    "2. EXECUTIVE SUMMARY\n",
    "\n",
    "Gunakan pemisah garis seperti =========\n",
    "Ringkasan harus mencakup poin-poin utama dari pemberitaan media online dan isu-isu terkini, termasuk:\n",
    "Isu utama (misal: sidak menteri, kebijakan cukai, revisi UU, dll.)\n",
    "Fokus pemerintah atau kementerian\n",
    "Pernyataan atau kebijakan penting dari pejabat\n",
    "3. MEDIA ONLINE\n",
    "\n",
    "Subjudul: \"Media Online\"\n",
    "Topik Berita: Sebutkan topik-topik utama yang dilaporkan [topik diambil dari config.json \"topic_keywords\"]\n",
    "Tonasi Berita: Tuliskan sentimen (misal: Netral, Positif, Negatif)\n",
    "Pesan Kunci dan Analisis:\n",
    "Bagian ini dibagi menjadi:\n",
    "ISU KEMENKEU (nomor 1, 2, 3, dst. dengan penjelasan singkat)\n",
    "ISU NASIONAL DAN INTERNASIONAL (nomor 1, 2, dst. dengan penjelasan singkat)\n",
    "Kegiatan yang dirujuk: Jelaskan jenis kegiatan (misal: Kegiatan Baru, Tanggal)\n",
    "Narasumber utama yang dirujuk: Tulis \"Belum ada narasumber\" jika tidak disebutkan\n",
    "Daftar Berita: Buat daftar berita dengan format:\n",
    "Nomor. Judul berita\n",
    "[URL]\n",
    "\n",
    "4. FORMAT UMUM\n",
    "\n",
    "Gunakan pemisah halaman seperti ===== Page X =====\n",
    "Gunakan tanda tebal untuk judul dan subjudul\n",
    "Gunakan tanda - untuk poin-poin dalam analisis\n",
    "Pastikan konsistensi penulisan tanggal, nama, dan istilah\"\"\"\n",
    "\n",
    "def categorize_berita(df_berita):\n",
    "    \"\"\"\n",
    "    Kategorikan berita berdasarkan relevansi dengan Kemenkeu\n",
    "    \n",
    "    Args:\n",
    "        df_berita (pd.DataFrame): DataFrame berita\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary dengan kategori berita\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Keywords untuk identifikasi isu Kemenkeu\n",
    "        kemenkeu_keywords = [\n",
    "            'kemenkeu', 'kementerian keuangan', 'menteri keuangan', 'menkeu',\n",
    "            'pajak', 'bea cukai', 'anggaran', 'fiskal', 'apbn', 'apbd',\n",
    "            'sbn', 'obligasi', 'deficit', 'surplus', 'pembiayaan',\n",
    "            'penerimaan negara', 'belanja negara', 'purbaya'\n",
    "        ]\n",
    "        \n",
    "        # Kategorisasi\n",
    "        isu_kemenkeu = []\n",
    "        isu_nasional_internasional = []\n",
    "        \n",
    "        for idx, row in df_berita.iterrows():\n",
    "            # Gabungkan teks untuk analisis\n",
    "            full_text = f\"{row['judul_berita']} {row['artikel_berita_bersih']}\"\n",
    "            full_text_lower = full_text.lower()\n",
    "            \n",
    "            # Cek apakah mengandung keyword Kemenkeu\n",
    "            is_kemenkeu = any(keyword in full_text_lower for keyword in kemenkeu_keywords)\n",
    "            \n",
    "            if is_kemenkeu or row['kategori_isu'] == 'Kemenkeu':\n",
    "                isu_kemenkeu.append(row)\n",
    "            else:\n",
    "                isu_nasional_internasional.append(row)\n",
    "        \n",
    "        return {\n",
    "            'kemenkeu': pd.DataFrame(isu_kemenkeu),\n",
    "            'nasional_internasional': pd.DataFrame(isu_nasional_internasional)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error categorizing berita: {str(e)}\")\n",
    "        return {'kemenkeu': pd.DataFrame(), 'nasional_internasional': df_berita}\n",
    "\n",
    "def extract_narasumber(df_berita):\n",
    "    \"\"\"\n",
    "    Extract narasumber utama dari berita\n",
    "    \n",
    "    Args:\n",
    "        df_berita (pd.DataFrame): DataFrame berita\n",
    "    \n",
    "    Returns:\n",
    "        list: Daftar narasumber yang teridentifikasi\n",
    "    \"\"\"\n",
    "    narasumber_list = []\n",
    "    \n",
    "    for idx, row in df_berita.iterrows():\n",
    "        if pd.notna(row['poin_of_interest']) and row['poin_of_interest'].strip():\n",
    "            narasumber_list.append(row['poin_of_interest'])\n",
    "        elif pd.notna(row['statement_pejabat']) and row['statement_pejabat'].strip():\n",
    "            # Extract nama dari statement (ambil kata pertama yang kapital)\n",
    "            words = row['statement_pejabat'].split()\n",
    "            for word in words[:5]:  # Cek 5 kata pertama\n",
    "                if word.istitle() and len(word) > 3:\n",
    "                    narasumber_list.append(word)\n",
    "                    break\n",
    "    \n",
    "    # Hapus duplikasi dan return unique narasumber\n",
    "    unique_narasumber = list(set(narasumber_list))[:5]  # Ambil max 5\n",
    "    \n",
    "    return unique_narasumber if unique_narasumber else [\"Belum ada narasumber\"]\n",
    "\n",
    "def generate_executive_summary(df_berita, categorized_berita):\n",
    "    \"\"\"\n",
    "    Generate executive summary untuk laporan\n",
    "    \n",
    "    Args:\n",
    "        df_berita (pd.DataFrame): DataFrame semua berita\n",
    "        categorized_berita (dict): Berita yang sudah dikategorikan\n",
    "    \n",
    "    Returns:\n",
    "        str: Executive summary text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Identifikasi isu utama\n",
    "        if not categorized_berita['kemenkeu'].empty:\n",
    "            main_kemenkeu_issue = categorized_berita['kemenkeu'].iloc[0]['subtopik_llm']\n",
    "        else:\n",
    "            main_kemenkeu_issue = \"Tidak ada isu Kemenkeu utama\"\n",
    "        \n",
    "        # Hitung statistik\n",
    "        total_berita = len(df_berita)\n",
    "        kemenkeu_count = len(categorized_berita['kemenkeu'])\n",
    "        nasional_count = len(categorized_berita['nasional_internasional'])\n",
    "        \n",
    "        # Sentimen overview\n",
    "        sentiment_summary = generate_sentiment_summary(df_berita)\n",
    "        \n",
    "        # Buat executive summary\n",
    "        executive_summary = f\"\"\"\n",
    "Periode pemantauan ini mencatat {total_berita} berita penting yang memenuhi kriteria analisis. \n",
    "Dari total tersebut, {kemenkeu_count} berita terkait langsung dengan Kementerian Keuangan dan {nasional_count} berita terkait isu nasional/internasional.\n",
    "\n",
    "Isu utama yang mendominasi pemberitaan Kemenkeu adalah {main_kemenkeu_issue}. \n",
    "Secara keseluruhan, tonasi pemberitaan menunjukkan {sentiment_summary['positif']} berita positif, \n",
    "{sentiment_summary['negatif']} berita negatif, dan {sentiment_summary['netral']} berita netral.\n",
    "\n",
    "Fokus pemerintah pada periode ini terkonsentrasi pada implementasi kebijakan fiskal dan \n",
    "monitoring pelaksanaan program prioritas nasional.\n",
    "\"\"\"\n",
    "        \n",
    "        return executive_summary.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating executive summary: {str(e)}\")\n",
    "        return \"Error dalam membuat executive summary\"\n",
    "\n",
    "def generate_laporan_analisis_lengkap(df_berita, periode_start=None, periode_end=None):\n",
    "    \"\"\"\n",
    "    Generate laporan analisis media online dan media sosial lengkap\n",
    "    \n",
    "    Args:\n",
    "        df_berita (pd.DataFrame): DataFrame berita yang sudah dianalisis\n",
    "        periode_start (str): Tanggal mulai periode\n",
    "        periode_end (str): Tanggal akhir periode\n",
    "    \n",
    "    Returns:\n",
    "        str: Laporan lengkap dalam format yang diinginkan\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if df_berita.empty:\n",
    "            return \"Tidak ada data berita untuk dianalisis.\"\n",
    "        \n",
    "        # Load config\n",
    "        with open('config.json', 'r', encoding='utf-8') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        # Header laporan\n",
    "        tanggal_laporan = format_tanggal_indonesia(datetime.now())\n",
    "        \n",
    "        laporan = f\"\"\"**Laporan Analisis Media Online dan Media Sosial**\n",
    "{tanggal_laporan}\n",
    "\n",
    "=========================================================\n",
    "\n",
    "**EXECUTIVE SUMMARY**\n",
    "=========================================================\n",
    "\"\"\"\n",
    "        \n",
    "        # Kategorisasi berita\n",
    "        categorized_berita = categorize_berita(df_berita)\n",
    "        \n",
    "        # Executive Summary\n",
    "        exec_summary = generate_executive_summary(df_berita, categorized_berita)\n",
    "        laporan += exec_summary\n",
    "        \n",
    "        laporan += \"\\n\\n=========================================================\\n\\n\"\n",
    "        \n",
    "        # Section Media Online\n",
    "        laporan += \"**MEDIA ONLINE**\\n\\n\"\n",
    "        \n",
    "        # Topik Berita\n",
    "        topic_keywords = config.get('topic_keywords', [])\n",
    "        topik_text = \", \".join(topic_keywords) if topic_keywords else \"Beragam topik\"\n",
    "        laporan += f\"**Topik Berita:** {topik_text}\\n\\n\"\n",
    "        \n",
    "        # Tonasi Berita\n",
    "        sentiment_summary = generate_sentiment_summary(df_berita)\n",
    "        if sentiment_summary['positif'] > sentiment_summary['negatif']:\n",
    "            tonasi_dominan = \"Positif\"\n",
    "        elif sentiment_summary['negatif'] > sentiment_summary['positif']:\n",
    "            tonasi_dominan = \"Negatif\"\n",
    "        else:\n",
    "            tonasi_dominan = \"Netral\"\n",
    "        \n",
    "        laporan += f\"**Tonasi Berita:** {tonasi_dominan}\\n\\n\"\n",
    "        \n",
    "        # Pesan Kunci dan Analisis\n",
    "        laporan += \"**Pesan Kunci dan Analisis:**\\n\\n\"\n",
    "        \n",
    "        # ISU KEMENKEU\n",
    "        laporan += \"**ISU KEMENKEU**\\n\"\n",
    "        if not categorized_berita['kemenkeu'].empty:\n",
    "            for i, (idx, row) in enumerate(categorized_berita['kemenkeu'].head(5).iterrows(), 1):\n",
    "                if 'ai_resume' in row and pd.notna(row['ai_resume']):\n",
    "                    desc = row['ai_resume']\n",
    "                else:\n",
    "                    desc = row['artikel_berita_bersih'][:200] + \"...\"\n",
    "                laporan += f\"{i}. {desc}\\n\"\n",
    "        else:\n",
    "            laporan += \"1. Tidak ada isu Kemenkeu yang dominan pada periode ini\\n\"\n",
    "        \n",
    "        laporan += \"\\n\"\n",
    "        \n",
    "        # ISU NASIONAL DAN INTERNASIONAL\n",
    "        laporan += \"**ISU NASIONAL DAN INTERNASIONAL**\\n\"\n",
    "        if not categorized_berita['nasional_internasional'].empty:\n",
    "            for i, (idx, row) in enumerate(categorized_berita['nasional_internasional'].head(5).iterrows(), 1):\n",
    "                if 'ai_resume' in row and pd.notna(row['ai_resume']):\n",
    "                    desc = row['ai_resume']\n",
    "                else:\n",
    "                    desc = row['artikel_berita_bersih'][:200] + \"...\"\n",
    "                laporan += f\"{i}. {desc}\\n\"\n",
    "        else:\n",
    "            laporan += \"1. Tidak ada isu nasional/internasional yang signifikan\\n\"\n",
    "        \n",
    "        laporan += \"\\n\"\n",
    "        \n",
    "        # Kegiatan yang dirujuk\n",
    "        laporan += f\"**Kegiatan yang dirujuk:** Kegiatan Pemantauan Berita, {tanggal_laporan}\\n\\n\"\n",
    "        \n",
    "        # Narasumber utama\n",
    "        narasumber_list = extract_narasumber(df_berita)\n",
    "        narasumber_text = \", \".join(narasumber_list[:3]) if len(narasumber_list) > 0 else \"Belum ada narasumber\"\n",
    "        laporan += f\"**Narasumber utama yang dirujuk:** {narasumber_text}\\n\\n\"\n",
    "        \n",
    "        # Daftar Berita\n",
    "        laporan += \"**Daftar Berita:**\\n\"\n",
    "        for i, (idx, row) in enumerate(df_berita.iterrows(), 1):\n",
    "            judul_clean = row['judul_berita'].replace('\\n', ' ').strip()\n",
    "            laporan += f\"{i}. {judul_clean}\\n[{row['url_berita']}]\\n\\n\"\n",
    "        \n",
    "        # Page separator\n",
    "        laporan += \"\\n===== Page 1 =====\\n\"\n",
    "        \n",
    "        return laporan\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating laporan analisis lengkap: {str(e)}\")\n",
    "        return f\"Error dalam generate laporan: {str(e)}\"\n",
    "\n",
    "def save_laporan_analisis_lengkap(laporan_text, output_dir=\"00_laporan_cetak\"):\n",
    "    \"\"\"\n",
    "    Simpan laporan analisis lengkap ke file\n",
    "    \n",
    "    Args:\n",
    "        laporan_text (str): Teks laporan\n",
    "        output_dir (str): Directory output\n",
    "    \n",
    "    Returns:\n",
    "        str: Path file yang disimpan\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Buat directory jika belum ada\n",
    "        Path(output_dir).mkdir(exist_ok=True)\n",
    "        \n",
    "        # Nama file dengan timestamp\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"laporan_analisis_media_{timestamp}.txt\"\n",
    "        filepath = Path(output_dir) / filename\n",
    "        \n",
    "        # Simpan file\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(laporan_text)\n",
    "        \n",
    "        logger.info(f\"Laporan analisis lengkap disimpan: {filepath}\")\n",
    "        return str(filepath)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving laporan analisis: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Test format dengan data dummy jika ada data\n",
    "if 'df_berita_penting' in locals() and not df_berita_penting.empty:\n",
    "    print(\"=== PREVIEW FORMAT LAPORAN ANALISIS LENGKAP ===\")\n",
    "    sample_laporan = generate_laporan_analisis_lengkap(df_berita_penting.head(3))\n",
    "    print(sample_laporan[:1000] + \"...\" if len(sample_laporan) > 1000 else sample_laporan)\n",
    "else:\n",
    "    print(\"Fungsi format laporan analisis lengkap telah disiapkan.\")\n",
    "    print(\"Gunakan: generate_laporan_analisis_lengkap(df_analyzed) untuk generate laporan lengkap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8e7e5af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 PIPELINE ANALISIS BERITA PENTING SIAP (Versi Parallel)!\n",
      "\n",
      "Pilihan penggunaan:\n",
      "\n",
      "1. QUICK PREVIEW (tanpa AI, tanpa save):\n",
      "   quick_preview()\n",
      "\n",
      "2. ANALISIS LENGKAP (dengan AI, otomatis pilih metode):\n",
      "   results = run_complete_analysis(\n",
      "       ai_provider=\"openai\", \n",
      "       api_key=\"your-openai-api-key\",\n",
      "       processing_method=\"auto\",  # auto/sequential/parallel/parallel_safe/parallel_v2/batch\n",
      "       save_outputs=True\n",
      "   )\n",
      "\n",
      "3. ANALISIS CEPAT 5 BERITA PERTAMA (uji cepat parallel_v2):\n",
      "   results = run_complete_analysis(\n",
      "       ai_provider=\"openai\",\n",
      "       api_key=\"your-openai-api-key\",\n",
      "       processing_method=\"parallel_v2\",\n",
      "       limit_articles=5,\n",
      "       save_outputs=False\n",
      "   )\n",
      "\n",
      "4. ANALISIS TANPA AI (hanya format):\n",
      "   results = run_complete_analysis(save_outputs=True)\n",
      "\n",
      "5. FUNGSI INDIVIDUAL:\n",
      "   - load_berita_penting()\n",
      "   - generate_daftar_berita_format(df)\n",
      "   - generate_news_update_format(df)\n",
      "   - generate_laporan_analisis_lengkap(df)\n",
      "\n",
      "Gunakan processing_method=\"parallel_v2\" untuk kecepatan + stabilitas, \"parallel_safe\" jika ingin extra aman.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IMPLEMENTASI LENGKAP PIPELINE ANALISIS BERITA PENTING (UPDATED: parallel support)\n",
    "# =============================================================================\n",
    "\n",
    "def run_complete_analysis(\n",
    "    ai_provider=\"openai\",\n",
    "    api_key=None,\n",
    "    save_outputs=True,\n",
    "    processing_method=\"auto\",  # 'auto' | 'sequential' | 'parallel' | 'parallel_safe' | 'parallel_v2' | 'batch'\n",
    "    limit_articles=None         # batasi jumlah berita untuk uji cepat (None = semua)\n",
    "):\n",
    "    \"\"\"\n",
    "    Menjalankan pipeline lengkap analisis berita penting dengan dukungan metode pemrosesan fleksibel.\n",
    "\n",
    "    Args:\n",
    "        ai_provider (str): Provider AI (\"openai\" atau \"deepseek\")\n",
    "        api_key (str): API key untuk AI provider  \n",
    "        save_outputs (bool): Simpan output ke file atau tidak\n",
    "        processing_method (str): Metode analisis ('auto','sequential','parallel','parallel_safe','parallel_v2','batch')\n",
    "        limit_articles (int|None): Jika diset, hanya analisis N berita pertama (untuk testing)\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary berisi semua hasil analisis\n",
    "    \"\"\"\n",
    "    print(\"🚀 Memulai pipeline analisis berita penting...\")\n",
    "    results = {}\n",
    "\n",
    "    def _choose_method(n):\n",
    "        # Fallback jika PERFORMANCE_CONFIG belum didefinisikan\n",
    "        seq_thr = PERFORMANCE_CONFIG.get('sequential_threshold', 10) if 'PERFORMANCE_CONFIG' in globals() else 10\n",
    "        par_thr = PERFORMANCE_CONFIG.get('parallel_threshold', 50) if 'PERFORMANCE_CONFIG' in globals() else 50\n",
    "        if n <= seq_thr:\n",
    "            return 'sequential'\n",
    "        elif n <= par_thr:\n",
    "            return 'parallel_v2'  # gunakan versi parallel v2 (stabil + cepat)\n",
    "        else:\n",
    "            return 'batch'\n",
    "\n",
    "    try:\n",
    "        # 1. Load berita penting\n",
    "        print(\"\\n📊 Step 1: Loading berita penting...\")\n",
    "        df_berita = load_berita_penting()\n",
    "\n",
    "        if df_berita.empty:\n",
    "            print(\"❌ Tidak ada berita penting yang memenuhi kriteria!\")\n",
    "            return {'error': 'No data'}\n",
    "\n",
    "        # Optional limit for quick test\n",
    "        if isinstance(limit_articles, int) and limit_articles > 0:\n",
    "            df_berita = df_berita.head(limit_articles).copy()\n",
    "            print(f\"⚡ Mode uji cepat: hanya {len(df_berita)} berita pertama dianalisis\")\n",
    "\n",
    "        print(f\"✅ Berhasil memuat {len(df_berita)} berita penting\")\n",
    "        results['raw_data'] = df_berita\n",
    "\n",
    "        # 2. Analisis AI (opsional jika ada API key)\n",
    "        if api_key:\n",
    "            print(f\"\\n🤖 Step 2: Analisis AI ({ai_provider.upper()})...\")\n",
    "\n",
    "            # Tentukan metode\n",
    "            chosen_method = processing_method\n",
    "            if processing_method == 'auto':\n",
    "                chosen_method = _choose_method(len(df_berita))\n",
    "            print(f\"🧠 Metode analisis dipilih: {chosen_method}\")\n",
    "\n",
    "            df_analyzed = df_berita.copy()\n",
    "            analysis_error = None\n",
    "\n",
    "            try:\n",
    "                if chosen_method == 'sequential':\n",
    "                    print(\"📝 Menjalankan sequential processing...\")\n",
    "                    df_analyzed = analyze_berita_batch(df_berita, ai_provider, api_key)\n",
    "                elif chosen_method == 'parallel':\n",
    "                    print(\"🧵 Menjalankan parallel processing (ThreadPoolExecutor klasik)...\")\n",
    "                    df_analyzed = analyze_berita_parallel(df_berita, ai_provider=ai_provider, api_key=api_key, max_workers=PERFORMANCE_CONFIG.get('max_workers', 3))\n",
    "                elif chosen_method == 'parallel_safe':\n",
    "                    print(\"🛡️  Menjalankan safe parallel processing...\")\n",
    "                    df_analyzed = analyze_berita_parallel_safe(df_berita, ai_provider=ai_provider, api_key=api_key, max_workers=min(2, PERFORMANCE_CONFIG.get('max_workers', 3)))\n",
    "                elif chosen_method == 'parallel_v2':\n",
    "                    print(\"⚡ Menjalankan parallel_v2 (index reset + assignment aman)...\")\n",
    "                    df_analyzed = analyze_berita_parallel_v2(df_berita, ai_provider=ai_provider, api_key=api_key, max_workers=PERFORMANCE_CONFIG.get('max_workers', 3))\n",
    "                elif chosen_method == 'batch':\n",
    "                    print(\"📦 Menjalankan batch + parallel processing...\")\n",
    "                    df_analyzed = analyze_berita_batch_optimized(\n",
    "                        df_berita,\n",
    "                        ai_provider=ai_provider,\n",
    "                        api_key=api_key,\n",
    "                        batch_size=PERFORMANCE_CONFIG.get('batch_size', 10),\n",
    "                        max_workers=PERFORMANCE_CONFIG.get('max_workers', 3)\n",
    "                    )\n",
    "                else:\n",
    "                    print(\"⚠️  Metode tidak dikenali, fallback ke sequential\")\n",
    "                    df_analyzed = analyze_berita_batch(df_berita, ai_provider, api_key)\n",
    "            except Exception as e:\n",
    "                analysis_error = e\n",
    "                logger.error(f\"Error pada metode {chosen_method}: {e}\")\n",
    "\n",
    "            # Fallback jika gagal\n",
    "            if analysis_error is not None:\n",
    "                try:\n",
    "                    print(f\"🔁 Fallback ke sequential karena error: {analysis_error}\")\n",
    "                    df_analyzed = analyze_berita_batch(df_berita, ai_provider, api_key)\n",
    "                except Exception as e2:\n",
    "                    print(f\"❌ Fallback sequential juga gagal: {e2}\")\n",
    "                    logger.error(f\"Sequential fallback failed: {e2}\")\n",
    "                    df_analyzed = df_berita  # tanpa kolom AI\n",
    "            else:\n",
    "                print(\"✅ Analisis AI selesai dengan metode:\", chosen_method)\n",
    "        else:\n",
    "            print(\"\\n⚠️  Step 2: Skip analisis AI (tidak ada API key)\")\n",
    "            df_analyzed = df_berita\n",
    "\n",
    "        results['analyzed_data'] = df_analyzed\n",
    "\n",
    "        # 3. Generate Format Daftar Berita\n",
    "        print(\"\\n📋 Step 3: Generate Daftar Berita...\")\n",
    "        daftar_berita = generate_daftar_berita_format(df_analyzed)\n",
    "        results['daftar_berita'] = daftar_berita\n",
    "\n",
    "        if save_outputs:\n",
    "            daftar_path = save_daftar_berita(daftar_berita)\n",
    "            results['daftar_berita_file'] = daftar_path\n",
    "            print(f\"✅ Daftar berita disimpan: {daftar_path}\")\n",
    "\n",
    "        # 4. Generate News Update\n",
    "        print(\"\\n📰 Step 4: Generate News Update...\")\n",
    "        news_update = generate_news_update_format(df_analyzed)\n",
    "        results['news_update'] = news_update\n",
    "\n",
    "        if save_outputs:\n",
    "            news_update_path = save_news_update(news_update)\n",
    "            results['news_update_file'] = news_update_path\n",
    "            print(f\"✅ News update disimpan: {news_update_path}\")\n",
    "\n",
    "        # 5. Generate Laporan Analisis Lengkap\n",
    "        print(\"\\n📊 Step 5: Generate Laporan Analisis Lengkap...\")\n",
    "        laporan_lengkap = generate_laporan_analisis_lengkap(df_analyzed)\n",
    "        results['laporan_lengkap'] = laporan_lengkap\n",
    "\n",
    "        if save_outputs:\n",
    "            laporan_path = save_laporan_analisis_lengkap(laporan_lengkap)\n",
    "            results['laporan_lengkap_file'] = laporan_path\n",
    "            print(f\"✅ Laporan lengkap disimpan: {laporan_path}\")\n",
    "\n",
    "        # 6. Summary hasil\n",
    "        print(f\"\\n🎉 Pipeline selesai! Summary:\")\n",
    "        print(f\"   - Total berita dianalisis: {len(df_analyzed)}\")\n",
    "        if 'sentimen' in df_analyzed.columns:\n",
    "            print(f\"   - Sentimen positif: {len(df_analyzed[df_analyzed['sentimen'] == 'positif'])}\")\n",
    "            print(f\"   - Sentimen negatif: {len(df_analyzed[df_analyzed['sentimen'] == 'negatif'])}\")\n",
    "            print(f\"   - Sentimen netral: {len(df_analyzed[df_analyzed['sentimen'] == 'netral'])}\")\n",
    "        ai_cols = [c for c in df_analyzed.columns if c.startswith('ai_')]\n",
    "        if ai_cols:\n",
    "            filled = df_analyzed['ai_resume'].notna().sum() if 'ai_resume' in df_analyzed.columns else 'N/A'\n",
    "            print(f\"   - Kolom AI: {ai_cols} (resume terisi: {filled})\")\n",
    "\n",
    "        if save_outputs:\n",
    "            print(f\"\\n📁 File output tersimpan di: ./00_laporan_cetak/\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error dalam pipeline: {str(e)}\")\n",
    "        print(f\"❌ Error: {str(e)}\")\n",
    "        return {'error': str(e)}\n",
    "\n",
    "def quick_preview():\n",
    "    \"\"\"\n",
    "    Quick preview hasil analisis tanpa AI dan tanpa save file\n",
    "    \"\"\"\n",
    "    print(\"🔍 Quick Preview Mode...\")\n",
    "\n",
    "    try:\n",
    "        # Load data\n",
    "        df_berita = load_berita_penting()\n",
    "\n",
    "        if df_berita.empty:\n",
    "            print(\"❌ Tidak ada data berita penting\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\n📊 Data Overview:\")\n",
    "        print(f\"   Total berita: {len(df_berita)}\")\n",
    "        if 'subtopik_llm' in df_berita.columns:\n",
    "            print(f\"   Topik utama: {df_berita['subtopik_llm'].value_counts().head().to_dict()}\")\n",
    "        if 'sentimen' in df_berita.columns:\n",
    "            print(f\"   Sentimen: {df_berita['sentimen'].value_counts().to_dict()}\")\n",
    "        if 'importance' in df_berita.columns:\n",
    "            print(f\"   Rata-rata importance: {df_berita['importance'].mean():.1f}\")\n",
    "\n",
    "        # Preview format\n",
    "        print(f\"\\n📋 Preview Daftar Berita (3 teratas):\")\n",
    "        print(\"-\" * 50)\n",
    "        preview_daftar = generate_daftar_berita_format(df_berita.head(3))\n",
    "        print(preview_daftar)\n",
    "\n",
    "        print(f\"\\n📰 Preview News Update (3 teratas):\")\n",
    "        print(\"-\" * 50)\n",
    "        preview_news = generate_news_update_format(df_berita.head(3))\n",
    "        print(preview_news[:500] + \"...\" if len(preview_news) > 500 else preview_news)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# CONTOH PENGGUNAAN (UPDATED)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\"\"\n",
    "🎯 PIPELINE ANALISIS BERITA PENTING SIAP (Versi Parallel)!\n",
    "\n",
    "Pilihan penggunaan:\n",
    "\n",
    "1. QUICK PREVIEW (tanpa AI, tanpa save):\n",
    "   quick_preview()\n",
    "\n",
    "2. ANALISIS LENGKAP (dengan AI, otomatis pilih metode):\n",
    "   results = run_complete_analysis(\n",
    "       ai_provider=\"openai\", \n",
    "       api_key=\"your-openai-api-key\",\n",
    "       processing_method=\"auto\",  # auto/sequential/parallel/parallel_safe/parallel_v2/batch\n",
    "       save_outputs=True\n",
    "   )\n",
    "\n",
    "3. ANALISIS CEPAT 5 BERITA PERTAMA (uji cepat parallel_v2):\n",
    "   results = run_complete_analysis(\n",
    "       ai_provider=\"openai\",\n",
    "       api_key=\"your-openai-api-key\",\n",
    "       processing_method=\"parallel_v2\",\n",
    "       limit_articles=5,\n",
    "       save_outputs=False\n",
    "   )\n",
    "\n",
    "4. ANALISIS TANPA AI (hanya format):\n",
    "   results = run_complete_analysis(save_outputs=True)\n",
    "\n",
    "5. FUNGSI INDIVIDUAL:\n",
    "   - load_berita_penting()\n",
    "   - generate_daftar_berita_format(df)\n",
    "   - generate_news_update_format(df)\n",
    "   - generate_laporan_analisis_lengkap(df)\n",
    "\n",
    "Gunakan processing_method=\"parallel_v2\" untuk kecepatan + stabilitas, \"parallel_safe\" jika ingin extra aman.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b3c5421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Keys - diganti dengan konfigurasi lengkap di cell selanjutnya\n",
    "DEEPSEEK_API_KEY = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e9e17f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️  Konfigurasi performance telah dimuat!\n",
      "🔧 Settings:\n",
      "   max_workers: 3\n",
      "   delay_between_requests: 0.5\n",
      "   batch_size: 10\n",
      "   delay_between_batches: 2\n",
      "   sequential_threshold: 10\n",
      "   parallel_threshold: 50\n",
      "   request_timeout: 30\n",
      "   max_retries: 2\n",
      "\n",
      "📊 Fungsi estimasi waktu tersedia: estimate_processing_time(num_berita)\n",
      "💡 Tips: Jalankan estimate_processing_time(len(df_berita)) sebelum analisis!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# KONFIGURASI OPTIMASI DAN PERFORMANCE SETTINGS  \n",
    "# =============================================================================\n",
    "\n",
    "# API Keys\n",
    "DEEPSEEK_API_KEY = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Performance Settings\n",
    "PERFORMANCE_CONFIG = {\n",
    "    # Parallel processing settings\n",
    "    'max_workers': 3,           # Jumlah thread maksimum (sesuaikan dengan rate limit)\n",
    "    'delay_between_requests': 0.5,  # Delay antar request (detik)\n",
    "    'batch_size': 10,           # Ukuran batch untuk dataset besar\n",
    "    'delay_between_batches': 2, # Delay antar batch (detik)\n",
    "    \n",
    "    # Threshold untuk memilih metode processing\n",
    "    'sequential_threshold': 10,  # Jika ≤ 10 berita, gunakan sequential\n",
    "    'parallel_threshold': 50,    # Jika ≤ 50 berita, gunakan parallel\n",
    "    # Jika > 50 berita, gunakan batch + parallel\n",
    "    \n",
    "    # Timeout dan retry settings  \n",
    "    'request_timeout': 30,      # Timeout per request (detik)\n",
    "    'max_retries': 2,          # Maksimum retry per request\n",
    "}\n",
    "\n",
    "print(\"⚙️  Konfigurasi performance telah dimuat!\")\n",
    "print(f\"🔧 Settings:\")\n",
    "for key, value in PERFORMANCE_CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "def estimate_processing_time(num_berita, method=\"auto\"):\n",
    "    \"\"\"\n",
    "    Estimasi waktu processing berdasarkan jumlah berita dan metode\n",
    "    \n",
    "    Args:\n",
    "        num_berita (int): Jumlah berita\n",
    "        method (str): Metode processing (\"sequential\", \"parallel\", \"batch\", \"auto\")\n",
    "    \n",
    "    Returns:\n",
    "        dict: Estimasi waktu untuk berbagai metode\n",
    "    \"\"\"\n",
    "    # Estimasi waktu per berita (detik)\n",
    "    time_per_berita = {\n",
    "        'sequential': 1.5,     # 1.5 detik per berita\n",
    "        'parallel': 0.5,       # ~3x lebih cepat dengan 3 workers\n",
    "        'batch': 0.4,          # Sedikit lebih cepat karena batch optimization\n",
    "    }\n",
    "    \n",
    "    estimates = {}\n",
    "    for method_name, time_per in time_per_berita.items():\n",
    "        total_time = num_berita * time_per\n",
    "        estimates[method_name] = {\n",
    "            'time_seconds': total_time,\n",
    "            'time_minutes': total_time / 60,\n",
    "            'time_formatted': f\"{total_time//60:.0f}m {total_time%60:.0f}s\" if total_time > 60 else f\"{total_time:.1f}s\"\n",
    "        }\n",
    "    \n",
    "    if method == \"auto\":\n",
    "        if num_berita <= PERFORMANCE_CONFIG['sequential_threshold']:\n",
    "            recommended = 'sequential'\n",
    "        elif num_berita <= PERFORMANCE_CONFIG['parallel_threshold']:\n",
    "            recommended = 'parallel'  \n",
    "        else:\n",
    "            recommended = 'batch'\n",
    "        \n",
    "        estimates['recommended'] = {\n",
    "            'method': recommended,\n",
    "            **estimates[recommended]\n",
    "        }\n",
    "    \n",
    "    return estimates\n",
    "\n",
    "print(\"\\n📊 Fungsi estimasi waktu tersedia: estimate_processing_time(num_berita)\")\n",
    "print(\"💡 Tips: Jalankan estimate_processing_time(len(df_berita)) sebelum analisis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "44812e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fungsi parallel processing yang aman telah disiapkan!\n",
      "📝 Gunakan analyze_berita_parallel_safe() untuk analisis yang lebih stabil\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PERBAIKAN UNTUK PARALLEL PROCESSING ERRORS\n",
    "# =============================================================================\n",
    "\n",
    "def safe_update_dataframe(df_target, df_source, prefix=\"ai_\"):\n",
    "    \"\"\"\n",
    "    Safely update DataFrame dengan hasil dari parallel processing\n",
    "    \n",
    "    Args:\n",
    "        df_target (pd.DataFrame): Target DataFrame\n",
    "        df_source (pd.DataFrame): Source DataFrame dengan hasil AI\n",
    "        prefix (str): Prefix untuk kolom baru\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df_result = df_target.copy()\n",
    "        \n",
    "        # Iterate through source DataFrame\n",
    "        for idx in df_source.index:\n",
    "            if idx in df_target.index:\n",
    "                # Get AI columns from source\n",
    "                ai_cols = [col for col in df_source.columns if col.startswith(prefix)]\n",
    "                for col in ai_cols:\n",
    "                    value = df_source.loc[idx, col]\n",
    "                    # Convert to string to avoid type issues\n",
    "                    df_result.loc[idx, col] = str(value) if pd.notna(value) else \"\"\n",
    "        \n",
    "        return df_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in safe_update_dataframe: {str(e)}\")\n",
    "        return df_target\n",
    "\n",
    "def analyze_berita_parallel_safe(df_berita, ai_provider=\"openai\", api_key=None, max_workers=2, delay=0.5):\n",
    "    \"\"\"\n",
    "    Versi aman dari parallel processing dengan error handling yang lebih baik\n",
    "    \n",
    "    Args:\n",
    "        df_berita (pd.DataFrame): DataFrame berita\n",
    "        ai_provider (str): Provider AI\n",
    "        api_key (str): API key\n",
    "        max_workers (int): Jumlah workers (dikurangi untuk stabilitas)\n",
    "        delay (float): Delay antar request\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame dengan hasil analisis\n",
    "    \"\"\"\n",
    "    if api_key is None or api_key.strip() == \"\":\n",
    "        logger.warning(\"API key tidak tersedia, skip analisis AI\")\n",
    "        return df_berita\n",
    "    \n",
    "    logger.info(f\"🚀 Memulai safe parallel analysis dengan {max_workers} workers...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    df_result = df_berita.copy()\n",
    "    \n",
    "    # Initialize AI columns\n",
    "    ai_columns = ['ai_resume', 'ai_dampak_kemenkeu', 'ai_alasan_dampak', 'ai_hal_menarik']\n",
    "    for col in ai_columns:\n",
    "        df_result[col] = \"\"\n",
    "    \n",
    "    # Persiapkan data untuk parallel processing\n",
    "    row_data = list(df_berita.iterrows())\n",
    "    \n",
    "    # Fungsi partial dengan parameter yang dikurangi untuk stabilitas\n",
    "    analyze_func = partial(\n",
    "        analyze_single_berita, \n",
    "        ai_provider=ai_provider, \n",
    "        api_key=api_key, \n",
    "        delay=delay\n",
    "    )\n",
    "    \n",
    "    # Parallel processing dengan error handling\n",
    "    results = {}\n",
    "    completed = 0\n",
    "    failed = 0\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit semua tugas\n",
    "        future_to_idx = {executor.submit(analyze_func, row): row[0] for row in row_data}\n",
    "        \n",
    "        # Collect hasil secara bertahap\n",
    "        for future in as_completed(future_to_idx):\n",
    "            try:\n",
    "                idx, analysis = future.result()\n",
    "                \n",
    "                if analysis and isinstance(analysis, dict):\n",
    "                    # Safely assign each AI column\n",
    "                    for key, value in analysis.items():\n",
    "                        col_name = f\"ai_{key}\"\n",
    "                        if col_name in ai_columns:\n",
    "                            df_result.loc[idx, col_name] = str(value) if value is not None else \"\"\n",
    "                    completed += 1\n",
    "                else:\n",
    "                    # Default values for failed analysis\n",
    "                    for col in ai_columns:\n",
    "                        df_result.loc[idx, col] = \"Analisis tidak tersedia\"\n",
    "                    failed += 1\n",
    "                \n",
    "                # Progress update\n",
    "                total_processed = completed + failed\n",
    "                progress = (total_processed / len(df_berita)) * 100\n",
    "                logger.info(f\"✅ Progress: {total_processed}/{len(df_berita)} ({progress:.1f}%) - Success: {completed}, Failed: {failed}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                original_idx = future_to_idx[future]\n",
    "                logger.error(f\"❌ Error processing berita {original_idx}: {str(e)}\")\n",
    "                \n",
    "                # Set default values for error case\n",
    "                for col in ai_columns:\n",
    "                    df_result.loc[original_idx, col] = f\"Error: {str(e)}\"\n",
    "                failed += 1\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    logger.info(f\"🎉 Safe parallel analysis selesai dalam {elapsed:.2f} detik\")\n",
    "    logger.info(f\"📊 Hasil: {completed} berhasil, {failed} gagal dari {len(df_berita)} berita\")\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "print(\"✅ Fungsi parallel processing yang aman telah disiapkan!\")\n",
    "print(\"📝 Gunakan analyze_berita_parallel_safe() untuk analisis yang lebih stabil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d17384bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VERSI PERBAIKAN: analyze_berita_parallel DENGAN NORMALISASI INDEX\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_berita_parallel_v2(df_berita, ai_provider=\"openai\", api_key=None, max_workers=3, delay=0.5):\n",
    "    \"\"\"\n",
    "    Versi revisi dari analyze_berita_parallel dengan:\n",
    "    - Reset index untuk menghindari mismatch\n",
    "    - Inisialisasi kolom AI terlebih dahulu\n",
    "    - Assignment langsung menggunakan index baru (0..n-1)\n",
    "    - Mengembalikan DataFrame dengan index asli (kolom original_index)\n",
    "    \"\"\"\n",
    "    if api_key is None or api_key.strip() == \"\":\n",
    "        logger.warning(\"API key tidak tersedia, skip analisis AI\")\n",
    "        return df_berita\n",
    "\n",
    "    logger.info(f\"🚀 (v2) Memulai analisis parallel dengan {max_workers} workers...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Simpan index asli dan reset index\n",
    "    df_work = df_berita.copy()\n",
    "    df_work = df_work.reset_index().rename(columns={'index': 'original_index'})\n",
    "\n",
    "    # Inisialisasi kolom AI\n",
    "    ai_columns = ['ai_resume', 'ai_dampak_kemenkeu', 'ai_alasan_dampak', 'ai_hal_menarik']\n",
    "    for col in ai_columns:\n",
    "        if col not in df_work.columns:\n",
    "            df_work[col] = \"\"\n",
    "\n",
    "    # Data untuk parallel (gunakan itertuples agar lebih ringan)\n",
    "    row_data = list(df_work.itertuples(index=False))  # each is a namedtuple\n",
    "\n",
    "    def _process_row(nt_row):\n",
    "        try:\n",
    "            judul = getattr(nt_row, 'judul_berita')\n",
    "            artikel = getattr(nt_row, 'artikel_berita_bersih')\n",
    "            source_domain = getattr(nt_row, 'source_domain')\n",
    "            idx_local = getattr(nt_row, 'original_index')\n",
    "\n",
    "            prompt = create_analysis_prompt(judul, artikel, source_domain)\n",
    "            if ai_provider == \"openai\":\n",
    "                analysis = analyze_with_openai(prompt, api_key)\n",
    "            elif ai_provider == \"deepseek\":\n",
    "                analysis = analyze_with_deepseek(prompt, api_key)\n",
    "            else:\n",
    "                raise ValueError(f\"AI provider tidak dikenali: {ai_provider}\")\n",
    "\n",
    "            if delay > 0:\n",
    "                time.sleep(delay)\n",
    "\n",
    "            if analysis and isinstance(analysis, dict):\n",
    "                return (idx_local, analysis)\n",
    "            else:\n",
    "                return (idx_local, {\n",
    "                    'resume': 'Analisis tidak tersedia',\n",
    "                    'dampak_kemenkeu': 'netral',\n",
    "                    'alasan_dampak': 'Tidak dapat dianalisis',\n",
    "                    'hal_menarik': 'Tidak dapat dianalisis'\n",
    "                })\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error (v2) processing berita {idx_local}: {e}\")\n",
    "            return (idx_local, {\n",
    "                'resume': 'Error dalam analisis',\n",
    "                'dampak_kemenkeu': 'netral',\n",
    "                'alasan_dampak': f'Error: {e}',\n",
    "                'hal_menarik': 'Tidak dapat dianalisis'\n",
    "            })\n",
    "\n",
    "    results = {}\n",
    "    completed = 0\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_idx = {executor.submit(_process_row, nt_row): getattr(nt_row, 'original_index') for nt_row in row_data}\n",
    "        for future in as_completed(future_to_idx):\n",
    "            try:\n",
    "                idx_orig, analysis = future.result()\n",
    "                results[idx_orig] = analysis\n",
    "                completed += 1\n",
    "                progress = (completed / len(row_data)) * 100\n",
    "                logger.info(f\"✅ (v2) Progress: {completed}/{len(row_data)} ({progress:.1f}%)\")\n",
    "            except Exception as e:\n",
    "                idx_orig = future_to_idx[future]\n",
    "                logger.error(f\"❌ (v2) Error future idx {idx_orig}: {e}\")\n",
    "                results[idx_orig] = {\n",
    "                    'resume': 'Error future',\n",
    "                    'dampak_kemenkeu': 'netral',\n",
    "                    'alasan_dampak': f'Future error: {e}',\n",
    "                    'hal_menarik': 'Tidak dapat dianalisis'\n",
    "                }\n",
    "\n",
    "    # Map hasil ke df_work berdasarkan original_index\n",
    "    for idx_orig, analysis in results.items():\n",
    "        mask = df_work['original_index'] == idx_orig\n",
    "        for key, value in analysis.items():\n",
    "            col_name = f\"ai_{key}\"\n",
    "            if col_name in ai_columns:\n",
    "                df_work.loc[mask, col_name] = str(value) if value is not None else \"\"\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    logger.info(f\"🎉 (v2) Analisis parallel selesai dalam {elapsed:.2f} detik\")\n",
    "\n",
    "    # Kembalikan ke index asli\n",
    "    df_work = df_work.set_index('original_index')\n",
    "    # Pastikan urutan sesuai df_berita awal\n",
    "    df_work = df_work.loc[df_berita.index]\n",
    "\n",
    "    return df_work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c7886172",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 15:25:28,070 - INFO - Membaca file analisis AI: 00_hasil_analisis/seluruh_berita/analisis_ai_20250930_deepseek_default.csv\n",
      "2025-09-30 15:25:28,076 - INFO - Total berita: 225\n",
      "2025-09-30 15:25:28,077 - INFO - Berita penting (filtered): 107\n",
      "2025-09-30 15:25:28,077 - INFO - 🚀 (v2) Memulai analisis parallel dengan 3 workers...\n",
      "2025-09-30 15:25:28,076 - INFO - Total berita: 225\n",
      "2025-09-30 15:25:28,077 - INFO - Berita penting (filtered): 107\n",
      "2025-09-30 15:25:28,077 - INFO - 🚀 (v2) Memulai analisis parallel dengan 3 workers...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Testing pipeline lengkap (parallel_v2 5 berita)...\n",
      "\n",
      "⏱️  Estimasi waktu processing:\n",
      "   Sequential: 2m 40s\n",
      "   Parallel: 53.5s\n",
      "   Recommended method: batch (42.8s)\n",
      "🚀 Memulai pipeline analisis berita penting...\n",
      "\n",
      "📊 Step 1: Loading berita penting...\n",
      "⚡ Mode uji cepat: hanya 5 berita pertama dianalisis\n",
      "✅ Berhasil memuat 5 berita penting\n",
      "\n",
      "🤖 Step 2: Analisis AI (OPENAI)...\n",
      "🧠 Metode analisis dipilih: parallel_v2\n",
      "⚡ Menjalankan parallel_v2 (index reset + assignment aman)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 15:25:33,545 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:25:34,054 - INFO - ✅ (v2) Progress: 1/5 (20.0%)\n",
      "2025-09-30 15:25:34,054 - INFO - ✅ (v2) Progress: 1/5 (20.0%)\n",
      "2025-09-30 15:25:34,269 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:25:34,269 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:25:34,377 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:25:34,377 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:25:34,779 - INFO - ✅ (v2) Progress: 2/5 (40.0%)\n",
      "2025-09-30 15:25:34,779 - INFO - ✅ (v2) Progress: 2/5 (40.0%)\n",
      "2025-09-30 15:25:34,885 - INFO - ✅ (v2) Progress: 3/5 (60.0%)\n",
      "2025-09-30 15:25:34,885 - INFO - ✅ (v2) Progress: 3/5 (60.0%)\n",
      "2025-09-30 15:25:40,486 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:25:40,486 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:25:41,005 - INFO - ✅ (v2) Progress: 4/5 (80.0%)\n",
      "2025-09-30 15:25:41,005 - INFO - ✅ (v2) Progress: 4/5 (80.0%)\n",
      "2025-09-30 15:25:42,818 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:25:42,818 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:25:43,326 - INFO - ✅ (v2) Progress: 5/5 (100.0%)\n",
      "2025-09-30 15:25:43,336 - INFO - 🎉 (v2) Analisis parallel selesai dalam 15.26 detik\n",
      "2025-09-30 15:25:43,326 - INFO - ✅ (v2) Progress: 5/5 (100.0%)\n",
      "2025-09-30 15:25:43,336 - INFO - 🎉 (v2) Analisis parallel selesai dalam 15.26 detik\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Analisis AI selesai dengan metode: parallel_v2\n",
      "\n",
      "📋 Step 3: Generate Daftar Berita...\n",
      "\n",
      "📰 Step 4: Generate News Update...\n",
      "\n",
      "📊 Step 5: Generate Laporan Analisis Lengkap...\n",
      "\n",
      "🎉 Pipeline selesai! Summary:\n",
      "   - Total berita dianalisis: 5\n",
      "   - Sentimen positif: 4\n",
      "   - Sentimen negatif: 1\n",
      "   - Sentimen netral: 0\n",
      "   - Kolom AI: ['ai_resume', 'ai_dampak_kemenkeu', 'ai_alasan_dampak', 'ai_hal_menarik'] (resume terisi: 5)\n",
      "\n",
      "🎉 Pipeline (uji cepat) berhasil dijalankan!\n",
      "\n",
      "📊 Hasil analisis AI (sample):\n",
      "   Kolom AI yang ditambahkan: ['ai_resume', 'ai_dampak_kemenkeu', 'ai_alasan_dampak', 'ai_hal_menarik']\n",
      "\n",
      "📝 Sample analisis berita pertama:\n",
      "   Judul: Menkeu Purbaya Sidak ke Kantor Pusat BNI, Ada apa? - Liputan6.com...\n",
      "   Resume: Menteri Keuangan Purbaya Yudhi Sadewa melakukan inspeksi mendadak ke kantor pusat BNI untuk memantau...\n",
      "   Dampak Kemenkeu: positif\n",
      "   Alasan: Berita ini berdampak positif terhadap Kementerian Keuangan karena menunjukkan komitmen Menkeu dalam ...\n",
      "\n",
      "📈 Statistik AI Analysis:\n",
      "   Total berita dianalisis: 5\n",
      "   Berhasil dianalisis AI: 5\n",
      "   Success rate: 100.0%\n",
      "\n",
      "📁 (Uji cepat) Save outputs dimatikan, tidak ada file yang dibuat.\n",
      "\n",
      "✅ Testing selesai!\n"
     ]
    }
   ],
   "source": [
    "# Test final: Pipeline lengkap dengan perbaikan error handling + parallel_v2 test\n",
    "print(\"🚀 Testing pipeline lengkap (parallel_v2 5 berita)...\")\n",
    "\n",
    "# Estimasi waktu sebelum mulai\n",
    "if 'df_berita_penting' in locals() and not df_berita_penting.empty:\n",
    "    time_estimate = estimate_processing_time(len(df_berita_penting))\n",
    "    print(f\"\\n⏱️  Estimasi waktu processing:\")\n",
    "    print(f\"   Sequential: {time_estimate['sequential']['time_formatted']}\")\n",
    "    print(f\"   Parallel: {time_estimate['parallel']['time_formatted']}\")\n",
    "    print(f\"   Recommended method: {time_estimate['recommended']['method']} ({time_estimate['recommended']['time_formatted']})\")\n",
    "\n",
    "# Jalankan pipeline lengkap dengan parallel_v2 hanya 5 berita (uji cepat)\n",
    "try:\n",
    "    results_fixed = run_complete_analysis(\n",
    "        ai_provider=\"openai\", \n",
    "        api_key=OPENAI_API_KEY,\n",
    "        processing_method=\"parallel_v2\",\n",
    "        limit_articles=5,\n",
    "        save_outputs=False\n",
    "    )\n",
    "\n",
    "    # Tampilkan summary hasil\n",
    "    if 'error' not in results_fixed:\n",
    "        print(\"\\n🎉 Pipeline (uji cepat) berhasil dijalankan!\")\n",
    "        \n",
    "        analyzed_df = results_fixed.get('analyzed_data')\n",
    "        if analyzed_df is not None and not analyzed_df.empty:\n",
    "            print(f\"\\n📊 Hasil analisis AI (sample):\")\n",
    "            ai_columns = [col for col in analyzed_df.columns if col.startswith('ai_')]\n",
    "            if ai_columns:\n",
    "                print(f\"   Kolom AI yang ditambahkan: {ai_columns}\")\n",
    "                \n",
    "                # Show sample AI analysis\n",
    "                first_analysis = analyzed_df.iloc[0]\n",
    "                print(f\"\\n📝 Sample analisis berita pertama:\")\n",
    "                print(f\"   Judul: {first_analysis.get('judul_berita', 'N/A')[:80]}...\")\n",
    "                print(f\"   Resume: {first_analysis.get('ai_resume', 'N/A')[:100]}...\")\n",
    "                print(f\"   Dampak Kemenkeu: {first_analysis.get('ai_dampak_kemenkeu', 'N/A')}\")\n",
    "                print(f\"   Alasan: {first_analysis.get('ai_alasan_dampak', 'N/A')[:100]}...\")\n",
    "                \n",
    "                # Statistik AI analysis\n",
    "                non_empty_resume = analyzed_df[analyzed_df['ai_resume'].notna() & (analyzed_df['ai_resume'] != \"\")].shape[0] if 'ai_resume' in analyzed_df.columns else 0\n",
    "                print(f\"\\n📈 Statistik AI Analysis:\")\n",
    "                print(f\"   Total berita dianalisis: {len(analyzed_df)}\")\n",
    "                print(f\"   Berhasil dianalisis AI: {non_empty_resume}\")\n",
    "                if len(analyzed_df) > 0:\n",
    "                    print(f\"   Success rate: {(non_empty_resume/len(analyzed_df)*100):.1f}%\")\n",
    "            else:\n",
    "                print(\"   ⚠️  Tidak ada kolom AI yang ditambahkan\")\n",
    "        \n",
    "        print(f\"\\n📁 (Uji cepat) Save outputs dimatikan, tidak ada file yang dibuat.\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"❌ Pipeline gagal: {results_fixed.get('error')}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error dalam testing pipeline: {str(e)}\")\n",
    "    logger.error(f\"Pipeline testing error: {str(e)}\")\n",
    "    \n",
    "    # Fallback: coba quick preview\n",
    "    print(\"\\n🔄 Mencoba quick preview mode sebagai fallback...\")\n",
    "    try:\n",
    "        quick_preview()\n",
    "    except Exception as preview_error:\n",
    "        print(f\"❌ Quick preview juga gagal: {str(preview_error)}\")\n",
    "\n",
    "print(\"\\n✅ Testing selesai!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ddd8bb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 15:25:43,364 - INFO - Membaca file analisis AI: 00_hasil_analisis/seluruh_berita/analisis_ai_20250930_deepseek_default.csv\n",
      "2025-09-30 15:25:43,374 - INFO - Total berita: 225\n",
      "2025-09-30 15:25:43,374 - INFO - Total berita: 225\n",
      "2025-09-30 15:25:43,374 - INFO - Berita penting (filtered): 107\n",
      "2025-09-30 15:25:43,375 - INFO - 🔄 Memulai batch processing dengan ukuran batch: 10\n",
      "2025-09-30 15:25:43,375 - INFO - 📦 Processing batch 1/11 (rows 0-9)\n",
      "2025-09-30 15:25:43,376 - INFO - 🚀 Memulai analisis parallel dengan 3 workers...\n",
      "2025-09-30 15:25:43,374 - INFO - Berita penting (filtered): 107\n",
      "2025-09-30 15:25:43,375 - INFO - 🔄 Memulai batch processing dengan ukuran batch: 10\n",
      "2025-09-30 15:25:43,375 - INFO - 📦 Processing batch 1/11 (rows 0-9)\n",
      "2025-09-30 15:25:43,376 - INFO - 🚀 Memulai analisis parallel dengan 3 workers...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 MENJALANKAN FULL PIPELINE ANALISIS BERITA PENTING\n",
      "⚙️  Metode: auto | Limit: Semua\n",
      "⏱️  Estimasi waktu (berdasarkan jumlah berita saat ini):\n",
      "   Sequential : 2m 40s\n",
      "   Parallel   : 53.5s\n",
      "   Batch      : 42.8s\n",
      "   Rekomendasi: batch (42.8s)\n",
      "🚀 Memulai pipeline analisis berita penting...\n",
      "\n",
      "📊 Step 1: Loading berita penting...\n",
      "✅ Berhasil memuat 107 berita penting\n",
      "\n",
      "🤖 Step 2: Analisis AI (OPENAI)...\n",
      "🧠 Metode analisis dipilih: batch\n",
      "📦 Menjalankan batch + parallel processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 15:25:48,944 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:25:49,255 - INFO - ✅ Progress: 1/10 (10.0%)\n",
      "2025-09-30 15:25:49,255 - INFO - ✅ Progress: 1/10 (10.0%)\n",
      "2025-09-30 15:25:51,066 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:25:51,066 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:25:51,375 - INFO - ✅ Progress: 2/10 (20.0%)\n",
      "2025-09-30 15:25:51,375 - INFO - ✅ Progress: 2/10 (20.0%)\n",
      "2025-09-30 15:25:51,604 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:25:51,604 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:25:51,910 - INFO - ✅ Progress: 3/10 (30.0%)\n",
      "2025-09-30 15:25:51,910 - INFO - ✅ Progress: 3/10 (30.0%)\n",
      "2025-09-30 15:25:54,686 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:25:54,686 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:25:54,996 - INFO - ✅ Progress: 4/10 (40.0%)\n",
      "2025-09-30 15:25:54,996 - INFO - ✅ Progress: 4/10 (40.0%)\n",
      "2025-09-30 15:25:58,239 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:25:58,239 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:25:58,551 - INFO - ✅ Progress: 5/10 (50.0%)\n",
      "2025-09-30 15:25:58,554 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:25:58,551 - INFO - ✅ Progress: 5/10 (50.0%)\n",
      "2025-09-30 15:25:58,554 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:25:58,869 - INFO - ✅ Progress: 6/10 (60.0%)\n",
      "2025-09-30 15:25:58,869 - INFO - ✅ Progress: 6/10 (60.0%)\n",
      "2025-09-30 15:26:01,279 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:01,279 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:01,584 - INFO - ✅ Progress: 7/10 (70.0%)\n",
      "2025-09-30 15:26:01,584 - INFO - ✅ Progress: 7/10 (70.0%)\n",
      "2025-09-30 15:26:04,346 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:04,346 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:04,662 - INFO - ✅ Progress: 8/10 (80.0%)\n",
      "2025-09-30 15:26:04,662 - INFO - ✅ Progress: 8/10 (80.0%)\n",
      "2025-09-30 15:26:06,153 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:06,153 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:06,457 - INFO - ✅ Progress: 9/10 (90.0%)\n",
      "2025-09-30 15:26:06,457 - INFO - ✅ Progress: 9/10 (90.0%)\n",
      "2025-09-30 15:26:09,292 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:09,292 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:09,601 - INFO - ✅ Progress: 10/10 (100.0%)\n",
      "2025-09-30 15:26:09,610 - INFO - 🎉 Analisis parallel selesai dalam 26.23 detik\n",
      "2025-09-30 15:26:09,611 - INFO - 📊 Rata-rata: 2.62 detik per berita\n",
      "2025-09-30 15:26:09,601 - INFO - ✅ Progress: 10/10 (100.0%)\n",
      "2025-09-30 15:26:09,610 - INFO - 🎉 Analisis parallel selesai dalam 26.23 detik\n",
      "2025-09-30 15:26:09,611 - INFO - 📊 Rata-rata: 2.62 detik per berita\n",
      "2025-09-30 15:26:09,623 - INFO - ⏱️  Waiting between batches...\n",
      "2025-09-30 15:26:09,623 - INFO - ⏱️  Waiting between batches...\n",
      "2025-09-30 15:26:11,629 - INFO - 📦 Processing batch 2/11 (rows 10-19)\n",
      "2025-09-30 15:26:11,630 - INFO - 🚀 Memulai analisis parallel dengan 3 workers...\n",
      "2025-09-30 15:26:11,629 - INFO - 📦 Processing batch 2/11 (rows 10-19)\n",
      "2025-09-30 15:26:11,630 - INFO - 🚀 Memulai analisis parallel dengan 3 workers...\n",
      "2025-09-30 15:26:17,725 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:17,725 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:18,029 - INFO - ✅ Progress: 1/10 (10.0%)\n",
      "2025-09-30 15:26:18,029 - INFO - ✅ Progress: 1/10 (10.0%)\n",
      "2025-09-30 15:26:18,249 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:18,249 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:18,394 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:18,394 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:18,557 - INFO - ✅ Progress: 2/10 (20.0%)\n",
      "2025-09-30 15:26:18,557 - INFO - ✅ Progress: 2/10 (20.0%)\n",
      "2025-09-30 15:26:18,704 - INFO - ✅ Progress: 3/10 (30.0%)\n",
      "2025-09-30 15:26:18,704 - INFO - ✅ Progress: 3/10 (30.0%)\n",
      "2025-09-30 15:26:23,839 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:23,839 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:24,145 - INFO - ✅ Progress: 4/10 (40.0%)\n",
      "2025-09-30 15:26:24,145 - INFO - ✅ Progress: 4/10 (40.0%)\n",
      "2025-09-30 15:26:24,387 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:24,387 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:24,696 - INFO - ✅ Progress: 5/10 (50.0%)\n",
      "2025-09-30 15:26:24,696 - INFO - ✅ Progress: 5/10 (50.0%)\n",
      "2025-09-30 15:26:24,809 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:24,809 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:25,120 - INFO - ✅ Progress: 6/10 (60.0%)\n",
      "2025-09-30 15:26:25,120 - INFO - ✅ Progress: 6/10 (60.0%)\n",
      "2025-09-30 15:26:30,349 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:30,349 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:30,659 - INFO - ✅ Progress: 7/10 (70.0%)\n",
      "2025-09-30 15:26:30,659 - INFO - ✅ Progress: 7/10 (70.0%)\n",
      "2025-09-30 15:26:30,685 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:30,685 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:30,765 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:30,765 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:31,018 - INFO - ✅ Progress: 8/10 (80.0%)\n",
      "2025-09-30 15:26:31,018 - INFO - ✅ Progress: 8/10 (80.0%)\n",
      "2025-09-30 15:26:31,078 - INFO - ✅ Progress: 9/10 (90.0%)\n",
      "2025-09-30 15:26:31,078 - INFO - ✅ Progress: 9/10 (90.0%)\n",
      "2025-09-30 15:26:37,130 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:37,130 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:37,440 - INFO - ✅ Progress: 10/10 (100.0%)\n",
      "2025-09-30 15:26:37,447 - INFO - 🎉 Analisis parallel selesai dalam 25.82 detik\n",
      "2025-09-30 15:26:37,440 - INFO - ✅ Progress: 10/10 (100.0%)\n",
      "2025-09-30 15:26:37,447 - INFO - 🎉 Analisis parallel selesai dalam 25.82 detik\n",
      "2025-09-30 15:26:37,447 - INFO - 📊 Rata-rata: 2.58 detik per berita\n",
      "2025-09-30 15:26:37,451 - INFO - ⏱️  Waiting between batches...\n",
      "2025-09-30 15:26:37,447 - INFO - 📊 Rata-rata: 2.58 detik per berita\n",
      "2025-09-30 15:26:37,451 - INFO - ⏱️  Waiting between batches...\n",
      "2025-09-30 15:26:39,457 - INFO - 📦 Processing batch 3/11 (rows 20-29)\n",
      "2025-09-30 15:26:39,459 - INFO - 🚀 Memulai analisis parallel dengan 3 workers...\n",
      "2025-09-30 15:26:39,457 - INFO - 📦 Processing batch 3/11 (rows 20-29)\n",
      "2025-09-30 15:26:39,459 - INFO - 🚀 Memulai analisis parallel dengan 3 workers...\n",
      "2025-09-30 15:26:45,475 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:45,477 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:45,475 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:45,477 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:45,788 - INFO - ✅ Progress: 1/10 (10.0%)\n",
      "2025-09-30 15:26:45,790 - INFO - ✅ Progress: 2/10 (20.0%)\n",
      "2025-09-30 15:26:45,788 - INFO - ✅ Progress: 1/10 (10.0%)\n",
      "2025-09-30 15:26:45,790 - INFO - ✅ Progress: 2/10 (20.0%)\n",
      "2025-09-30 15:26:49,137 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:49,137 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:49,446 - INFO - ✅ Progress: 3/10 (30.0%)\n",
      "2025-09-30 15:26:49,446 - INFO - ✅ Progress: 3/10 (30.0%)\n",
      "2025-09-30 15:26:52,233 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:52,233 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:52,540 - INFO - ✅ Progress: 4/10 (40.0%)\n",
      "2025-09-30 15:26:52,540 - INFO - ✅ Progress: 4/10 (40.0%)\n",
      "2025-09-30 15:26:55,873 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:55,873 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:56,180 - INFO - ✅ Progress: 5/10 (50.0%)\n",
      "2025-09-30 15:26:56,180 - INFO - ✅ Progress: 5/10 (50.0%)\n",
      "2025-09-30 15:26:56,577 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:56,577 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:56,892 - INFO - ✅ Progress: 6/10 (60.0%)\n",
      "2025-09-30 15:26:56,892 - INFO - ✅ Progress: 6/10 (60.0%)\n",
      "2025-09-30 15:26:58,682 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:58,682 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:26:58,988 - INFO - ✅ Progress: 7/10 (70.0%)\n",
      "2025-09-30 15:26:58,988 - INFO - ✅ Progress: 7/10 (70.0%)\n",
      "2025-09-30 15:27:01,450 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:01,450 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:01,754 - INFO - ✅ Progress: 8/10 (80.0%)\n",
      "2025-09-30 15:27:01,754 - INFO - ✅ Progress: 8/10 (80.0%)\n",
      "2025-09-30 15:27:03,906 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:03,906 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:04,215 - INFO - ✅ Progress: 9/10 (90.0%)\n",
      "2025-09-30 15:27:04,215 - INFO - ✅ Progress: 9/10 (90.0%)\n",
      "2025-09-30 15:27:05,653 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:05,653 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:05,962 - INFO - ✅ Progress: 10/10 (100.0%)\n",
      "2025-09-30 15:27:05,971 - INFO - 🎉 Analisis parallel selesai dalam 26.51 detik\n",
      "2025-09-30 15:27:05,962 - INFO - ✅ Progress: 10/10 (100.0%)\n",
      "2025-09-30 15:27:05,971 - INFO - 🎉 Analisis parallel selesai dalam 26.51 detik\n",
      "2025-09-30 15:27:05,972 - INFO - 📊 Rata-rata: 2.65 detik per berita\n",
      "2025-09-30 15:27:05,977 - INFO - ⏱️  Waiting between batches...\n",
      "2025-09-30 15:27:05,972 - INFO - 📊 Rata-rata: 2.65 detik per berita\n",
      "2025-09-30 15:27:05,977 - INFO - ⏱️  Waiting between batches...\n",
      "2025-09-30 15:27:07,982 - INFO - 📦 Processing batch 4/11 (rows 30-39)\n",
      "2025-09-30 15:27:07,985 - INFO - 🚀 Memulai analisis parallel dengan 3 workers...\n",
      "2025-09-30 15:27:07,982 - INFO - 📦 Processing batch 4/11 (rows 30-39)\n",
      "2025-09-30 15:27:07,985 - INFO - 🚀 Memulai analisis parallel dengan 3 workers...\n",
      "2025-09-30 15:27:15,375 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:15,375 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:15,435 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:15,435 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:15,681 - INFO - ✅ Progress: 1/10 (10.0%)\n",
      "2025-09-30 15:27:15,681 - INFO - ✅ Progress: 1/10 (10.0%)\n",
      "2025-09-30 15:27:15,730 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:15,730 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:15,751 - INFO - ✅ Progress: 2/10 (20.0%)\n",
      "2025-09-30 15:27:15,751 - INFO - ✅ Progress: 2/10 (20.0%)\n",
      "2025-09-30 15:27:16,037 - INFO - ✅ Progress: 3/10 (30.0%)\n",
      "2025-09-30 15:27:16,037 - INFO - ✅ Progress: 3/10 (30.0%)\n",
      "2025-09-30 15:27:21,622 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:21,622 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:21,778 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:21,778 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:21,928 - INFO - ✅ Progress: 4/10 (40.0%)\n",
      "2025-09-30 15:27:21,928 - INFO - ✅ Progress: 4/10 (40.0%)\n",
      "2025-09-30 15:27:22,086 - INFO - ✅ Progress: 5/10 (50.0%)\n",
      "2025-09-30 15:27:22,086 - INFO - ✅ Progress: 5/10 (50.0%)\n",
      "2025-09-30 15:27:23,994 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:23,994 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:24,304 - INFO - ✅ Progress: 6/10 (60.0%)\n",
      "2025-09-30 15:27:24,304 - INFO - ✅ Progress: 6/10 (60.0%)\n",
      "2025-09-30 15:27:28,576 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:28,576 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:28,887 - INFO - ✅ Progress: 7/10 (70.0%)\n",
      "2025-09-30 15:27:28,887 - INFO - ✅ Progress: 7/10 (70.0%)\n",
      "2025-09-30 15:27:29,452 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:29,452 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:29,781 - INFO - ✅ Progress: 8/10 (80.0%)\n",
      "2025-09-30 15:27:29,781 - INFO - ✅ Progress: 8/10 (80.0%)\n",
      "2025-09-30 15:27:33,232 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:33,232 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:33,546 - INFO - ✅ Progress: 9/10 (90.0%)\n",
      "2025-09-30 15:27:33,546 - INFO - ✅ Progress: 9/10 (90.0%)\n",
      "2025-09-30 15:27:35,266 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:35,266 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:35,581 - INFO - ✅ Progress: 10/10 (100.0%)\n",
      "2025-09-30 15:27:35,581 - INFO - ✅ Progress: 10/10 (100.0%)\n",
      "2025-09-30 15:27:35,593 - INFO - 🎉 Analisis parallel selesai dalam 27.61 detik\n",
      "2025-09-30 15:27:35,596 - INFO - 📊 Rata-rata: 2.76 detik per berita\n",
      "2025-09-30 15:27:35,602 - INFO - ⏱️  Waiting between batches...\n",
      "2025-09-30 15:27:35,593 - INFO - 🎉 Analisis parallel selesai dalam 27.61 detik\n",
      "2025-09-30 15:27:35,596 - INFO - 📊 Rata-rata: 2.76 detik per berita\n",
      "2025-09-30 15:27:35,602 - INFO - ⏱️  Waiting between batches...\n",
      "2025-09-30 15:27:37,608 - INFO - 📦 Processing batch 5/11 (rows 40-49)\n",
      "2025-09-30 15:27:37,610 - INFO - 🚀 Memulai analisis parallel dengan 3 workers...\n",
      "2025-09-30 15:27:37,608 - INFO - 📦 Processing batch 5/11 (rows 40-49)\n",
      "2025-09-30 15:27:37,610 - INFO - 🚀 Memulai analisis parallel dengan 3 workers...\n",
      "2025-09-30 15:27:43,224 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:43,224 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:43,535 - INFO - ✅ Progress: 1/10 (10.0%)\n",
      "2025-09-30 15:27:43,535 - INFO - ✅ Progress: 1/10 (10.0%)\n",
      "2025-09-30 15:27:45,145 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:45,145 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:45,418 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:45,418 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:45,452 - INFO - ✅ Progress: 2/10 (20.0%)\n",
      "2025-09-30 15:27:45,452 - INFO - ✅ Progress: 2/10 (20.0%)\n",
      "2025-09-30 15:27:45,723 - INFO - ✅ Progress: 3/10 (30.0%)\n",
      "2025-09-30 15:27:45,723 - INFO - ✅ Progress: 3/10 (30.0%)\n",
      "2025-09-30 15:27:52,034 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:52,034 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:52,344 - INFO - ✅ Progress: 4/10 (40.0%)\n",
      "2025-09-30 15:27:52,344 - INFO - ✅ Progress: 4/10 (40.0%)\n",
      "2025-09-30 15:27:52,852 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:52,852 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:53,157 - INFO - ✅ Progress: 5/10 (50.0%)\n",
      "2025-09-30 15:27:53,157 - INFO - ✅ Progress: 5/10 (50.0%)\n",
      "2025-09-30 15:27:54,061 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:54,061 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:54,371 - INFO - ✅ Progress: 6/10 (60.0%)\n",
      "2025-09-30 15:27:54,371 - INFO - ✅ Progress: 6/10 (60.0%)\n",
      "2025-09-30 15:27:59,832 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:27:59,832 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:00,139 - INFO - ✅ Progress: 7/10 (70.0%)\n",
      "2025-09-30 15:28:00,139 - INFO - ✅ Progress: 7/10 (70.0%)\n",
      "2025-09-30 15:28:00,623 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:00,623 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:00,856 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:00,856 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:00,927 - INFO - ✅ Progress: 8/10 (80.0%)\n",
      "2025-09-30 15:28:00,927 - INFO - ✅ Progress: 8/10 (80.0%)\n",
      "2025-09-30 15:28:01,163 - INFO - ✅ Progress: 9/10 (90.0%)\n",
      "2025-09-30 15:28:01,163 - INFO - ✅ Progress: 9/10 (90.0%)\n",
      "2025-09-30 15:28:09,509 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:09,509 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:09,815 - INFO - ✅ Progress: 10/10 (100.0%)\n",
      "2025-09-30 15:28:09,825 - INFO - 🎉 Analisis parallel selesai dalam 32.21 detik\n",
      "2025-09-30 15:28:09,826 - INFO - 📊 Rata-rata: 3.22 detik per berita\n",
      "2025-09-30 15:28:09,833 - INFO - ⏱️  Waiting between batches...\n",
      "2025-09-30 15:28:09,815 - INFO - ✅ Progress: 10/10 (100.0%)\n",
      "2025-09-30 15:28:09,825 - INFO - 🎉 Analisis parallel selesai dalam 32.21 detik\n",
      "2025-09-30 15:28:09,826 - INFO - 📊 Rata-rata: 3.22 detik per berita\n",
      "2025-09-30 15:28:09,833 - INFO - ⏱️  Waiting between batches...\n",
      "2025-09-30 15:28:11,837 - INFO - 📦 Processing batch 6/11 (rows 50-59)\n",
      "2025-09-30 15:28:11,839 - INFO - 🚀 Memulai analisis parallel dengan 3 workers...\n",
      "2025-09-30 15:28:11,837 - INFO - 📦 Processing batch 6/11 (rows 50-59)\n",
      "2025-09-30 15:28:11,839 - INFO - 🚀 Memulai analisis parallel dengan 3 workers...\n",
      "2025-09-30 15:28:18,250 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:18,250 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:18,556 - INFO - ✅ Progress: 1/10 (10.0%)\n",
      "2025-09-30 15:28:18,556 - INFO - ✅ Progress: 1/10 (10.0%)\n",
      "2025-09-30 15:28:21,117 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:21,117 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:21,427 - INFO - ✅ Progress: 2/10 (20.0%)\n",
      "2025-09-30 15:28:21,427 - INFO - ✅ Progress: 2/10 (20.0%)\n",
      "2025-09-30 15:28:21,982 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:21,982 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:22,289 - INFO - ✅ Progress: 3/10 (30.0%)\n",
      "2025-09-30 15:28:22,289 - INFO - ✅ Progress: 3/10 (30.0%)\n",
      "2025-09-30 15:28:25,589 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:25,589 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:25,902 - INFO - ✅ Progress: 4/10 (40.0%)\n",
      "2025-09-30 15:28:25,902 - INFO - ✅ Progress: 4/10 (40.0%)\n",
      "2025-09-30 15:28:27,512 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:27,512 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:27,822 - INFO - ✅ Progress: 5/10 (50.0%)\n",
      "2025-09-30 15:28:27,822 - INFO - ✅ Progress: 5/10 (50.0%)\n",
      "2025-09-30 15:28:30,845 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:30,845 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:31,155 - INFO - ✅ Progress: 6/10 (60.0%)\n",
      "2025-09-30 15:28:31,155 - INFO - ✅ Progress: 6/10 (60.0%)\n",
      "2025-09-30 15:28:31,748 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:31,748 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:32,065 - INFO - ✅ Progress: 7/10 (70.0%)\n",
      "2025-09-30 15:28:32,065 - INFO - ✅ Progress: 7/10 (70.0%)\n",
      "2025-09-30 15:28:34,738 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:34,738 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:35,045 - INFO - ✅ Progress: 8/10 (80.0%)\n",
      "2025-09-30 15:28:35,045 - INFO - ✅ Progress: 8/10 (80.0%)\n",
      "2025-09-30 15:28:37,707 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:37,707 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:38,016 - INFO - ✅ Progress: 9/10 (90.0%)\n",
      "2025-09-30 15:28:38,016 - INFO - ✅ Progress: 9/10 (90.0%)\n",
      "2025-09-30 15:28:40,473 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:40,473 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:40,777 - INFO - ✅ Progress: 10/10 (100.0%)\n",
      "2025-09-30 15:28:40,787 - INFO - 🎉 Analisis parallel selesai dalam 28.95 detik\n",
      "2025-09-30 15:28:40,777 - INFO - ✅ Progress: 10/10 (100.0%)\n",
      "2025-09-30 15:28:40,787 - INFO - 🎉 Analisis parallel selesai dalam 28.95 detik\n",
      "2025-09-30 15:28:40,788 - INFO - 📊 Rata-rata: 2.89 detik per berita\n",
      "2025-09-30 15:28:40,795 - INFO - ⏱️  Waiting between batches...\n",
      "2025-09-30 15:28:40,788 - INFO - 📊 Rata-rata: 2.89 detik per berita\n",
      "2025-09-30 15:28:40,795 - INFO - ⏱️  Waiting between batches...\n",
      "2025-09-30 15:28:42,800 - INFO - 📦 Processing batch 7/11 (rows 60-69)\n",
      "2025-09-30 15:28:42,803 - INFO - 🚀 Memulai analisis parallel dengan 3 workers...\n",
      "2025-09-30 15:28:42,800 - INFO - 📦 Processing batch 7/11 (rows 60-69)\n",
      "2025-09-30 15:28:42,803 - INFO - 🚀 Memulai analisis parallel dengan 3 workers...\n",
      "2025-09-30 15:28:48,381 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:48,381 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:48,689 - INFO - ✅ Progress: 1/10 (10.0%)\n",
      "2025-09-30 15:28:48,689 - INFO - ✅ Progress: 1/10 (10.0%)\n",
      "2025-09-30 15:28:50,505 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:50,515 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:50,505 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:50,515 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:50,815 - INFO - ✅ Progress: 2/10 (20.0%)\n",
      "2025-09-30 15:28:50,825 - INFO - ✅ Progress: 3/10 (30.0%)\n",
      "2025-09-30 15:28:50,815 - INFO - ✅ Progress: 2/10 (20.0%)\n",
      "2025-09-30 15:28:50,825 - INFO - ✅ Progress: 3/10 (30.0%)\n",
      "2025-09-30 15:28:55,952 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:55,952 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:56,263 - INFO - ✅ Progress: 4/10 (40.0%)\n",
      "2025-09-30 15:28:56,263 - INFO - ✅ Progress: 4/10 (40.0%)\n",
      "2025-09-30 15:28:56,526 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:56,526 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:56,842 - INFO - ✅ Progress: 5/10 (50.0%)\n",
      "2025-09-30 15:28:56,842 - INFO - ✅ Progress: 5/10 (50.0%)\n",
      "2025-09-30 15:28:58,460 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:58,460 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:28:58,772 - INFO - ✅ Progress: 6/10 (60.0%)\n",
      "2025-09-30 15:28:58,772 - INFO - ✅ Progress: 6/10 (60.0%)\n",
      "2025-09-30 15:29:03,953 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:03,953 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:04,265 - INFO - ✅ Progress: 7/10 (70.0%)\n",
      "2025-09-30 15:29:04,265 - INFO - ✅ Progress: 7/10 (70.0%)\n",
      "2025-09-30 15:29:05,135 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:05,135 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:05,445 - INFO - ✅ Progress: 8/10 (80.0%)\n",
      "2025-09-30 15:29:05,456 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:05,445 - INFO - ✅ Progress: 8/10 (80.0%)\n",
      "2025-09-30 15:29:05,456 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:05,765 - INFO - ✅ Progress: 9/10 (90.0%)\n",
      "2025-09-30 15:29:05,765 - INFO - ✅ Progress: 9/10 (90.0%)\n",
      "2025-09-30 15:29:10,276 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:10,276 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:10,587 - INFO - ✅ Progress: 10/10 (100.0%)\n",
      "2025-09-30 15:29:10,591 - INFO - 🎉 Analisis parallel selesai dalam 27.79 detik\n",
      "2025-09-30 15:29:10,587 - INFO - ✅ Progress: 10/10 (100.0%)\n",
      "2025-09-30 15:29:10,591 - INFO - 🎉 Analisis parallel selesai dalam 27.79 detik\n",
      "2025-09-30 15:29:10,592 - INFO - 📊 Rata-rata: 2.78 detik per berita\n",
      "2025-09-30 15:29:10,596 - INFO - ⏱️  Waiting between batches...\n",
      "2025-09-30 15:29:10,592 - INFO - 📊 Rata-rata: 2.78 detik per berita\n",
      "2025-09-30 15:29:10,596 - INFO - ⏱️  Waiting between batches...\n",
      "2025-09-30 15:29:12,604 - INFO - 📦 Processing batch 8/11 (rows 70-79)\n",
      "2025-09-30 15:29:12,608 - INFO - 🚀 Memulai analisis parallel dengan 3 workers...\n",
      "2025-09-30 15:29:12,604 - INFO - 📦 Processing batch 8/11 (rows 70-79)\n",
      "2025-09-30 15:29:12,608 - INFO - 🚀 Memulai analisis parallel dengan 3 workers...\n",
      "2025-09-30 15:29:17,428 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:17,428 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:17,736 - INFO - ✅ Progress: 1/10 (10.0%)\n",
      "2025-09-30 15:29:17,736 - INFO - ✅ Progress: 1/10 (10.0%)\n",
      "2025-09-30 15:29:18,609 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:18,609 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:18,922 - INFO - ✅ Progress: 2/10 (20.0%)\n",
      "2025-09-30 15:29:18,922 - INFO - ✅ Progress: 2/10 (20.0%)\n",
      "2025-09-30 15:29:20,821 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:20,821 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:21,128 - INFO - ✅ Progress: 3/10 (30.0%)\n",
      "2025-09-30 15:29:21,128 - INFO - ✅ Progress: 3/10 (30.0%)\n",
      "2025-09-30 15:29:23,891 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:23,891 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:24,200 - INFO - ✅ Progress: 4/10 (40.0%)\n",
      "2025-09-30 15:29:24,200 - INFO - ✅ Progress: 4/10 (40.0%)\n",
      "2025-09-30 15:29:25,807 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:25,807 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:26,129 - INFO - ✅ Progress: 5/10 (50.0%)\n",
      "2025-09-30 15:29:26,129 - INFO - ✅ Progress: 5/10 (50.0%)\n",
      "2025-09-30 15:29:26,177 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:26,177 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:26,484 - INFO - ✅ Progress: 6/10 (60.0%)\n",
      "2025-09-30 15:29:26,484 - INFO - ✅ Progress: 6/10 (60.0%)\n",
      "2025-09-30 15:29:29,831 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:29,831 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:30,143 - INFO - ✅ Progress: 7/10 (70.0%)\n",
      "2025-09-30 15:29:30,143 - INFO - ✅ Progress: 7/10 (70.0%)\n",
      "2025-09-30 15:29:32,797 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:32,797 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:33,103 - INFO - ✅ Progress: 8/10 (80.0%)\n",
      "2025-09-30 15:29:33,103 - INFO - ✅ Progress: 8/10 (80.0%)\n",
      "2025-09-30 15:29:33,754 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:33,754 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:34,070 - INFO - ✅ Progress: 9/10 (90.0%)\n",
      "2025-09-30 15:29:34,070 - INFO - ✅ Progress: 9/10 (90.0%)\n",
      "2025-09-30 15:29:35,671 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:35,671 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:35,983 - INFO - ✅ Progress: 10/10 (100.0%)\n",
      "2025-09-30 15:29:35,992 - INFO - 🎉 Analisis parallel selesai dalam 23.38 detik\n",
      "2025-09-30 15:29:35,993 - INFO - 📊 Rata-rata: 2.34 detik per berita\n",
      "2025-09-30 15:29:35,983 - INFO - ✅ Progress: 10/10 (100.0%)\n",
      "2025-09-30 15:29:35,992 - INFO - 🎉 Analisis parallel selesai dalam 23.38 detik\n",
      "2025-09-30 15:29:35,993 - INFO - 📊 Rata-rata: 2.34 detik per berita\n",
      "2025-09-30 15:29:36,003 - INFO - ⏱️  Waiting between batches...\n",
      "2025-09-30 15:29:36,003 - INFO - ⏱️  Waiting between batches...\n",
      "2025-09-30 15:29:38,009 - INFO - 📦 Processing batch 9/11 (rows 80-89)\n",
      "2025-09-30 15:29:38,010 - INFO - 🚀 Memulai analisis parallel dengan 3 workers...\n",
      "2025-09-30 15:29:38,009 - INFO - 📦 Processing batch 9/11 (rows 80-89)\n",
      "2025-09-30 15:29:38,010 - INFO - 🚀 Memulai analisis parallel dengan 3 workers...\n",
      "2025-09-30 15:29:44,317 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:44,317 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:44,573 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:44,573 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:44,623 - INFO - ✅ Progress: 1/10 (10.0%)\n",
      "2025-09-30 15:29:44,623 - INFO - ✅ Progress: 1/10 (10.0%)\n",
      "2025-09-30 15:29:44,888 - INFO - ✅ Progress: 2/10 (20.0%)\n",
      "2025-09-30 15:29:44,888 - INFO - ✅ Progress: 2/10 (20.0%)\n",
      "2025-09-30 15:29:45,705 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:45,705 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:46,013 - INFO - ✅ Progress: 3/10 (30.0%)\n",
      "2025-09-30 15:29:46,013 - INFO - ✅ Progress: 3/10 (30.0%)\n",
      "2025-09-30 15:29:51,370 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:51,370 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:51,405 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:51,405 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:51,712 - INFO - ✅ Progress: 4/10 (40.0%)\n",
      "2025-09-30 15:29:51,714 - INFO - ✅ Progress: 5/10 (50.0%)\n",
      "2025-09-30 15:29:51,712 - INFO - ✅ Progress: 4/10 (40.0%)\n",
      "2025-09-30 15:29:51,714 - INFO - ✅ Progress: 5/10 (50.0%)\n",
      "2025-09-30 15:29:56,026 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:56,026 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:56,337 - INFO - ✅ Progress: 6/10 (60.0%)\n",
      "2025-09-30 15:29:56,337 - INFO - ✅ Progress: 6/10 (60.0%)\n",
      "2025-09-30 15:29:57,910 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:57,910 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:58,228 - INFO - ✅ Progress: 7/10 (70.0%)\n",
      "2025-09-30 15:29:58,228 - INFO - ✅ Progress: 7/10 (70.0%)\n",
      "2025-09-30 15:29:58,810 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:58,810 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:59,118 - INFO - ✅ Progress: 8/10 (80.0%)\n",
      "2025-09-30 15:29:59,118 - INFO - ✅ Progress: 8/10 (80.0%)\n",
      "2025-09-30 15:29:59,314 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:59,314 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:29:59,623 - INFO - ✅ Progress: 9/10 (90.0%)\n",
      "2025-09-30 15:29:59,623 - INFO - ✅ Progress: 9/10 (90.0%)\n",
      "2025-09-30 15:30:04,304 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:30:04,304 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:30:04,615 - INFO - ✅ Progress: 10/10 (100.0%)\n",
      "2025-09-30 15:30:04,626 - INFO - 🎉 Analisis parallel selesai dalam 26.61 detik\n",
      "2025-09-30 15:30:04,615 - INFO - ✅ Progress: 10/10 (100.0%)\n",
      "2025-09-30 15:30:04,626 - INFO - 🎉 Analisis parallel selesai dalam 26.61 detik\n",
      "2025-09-30 15:30:04,627 - INFO - 📊 Rata-rata: 2.66 detik per berita\n",
      "2025-09-30 15:30:04,637 - INFO - ⏱️  Waiting between batches...\n",
      "2025-09-30 15:30:04,627 - INFO - 📊 Rata-rata: 2.66 detik per berita\n",
      "2025-09-30 15:30:04,637 - INFO - ⏱️  Waiting between batches...\n",
      "2025-09-30 15:30:06,642 - INFO - 📦 Processing batch 10/11 (rows 90-99)\n",
      "2025-09-30 15:30:06,644 - INFO - 🚀 Memulai analisis parallel dengan 3 workers...\n",
      "2025-09-30 15:30:06,642 - INFO - 📦 Processing batch 10/11 (rows 90-99)\n",
      "2025-09-30 15:30:06,644 - INFO - 🚀 Memulai analisis parallel dengan 3 workers...\n",
      "2025-09-30 15:30:13,144 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:30:13,146 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:30:13,144 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:30:13,146 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:30:13,239 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:30:13,239 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:30:13,451 - INFO - ✅ Progress: 1/10 (10.0%)\n",
      "2025-09-30 15:30:13,457 - INFO - ✅ Progress: 2/10 (20.0%)\n",
      "2025-09-30 15:30:13,451 - INFO - ✅ Progress: 1/10 (10.0%)\n",
      "2025-09-30 15:30:13,457 - INFO - ✅ Progress: 2/10 (20.0%)\n",
      "2025-09-30 15:30:13,554 - INFO - ✅ Progress: 3/10 (30.0%)\n",
      "2025-09-30 15:30:13,554 - INFO - ✅ Progress: 3/10 (30.0%)\n",
      "2025-09-30 15:30:20,720 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:30:20,720 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:30:21,025 - INFO - ✅ Progress: 4/10 (40.0%)\n",
      "2025-09-30 15:30:21,025 - INFO - ✅ Progress: 4/10 (40.0%)\n",
      "2025-09-30 15:30:21,333 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:30:21,333 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:30:21,642 - INFO - ✅ Progress: 5/10 (50.0%)\n",
      "2025-09-30 15:30:21,642 - INFO - ✅ Progress: 5/10 (50.0%)\n",
      "2025-09-30 15:30:22,155 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:30:22,155 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:30:22,463 - INFO - ✅ Progress: 6/10 (60.0%)\n",
      "2025-09-30 15:30:22,463 - INFO - ✅ Progress: 6/10 (60.0%)\n",
      "2025-09-30 15:30:27,582 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:30:27,582 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:30:27,889 - INFO - ✅ Progress: 7/10 (70.0%)\n",
      "2025-09-30 15:30:27,889 - INFO - ✅ Progress: 7/10 (70.0%)\n",
      "2025-09-30 15:30:28,808 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:30:28,808 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:30:29,115 - INFO - ✅ Progress: 8/10 (80.0%)\n",
      "2025-09-30 15:30:29,115 - INFO - ✅ Progress: 8/10 (80.0%)\n",
      "2025-09-30 15:30:29,732 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:30:29,732 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:30:30,036 - INFO - ✅ Progress: 9/10 (90.0%)\n",
      "2025-09-30 15:30:30,036 - INFO - ✅ Progress: 9/10 (90.0%)\n",
      "2025-09-30 15:30:34,230 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:30:34,230 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:30:34,542 - INFO - ✅ Progress: 10/10 (100.0%)\n",
      "2025-09-30 15:30:34,552 - INFO - 🎉 Analisis parallel selesai dalam 27.91 detik\n",
      "2025-09-30 15:30:34,542 - INFO - ✅ Progress: 10/10 (100.0%)\n",
      "2025-09-30 15:30:34,552 - INFO - 🎉 Analisis parallel selesai dalam 27.91 detik\n",
      "2025-09-30 15:30:34,553 - INFO - 📊 Rata-rata: 2.79 detik per berita\n",
      "2025-09-30 15:30:34,563 - INFO - ⏱️  Waiting between batches...\n",
      "2025-09-30 15:30:34,553 - INFO - 📊 Rata-rata: 2.79 detik per berita\n",
      "2025-09-30 15:30:34,563 - INFO - ⏱️  Waiting between batches...\n",
      "2025-09-30 15:30:36,569 - INFO - 📦 Processing batch 11/11 (rows 100-106)\n",
      "2025-09-30 15:30:36,571 - INFO - 🚀 Memulai analisis parallel dengan 3 workers...\n",
      "2025-09-30 15:30:36,569 - INFO - 📦 Processing batch 11/11 (rows 100-106)\n",
      "2025-09-30 15:30:36,571 - INFO - 🚀 Memulai analisis parallel dengan 3 workers...\n",
      "2025-09-30 15:30:42,329 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:30:42,329 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:30:42,638 - INFO - ✅ Progress: 1/7 (14.3%)\n",
      "2025-09-30 15:30:42,638 - INFO - ✅ Progress: 1/7 (14.3%)\n",
      "2025-09-30 15:30:43,043 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:30:43,043 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:30:43,353 - INFO - ✅ Progress: 2/7 (28.6%)\n",
      "2025-09-30 15:30:43,353 - INFO - ✅ Progress: 2/7 (28.6%)\n",
      "2025-09-30 15:30:46,116 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:30:46,116 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:30:46,423 - INFO - ✅ Progress: 3/7 (42.9%)\n",
      "2025-09-30 15:30:46,423 - INFO - ✅ Progress: 3/7 (42.9%)\n",
      "2025-09-30 15:30:50,583 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:30:50,583 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:30:50,896 - INFO - ✅ Progress: 4/7 (57.1%)\n",
      "2025-09-30 15:30:50,896 - INFO - ✅ Progress: 4/7 (57.1%)\n",
      "2025-09-30 15:30:51,821 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:30:51,821 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:30:52,131 - INFO - ✅ Progress: 5/7 (71.4%)\n",
      "2025-09-30 15:30:52,131 - INFO - ✅ Progress: 5/7 (71.4%)\n",
      "2025-09-30 15:30:52,464 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:30:52,464 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:30:52,773 - INFO - ✅ Progress: 6/7 (85.7%)\n",
      "2025-09-30 15:30:52,773 - INFO - ✅ Progress: 6/7 (85.7%)\n",
      "2025-09-30 15:30:58,713 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:30:58,713 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 15:30:59,020 - INFO - ✅ Progress: 7/7 (100.0%)\n",
      "2025-09-30 15:30:59,028 - INFO - 🎉 Analisis parallel selesai dalam 22.46 detik\n",
      "2025-09-30 15:30:59,020 - INFO - ✅ Progress: 7/7 (100.0%)\n",
      "2025-09-30 15:30:59,028 - INFO - 🎉 Analisis parallel selesai dalam 22.46 detik\n",
      "2025-09-30 15:30:59,030 - INFO - 📊 Rata-rata: 3.21 detik per berita\n",
      "2025-09-30 15:30:59,046 - INFO - Laporan daftar berita disimpan: 00_laporan_cetak/daftar_berita_20250930_153059.txt\n",
      "2025-09-30 15:30:59,030 - INFO - 📊 Rata-rata: 3.21 detik per berita\n",
      "2025-09-30 15:30:59,046 - INFO - Laporan daftar berita disimpan: 00_laporan_cetak/daftar_berita_20250930_153059.txt\n",
      "2025-09-30 15:30:59,052 - INFO - News update disimpan: 00_laporan_cetak/news_update_general_20250930_153059.txt\n",
      "2025-09-30 15:30:59,063 - INFO - Laporan analisis lengkap disimpan: 00_laporan_cetak/laporan_analisis_media_20250930_153059.txt\n",
      "2025-09-30 15:30:59,052 - INFO - News update disimpan: 00_laporan_cetak/news_update_general_20250930_153059.txt\n",
      "2025-09-30 15:30:59,063 - INFO - Laporan analisis lengkap disimpan: 00_laporan_cetak/laporan_analisis_media_20250930_153059.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Analisis AI selesai dengan metode: batch\n",
      "\n",
      "📋 Step 3: Generate Daftar Berita...\n",
      "✅ Daftar berita disimpan: 00_laporan_cetak/daftar_berita_20250930_153059.txt\n",
      "\n",
      "📰 Step 4: Generate News Update...\n",
      "✅ News update disimpan: 00_laporan_cetak/news_update_general_20250930_153059.txt\n",
      "\n",
      "📊 Step 5: Generate Laporan Analisis Lengkap...\n",
      "✅ Laporan lengkap disimpan: 00_laporan_cetak/laporan_analisis_media_20250930_153059.txt\n",
      "\n",
      "🎉 Pipeline selesai! Summary:\n",
      "   - Total berita dianalisis: 107\n",
      "   - Sentimen positif: 78\n",
      "   - Sentimen negatif: 5\n",
      "   - Sentimen netral: 24\n",
      "   - Kolom AI: ['ai_resume', 'ai_dampak_kemenkeu', 'ai_alasan_dampak', 'ai_hal_menarik'] (resume terisi: 107)\n",
      "\n",
      "📁 File output tersimpan di: ./00_laporan_cetak/\n",
      "\n",
      "🎯 FULL PIPELINE SELESAI\n",
      "📊 Total berita dianalisis: 107 | Kolom AI: 4\n",
      "   Contoh kolom AI: ['ai_resume', 'ai_dampak_kemenkeu', 'ai_alasan_dampak', 'ai_hal_menarik']\n",
      "   Isi kolom pertama terisi: 107 baris\n",
      "💾 DataFrame enriched disimpan: 00_hasil_analisis/berita_penting/analisis_berita_penting_enriched_openai_20250930_153059.csv\n",
      "\n",
      "📁 File teks yang dihasilkan:\n",
      "   - daftar_berita_file: 00_laporan_cetak/daftar_berita_20250930_153059.txt\n",
      "   - news_update_file: 00_laporan_cetak/news_update_general_20250930_153059.txt\n",
      "   - laporan_lengkap_file: 00_laporan_cetak/laporan_analisis_media_20250930_153059.txt\n",
      "\n",
      "⏲️  Total waktu eksekusi: 315.71 detik\n",
      "✅ Selesai.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EKSEKUSI OTOMATIS SELURUH PIPELINE (FULL RUN)\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "Cara pakai:\n",
    "1. Pastikan semua cell sebelumnya (fungsi & konfigurasi) sudah dijalankan.\n",
    "2. Set environment variable API key (export OPENAI_API_KEY=... atau DEEPSEEK_API_KEY=...).\n",
    "3. Jalankan cell ini untuk memproses semua berita penting dan menghasilkan:\n",
    "   - Daftar berita (txt)\n",
    "   - News update (txt)\n",
    "   - Laporan analisis lengkap (txt)\n",
    "   - DataFrame hasil analisis AI (csv tambahan) jika AI dijalankan\n",
    "\n",
    "Parameter yang bisa diatur di bawah:\n",
    "- FORCE_METHOD: paksa metode tertentu (\"auto\",\"sequential\",\"parallel_v2\",\"parallel_safe\",\"parallel\",\"batch\")\n",
    "- LIMIT_ARTICLES: batasi jumlah berita untuk uji cepat (None = semua)\n",
    "- SAVE_AI_ENRICHED_CSV: simpan DataFrame hasil analisis AI yang diperkaya\n",
    "\"\"\"\n",
    "\n",
    "import os, time, json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# ================== PARAMETER EKSEKUSI ==================\n",
    "FORCE_METHOD = \"auto\"          # ganti ke \"parallel_v2\" jika ingin paksa\n",
    "LIMIT_ARTICLES = None           # contoh: 5 untuk uji cepat\n",
    "SAVE_AI_ENRICHED_CSV = True     # simpan hasil enriched df\n",
    "OUTPUT_ENRICH_DIR = Path(\"00_hasil_analisis/berita_penting\")\n",
    "# ========================================================\n",
    "\n",
    "start_global = time.time()\n",
    "print(\"🚀 MENJALANKAN FULL PIPELINE ANALISIS BERITA PENTING\")\n",
    "print(f\"⚙️  Metode: {FORCE_METHOD} | Limit: {LIMIT_ARTICLES if LIMIT_ARTICLES else 'Semua'}\")\n",
    "\n",
    "# Deteksi API key dan provider\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "deepseek_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "\n",
    "selected_provider = None\n",
    "selected_key = None\n",
    "if openai_key and openai_key.strip():\n",
    "    selected_provider = \"openai\"\n",
    "    selected_key = openai_key\n",
    "elif deepseek_key and deepseek_key.strip():\n",
    "    selected_provider = \"deepseek\"\n",
    "    selected_key = deepseek_key\n",
    "else:\n",
    "    print(\"⚠️  Tidak ada API key ditemukan. Analisis AI akan dilewati.\")\n",
    "\n",
    "# Estimasi waktu (jika data sudah ada di memori)\n",
    "try:\n",
    "    if 'df_berita_penting' in globals() and not df_berita_penting.empty:\n",
    "        est = estimate_processing_time(len(df_berita_penting))\n",
    "        rec = est.get('recommended', {})\n",
    "        print(\"⏱️  Estimasi waktu (berdasarkan jumlah berita saat ini):\")\n",
    "        print(f\"   Sequential : {est['sequential']['time_formatted']}\")\n",
    "        print(f\"   Parallel   : {est['parallel']['time_formatted']}\")\n",
    "        print(f\"   Batch      : {est['batch']['time_formatted']}\")\n",
    "        if rec:\n",
    "            print(f\"   Rekomendasi: {rec['method']} ({rec['time_formatted']})\")\n",
    "except Exception as e:\n",
    "    print(f\"(Info) Gagal menghitung estimasi: {e}\")\n",
    "\n",
    "# Jalankan pipeline\n",
    "results_full = run_complete_analysis(\n",
    "    ai_provider=selected_provider if selected_provider else \"openai\",\n",
    "    api_key=selected_key,\n",
    "    processing_method=FORCE_METHOD,\n",
    "    limit_articles=LIMIT_ARTICLES,\n",
    "    save_outputs=True\n",
    ")\n",
    "\n",
    "# Handling error\n",
    "if 'error' in results_full:\n",
    "    print(f\"❌ Pipeline gagal: {results_full['error']}\")\n",
    "else:\n",
    "    print(\"\\n🎯 FULL PIPELINE SELESAI\")\n",
    "    analyzed_df = results_full.get('analyzed_data')\n",
    "    if analyzed_df is not None and not analyzed_df.empty:\n",
    "        ai_cols = [c for c in analyzed_df.columns if c.startswith('ai_')]\n",
    "        print(f\"📊 Total berita dianalisis: {len(analyzed_df)} | Kolom AI: {len(ai_cols)}\")\n",
    "        if ai_cols:\n",
    "            filled = analyzed_df[ai_cols[0]].notna().sum()\n",
    "            print(f\"   Contoh kolom AI: {ai_cols[:4]}\")\n",
    "            print(f\"   Isi kolom pertama terisi: {filled} baris\")\n",
    "        # Simpan enriched CSV\n",
    "        if SAVE_AI_ENRICHED_CSV:\n",
    "            try:\n",
    "                OUTPUT_ENRICH_DIR.mkdir(parents=True, exist_ok=True)\n",
    "                ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "                out_csv = OUTPUT_ENRICH_DIR / f\"analisis_berita_penting_enriched_{selected_provider or 'noai'}_{ts}.csv\"\n",
    "                analyzed_df.to_csv(out_csv, index=False)\n",
    "                print(f\"💾 DataFrame enriched disimpan: {out_csv}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️  Gagal menyimpan enriched CSV: {e}\")\n",
    "    else:\n",
    "        print(\"⚠️  Tidak ada DataFrame hasil analisis yang dapat ditampilkan.\")\n",
    "\n",
    "    # Tampilkan file output yang dihasilkan\n",
    "    output_files = {k:v for k,v in results_full.items() if k.endswith('_file')}\n",
    "    if output_files:\n",
    "        print(\"\\n📁 File teks yang dihasilkan:\")\n",
    "        for k,v in output_files.items():\n",
    "            print(f\"   - {k}: {v}\")\n",
    "\n",
    "elapsed_global = time.time() - start_global\n",
    "print(f\"\\n⏲️  Total waktu eksekusi: {elapsed_global:.2f} detik\")\n",
    "print(\"✅ Selesai.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
