{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP0p6DRsWsNpGoCCUlRd8Dh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D24GqrCzlvQ6","executionInfo":{"status":"ok","timestamp":1758162669821,"user_tz":-420,"elapsed":16525,"user":{"displayName":"Monitoring Berita","userId":"16755502473357078001"}},"outputId":"b541d44d-52f3-4427-bc8a-e3a3eafed8eb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# mount google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# set folder tempat kerja (current working directory)\n","import os\n","cwd = '/content/drive/MyDrive/Monitoring Berita'\n","os.chdir(cwd)"],"metadata":{"id":"8lAhCOQLo_qw","executionInfo":{"status":"ok","timestamp":1758162670109,"user_tz":-420,"elapsed":285,"user":{"displayName":"Monitoring Berita","userId":"16755502473357078001"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import re\n","import time\n","import random\n","import traceback\n","from urllib.parse import quote_plus, urlparse\n","from typing import Optional, List, Dict, Tuple\n","from dataclasses import dataclass\n","\n","import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","\n","\n","# =========================\n","# Konfigurasi umum\n","# =========================\n","HEADERS = {\n","    \"User-Agent\": (\n","        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n","        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n","        \"Chrome/127.0.0.0 Safari/537.36\"\n","    ),\n","    \"Accept-Language\": \"id,en;q=0.9\",\n","}\n","BASE_SEARCH = \"https://search.kompas.com/search?q=\"\n","REQUEST_TIMEOUT = 12\n","MAX_RETRIES = 3\n","BACKOFF_BASE = 1.7\n","\n","# Domain dan pola URL artikel Kompas yang valid\n","VALID_HOST = re.compile(r\"(?:^|\\.)kompas\\.com$\", re.I)\n","# Artikel Kompas umumnya memiliki /read/ pada path\n","ARTICLE_PATH = re.compile(r\"/read/\", re.I)\n","# Kecualikan path non-artikel umum\n","EXCLUDE_PATH = re.compile(\n","    r\"/(search|plus|apps|video|foto|vik|parapuan|play|indeks|inside|kgnow|kabarpalmerah|jobs|pasangiklan|tag)\\b\",\n","    re.I,\n",")\n","\n","# =========================\n","# Utilitas\n","# =========================\n","def _sleep(min_s=0.6, max_s=1.2):\n","    time.sleep(random.uniform(min_s, max_s))\n","\n","def http_get(url: str, session: Optional[requests.Session] = None) -> Optional[requests.Response]:\n","    sess = session or requests.Session()\n","    backoff = 1.0\n","    for attempt in range(1, MAX_RETRIES + 1):\n","        try:\n","            resp = sess.get(url, headers=HEADERS, timeout=REQUEST_TIMEOUT)\n","            resp.raise_for_status()\n","            return resp\n","        except requests.RequestException as e:\n","            if attempt == MAX_RETRIES:\n","                print(f\"[ERROR] GET gagal setelah {attempt}x: {url}\\n  -> {repr(e)}\")\n","                return None\n","            time.sleep(backoff)\n","            backoff *= BACKOFF_BASE\n","    return None\n","\n","def build_search_url(topic: str, page: int = 1) -> str:\n","    q = quote_plus(topic.strip())\n","    return f\"{BASE_SEARCH}{q}\" if page <= 1 else f\"{BASE_SEARCH}{q}&page={page}\"\n","\n","def normalize_text(s: str) -> str:\n","    return re.sub(r\"\\s+\", \" \", (s or \"\").strip().lower())\n","\n","def split_terms(topic: str) -> List[str]:\n","    # dukung kutip ganda sebagai frasa\n","    topic = topic.strip()\n","    phrases = re.findall(r'\"([^\"]+)\"', topic)\n","    no_quotes = re.sub(r'\"[^\"]+\"', \" \", topic).strip()\n","    terms = [t for t in re.split(r\"\\s+\", no_quotes) if t]\n","    # gabungkan frasa sebagai satuan utuh (akan diproses terpisah)\n","    return terms, phrases\n","\n","def term_hits(text: str, terms: List[str]) -> int:\n","    # hit sederhana berdasarkan kemunculan kata utuh (word-like)\n","    score = 0\n","    for t in terms:\n","        if not t:\n","            continue\n","        # gunakan boundary longgar untuk bahasa Indonesia\n","        if re.search(rf\"(?i)\\b{re.escape(t)}\\b\", text):\n","            score += 1\n","    return score\n","\n","def phrase_hits(text: str, phrases: List[str]) -> int:\n","    score = 0\n","    for ph in phrases:\n","        if ph and ph.lower() in text:\n","            score += 1\n","    return score\n","\n","def relevance_score(topic: str, title: str, desc: str, body: str,\n","                    require_all_terms: bool, require_phrase: bool) -> Tuple[int, Dict[str, int]]:\n","    terms, phrases = split_terms(topic)\n","    t = normalize_text(title)\n","    d = normalize_text(desc)\n","    b = normalize_text(body)\n","\n","    # hit per bagian\n","    th = term_hits(t, terms) + (2 * phrase_hits(t, phrases))\n","    dh = term_hits(d, terms) + (2 * phrase_hits(d, phrases))\n","    bh = term_hits(b, terms) + (2 * phrase_hits(b, phrases))\n","\n","    # Bobot: judul 3x, deskripsi 2x, isi 1x\n","    score = 3 * th + 2 * dh + 1 * bh\n","\n","    # Aturan ketat (opsional)\n","    if require_all_terms and terms:\n","        # pastikan semua term muncul di gabungan title+desc+body\n","        joined = f\"{t} {d} {b}\"\n","        for tt in terms:\n","            if not re.search(rf\"(?i)\\b{re.escape(tt)}\\b\", joined):\n","                score = 0  # gugurkan\n","                break\n","    if require_phrase and phrases:\n","        # pastikan sedikitnya satu frasa persis muncul\n","        joined = f\"{t} {d} {b}\"\n","        if not any(ph.lower() in joined for ph in phrases):\n","            score = 0\n","\n","    return score, {\"title_hits\": th, \"desc_hits\": dh, \"body_hits\": bh}\n","\n","# =========================\n","# Parsing halaman\n","# =========================\n","def extract_search_links(soup: BeautifulSoup) -> List[str]:\n","    \"\"\"\n","    Ambil link kandidat dari blok hasil (lebih ketat):\n","    - Coba selector kartu hasil dulu (lebih akurat)\n","    - Jika gagal, fallback ke semua <a> dengan filter domain + /read/ + exclude path\n","    \"\"\"\n","    links = []\n","\n","    # 1) Selector kartu hasil yang umum dipakai Kompas\n","    # (Struktur bisa berubah, jadi kita kombinasikan beberapa kandidat)\n","    blocks = []\n","    blocks.extend(soup.select(\"div.article__list, div.article__grid, div.search__result\"))\n","    blocks.extend(soup.select(\"div.gsc-webResult, div.gsc-result\"))\n","    blocks.extend(soup.select(\"article.search-result, li.search-result\"))\n","\n","    for block in blocks:\n","        a = block.select_one(\"a[href]\")\n","        if not a:\n","            continue\n","        href = a.get(\"href\") or \"\"\n","        p = urlparse(href)\n","        if not href.startswith(\"http\"):\n","            continue\n","        if not VALID_HOST.search(p.hostname or \"\"):\n","            continue\n","        if EXCLUDE_PATH.search(p.path or \"\"):\n","            continue\n","        if not ARTICLE_PATH.search(p.path or \"\"):\n","            continue\n","        links.append(href)\n","\n","    # 2) Fallback: scan semua <a> namun tetap syarat /read/ agar fokus artikel\n","    if not links:\n","        for a in soup.select(\"a[href]\"):\n","            href = (a.get(\"href\") or \"\").strip()\n","            if not href.startswith(\"http\"):\n","                continue\n","            p = urlparse(href)\n","            if not VALID_HOST.search(p.hostname or \"\"):\n","                continue\n","            if EXCLUDE_PATH.search(p.path or \"\"):\n","                continue\n","            if not ARTICLE_PATH.search(p.path or \"\"):\n","                continue\n","            links.append(href)\n","\n","    # Unik + pertahankan urutan\n","    seen, uniq = set(), []\n","    for u in links:\n","        if u not in seen:\n","            seen.add(u)\n","            uniq.append(u)\n","    return uniq\n","\n","def parse_article(html: str) -> Dict[str, str]:\n","    soup = BeautifulSoup(html, \"lxml\")\n","\n","    # Judul\n","    title = None\n","    for sel in [\n","        \"meta[property='og:title']\",\n","        \"meta[name='og:title']\",\n","        \"meta[name='twitter:title']\",\n","    ]:\n","        m = soup.select_one(sel)\n","        if m and m.get(\"content\"):\n","            title = m[\"content\"].strip()\n","            break\n","    if not title:\n","        h1 = soup.find(\"h1\")\n","        if h1 and h1.get_text(strip=True):\n","            title = h1.get_text(strip=True)\n","    if not title:\n","        t = soup.find(\"title\")\n","        if t and t.get_text(strip=True):\n","            title = t.get_text(strip=True)\n","\n","    # Tanggal\n","    tanggal = None\n","    for sel in [\n","        \"meta[property='article:published_time']\",\n","        \"meta[name='article:published_time']\",\n","        \"meta[itemprop='datePublished']\",\n","        \"time[datetime]\",\n","    ]:\n","        m = soup.select_one(sel)\n","        if m:\n","            tanggal = m.get(\"content\") or m.get(\"datetime\")\n","            if tanggal:\n","                tanggal = tanggal.strip()\n","                break\n","    if not tanggal:\n","        t = soup.find(\"time\")\n","        if t and t.get_text(strip=True):\n","            tanggal = t.get_text(strip=True)\n","\n","    # Penulis\n","    penulis = None\n","    for sel in [\n","        \"meta[name='author']\",\n","        \"meta[property='article:author']\",\n","        \"[itemprop='author'] [itemprop='name']\",\n","        \".author, .article__author, span[class*='author']\",\n","    ]:\n","        a = soup.select_one(sel)\n","        if a:\n","            penulis = a.get(\"content\") or a.get_text(\" \", strip=True)\n","            if penulis:\n","                penulis = penulis.strip()\n","                break\n","\n","    # Meta description (untuk skor)\n","    desc = \"\"\n","    md = soup.select_one(\"meta[name='description'], meta[property='og:description']\")\n","    if md and md.get(\"content\"):\n","        desc = md[\"content\"].strip()\n","\n","    # Ambil paragraf awal artikel (untuk skor)\n","    body = \"\"\n","    # beberapa pola umum konten artikel kompas\n","    body_sel = [\n","        \"[itemprop='articleBody']\",\n","        \"div.read__content\",\n","        \"div#read__content\",\n","        \"article\",\n","    ]\n","    for sel in body_sel:\n","        node = soup.select_one(sel)\n","        if node:\n","            # Ambil 1-3 paragraf awal agar tidak berat\n","            ps = node.select(\"p\")\n","            if ps:\n","                body = \" \".join(p.get_text(\" \", strip=True) for p in ps[:3])\n","                break\n","    if not body:\n","        # fallback: ambil seluruh teks paragraf pertama yang terlihat\n","        p = soup.find(\"p\")\n","        if p:\n","            body = p.get_text(\" \", strip=True)\n","\n","    return {\n","        \"title\": title or \"\",\n","        \"tanggal\": tanggal or \"\",\n","        \"penulis\": penulis or \"\",\n","        \"desc\": desc,\n","        \"body\": body,\n","    }\n","\n","# =========================\n","# Main scraper\n","# =========================\n","@dataclass\n","class Row:\n","    judul_berita: Optional[str] = None\n","    tanggal_berita: Optional[str] = None\n","    penulis_berita: Optional[str] = None\n","    url_berita: Optional[str] = None\n","    keterangan: Optional[str] = None\n","    _score: Optional[int] = None   # kolom internal (debug)\n","    _hits: Optional[dict] = None   # kolom internal (debug)\n","\n","def scrape_kompas(\n","    topic: str,\n","    max_pages: int = 3,\n","    min_score: int = 3,            # ambang skor relevansi\n","    require_all_terms: bool = True, # semua kata harus muncul di total teks\n","    require_phrase: bool = False,   # jika ada \"frasa\" di topik, wajib muncul\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Kembalikan DataFrame (judul_berita, tanggal_berita, penulis_berita, url_berita)\n","    yang sudah difilter agar relevan terhadap 'topic'.\n","    \"\"\"\n","    topic = (topic or \"\").strip()\n","    if not topic:\n","        raise ValueError(\"Parameter 'topic' kosong.\")\n","\n","    session = requests.Session()\n","    candidate_urls: List[str] = []\n","\n","    for page in range(1, max_pages + 1):\n","        search_url = build_search_url(topic, page)\n","        resp = http_get(search_url, session=session)\n","        if resp is None:\n","            print(f\"[WARN] Melewati halaman {page} (gagal load).\")\n","            continue\n","        soup = BeautifulSoup(resp.text, \"lxml\")\n","        page_links = extract_search_links(soup)\n","        if not page_links:\n","            print(f\"[INFO] Tidak ada link kandidat pada halaman {page}.\")\n","            if page == 1:\n","                break\n","        candidate_urls.extend(page_links)\n","        _sleep(0.8, 1.5)\n","\n","    # Unik\n","    seen = set()\n","    urls = []\n","    for u in candidate_urls:\n","        if u not in seen:\n","            seen.add(u)\n","            urls.append(u)\n","\n","    rows: List[Row] = []\n","    for u in urls:\n","        r = Row(url_berita=u)\n","        try:\n","            _sleep(0.6, 1.2)\n","            art = http_get(u, session=session)\n","            if art is None:\n","                r.keterangan = \"Gagal memuat halaman artikel.\"\n","                rows.append(r)\n","                continue\n","\n","            meta = parse_article(art.text)\n","            score, hits = relevance_score(\n","                topic=topic,\n","                title=meta[\"title\"],\n","                desc=meta[\"desc\"],\n","                body=meta[\"body\"],\n","                require_all_terms=require_all_terms,\n","                require_phrase=require_phrase,\n","            )\n","\n","            if score >= min_score:\n","                r.judul_berita = meta[\"title\"]\n","                r.tanggal_berita = meta[\"tanggal\"]\n","                r.penulis_berita = meta[\"penulis\"]\n","                r._score = score\n","                r._hits = hits\n","            else:\n","                r.keterangan = f\"Terfilter (skor {score} < min_score {min_score})\"\n","                r._score = score\n","                r._hits = hits\n","\n","        except Exception as e:\n","            r.keterangan = f\"Exception saat parse artikel: {repr(e)}\"\n","        rows.append(r)\n","\n","    # Bangun DF & hanya tampilkan baris lolos filter\n","    df_full = pd.DataFrame([{\n","        \"judul_berita\": i.judul_berita,\n","        \"tanggal_berita\": i.tanggal_berita,\n","        \"penulis_berita\": i.penulis_berita,\n","        \"url_berita\": i.url_berita,\n","        \"keterangan\": i.keterangan,\n","        \"_score\": i._score,\n","        \"_hits\": i._hits\n","    } for i in rows])\n","\n","    # ambil yang relevan (judul_berita terisi)\n","    df = df_full[df_full[\"judul_berita\"].notna()].copy()\n","    df.drop_duplicates(subset=[\"url_berita\"], inplace=True, ignore_index=True)\n","\n","    # Urutkan berdasarkan skor desc (kalau ingin transparan)\n","    if \"_score\" in df.columns:\n","        df.sort_values(by=\"_score\", ascending=False, inplace=True, ignore_index=True)\n","\n","    # Kembalikan hanya kolom yang diminta + keterangan (opsional untuk audit)\n","    # Jika ingin benar-benar minimal, tinggal drop 'keterangan'\n","    return df[[\"judul_berita\", \"tanggal_berita\", \"penulis_berita\", \"url_berita\"]]\n","\n","\n","\n"],"metadata":{"id":"r_Yck-BIl0y7","executionInfo":{"status":"ok","timestamp":1758162671190,"user_tz":-420,"elapsed":1068,"user":{"displayName":"Monitoring Berita","userId":"16755502473357078001"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    # === CONTOH PENGGUNAAN ===\n","    TOPIC = 'purbaya yudhi'  # contoh: dukung kata & frasa \"subsidi bbm\"\n","    MAX_PAGES = 3\n","    date = time.strftime(\"%Y%m%d\")\n","    try:\n","        df = scrape_kompas(\n","            topic=TOPIC,\n","            max_pages=MAX_PAGES,\n","            min_score=2,             # boleh dinaikkan utk lebih ketat (mis. 5–7)\n","            require_all_terms=True,  # semua kata non-kutip wajib ada\n","            require_phrase=False,    # set True jika frasa dalam \"...\" wajib ada\n","        )\n","        print(df.head(20))\n","        # Simpan jika perlu:\n","        df.to_excel(cwd + f\"/daftar_berita/kompas/{TOPIC}_{date}.xlsx\", index=False)\n","    except Exception:\n","        print(\"[FATAL] Proses scraping gagal:\")\n","        traceback.print_exc()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VOElKLdEprVk","executionInfo":{"status":"ok","timestamp":1758162881987,"user_tz":-420,"elapsed":68982,"user":{"displayName":"Monitoring Berita","userId":"16755502473357078001"}},"outputId":"e7997021-0bb4-4420-a616-05664667fbda"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["                                         judul_berita  \\\n","0   Bisakah Ekonomi Indonesia Membaik di Tangan Me...   \n","1   Menteri Keuangan Baru Purbaya Yudhi Sadewa: Ko...   \n","2   Sah, Purbaya Yudhi Sadewa Gantikan Sri Mulyani...   \n","3   Profil Purbaya Yudhi Sadewa, Menteri Keuangan ...   \n","4   Profil Pendidikan Purbaya Yudhi Sadewa, Menkeu...   \n","5   Purbaya Yudhi Sadewo, Menkeu Baru Lulusan Elek...   \n","6   Baru 3 Hari Jadi Menteri Keuangan, Purbaya Yud...   \n","7   Profil Menkeu Purbaya Yudhi Sadewa, Bos LPS ya...   \n","8   Profil dan Daftar Kekayaan Purbaya Yudhi Sadew...   \n","9   Siapa Purbaya Yudhi Sadewa, Menkeu Baru Pengga...   \n","10  Jejak Karier Purbaya Yudhi Sadewa, Menteri Keu...   \n","11  Profil Purbaya Yudhi Sadewa, Menteri Keuangan ...   \n","12  Prabowo Ganti Sri Mulyani dengan Purbaya Yudhi...   \n","13  Profil Purbaya Yudhi Sadewa, Menteri Keuangan ...   \n","14  Perjalanan Karier Purbaya Yudhi Sadewa, Menter...   \n","15  Anak Sebut Sri Mulyani Agen CIA, Menkeu Purbay...   \n","16  Klarifikasi Menkeu Purbaya Yudhi soal Anak Seb...   \n","17  IHSG Melemah Dua Hari Beruntun, Ekonom: Pasar ...   \n","18  Menkeu Purbaya Yudhi Sadewa Tanggapi 17+8 Tunt...   \n","19  Purbaya Yudhi Sadewa Jadi Menkeu Baru: Kata Is...   \n","\n","               tanggal_berita      penulis_berita  \\\n","0   2025-09-09T09:30:00+00:00  Kompas Cyber Media   \n","1   2025-09-09T00:02:00+00:00  Kompas Cyber Media   \n","2   2025-09-08T09:24:14+00:00  Kompas Cyber Media   \n","3   2025-09-08T09:05:43+00:00  Kompas Cyber Media   \n","4   2025-09-08T10:16:42+00:00  Kompas Cyber Media   \n","5   2025-09-08T23:08:05+00:00  Kompas Cyber Media   \n","6   2025-09-11T06:00:00+00:00  Kompas Cyber Media   \n","7   2025-09-08T09:55:01+00:00  Kompas Cyber Media   \n","8   2025-09-08T10:37:12+00:00  Kompas Cyber Media   \n","9   2025-09-08T22:45:00+00:00  Kompas Cyber Media   \n","10  2025-09-08T11:20:44+00:00  Kompas Cyber Media   \n","11  2025-09-08T11:45:00+00:00  Kompas Cyber Media   \n","12  2025-09-08T09:13:10+00:00  Kompas Cyber Media   \n","13  2025-09-08T10:38:42+00:00  Kompas Cyber Media   \n","14  2025-09-08T09:27:48+00:00  Kompas Cyber Media   \n","15  2025-09-11T10:15:00+00:00  Kompas Cyber Media   \n","16  2025-09-11T07:00:00+00:00  Kompas Cyber Media   \n","17  2025-09-09T05:15:00+00:00  Kompas Cyber Media   \n","18  2025-09-08T13:30:00+00:00  Kompas Cyber Media   \n","19  2025-09-09T00:15:00+00:00  Kompas Cyber Media   \n","\n","                                           url_berita  \n","0   https://www.kompas.com/tren/read/2025/09/09/16...  \n","1   https://otomotif.kompas.com/read/2025/09/09/07...  \n","2   https://money.kompas.com/read/2025/09/08/16241...  \n","3   https://money.kompas.com/read/2025/09/08/16054...  \n","4   https://www.kompas.com/edu/read/2025/09/08/171...  \n","5   https://money.kompas.com/read/2025/09/09/06080...  \n","6   https://www.kompas.com/tren/read/2025/09/11/13...  \n","7   https://money.kompas.com/read/2025/09/08/16550...  \n","8   https://www.kompas.com/kalimantan-timur/read/2...  \n","9   https://www.kompas.com/kalimantan-timur/read/2...  \n","10  https://money.kompas.com/read/2025/09/08/18204...  \n","11  https://www.kompas.com/jawa-timur/read/2025/09...  \n","12  https://nasional.kompas.com/read/2025/09/08/16...  \n","13  https://money.kompas.com/read/2025/09/08/17384...  \n","14  https://www.kompas.com/tren/read/2025/09/08/16...  \n","15  https://www.kompas.com/kalimantan-timur/read/2...  \n","16  https://www.kompas.com/riau/read/2025/09/11/14...  \n","17  https://www.kompas.com/lampung/read/2025/09/09...  \n","18  https://www.kompas.com/jawa-timur/read/2025/09...  \n","19  https://www.kompas.com/tren/read/2025/09/09/07...  \n"]}]}]}