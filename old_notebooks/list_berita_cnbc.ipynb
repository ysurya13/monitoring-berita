{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMGD7470cE5U6quqRv0MCwu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LkyXDX7ukGmJ","executionInfo":{"status":"ok","timestamp":1758110760341,"user_tz":-420,"elapsed":21328,"user":{"displayName":"Monitoring Berita","userId":"16755502473357078001"}},"outputId":"04d48e44-6675-4688-8323-e16c53368b06"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# mount the colab with google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# set folder tempat kerja (current working directory)\n","import os\n","\n","cwd = '/content/drive/MyDrive/Monitoring Berita'\n","os.chdir(cwd)"],"metadata":{"id":"Zgi99uL9kKwc","executionInfo":{"status":"ok","timestamp":1758110762484,"user_tz":-420,"elapsed":192,"user":{"displayName":"Monitoring Berita","userId":"16755502473357078001"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import re\n","import time\n","import math\n","import logging\n","from typing import List, Dict, Optional\n","import requests\n","from requests.adapters import HTTPAdapter\n","from urllib3.util.retry import Retry\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","\n","logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n","\n","CNBC_SEARCH_URL = \"https://www.cnbcindonesia.com/search\"\n","\n","def _build_session(total_retries: int = 5, backoff: float = 0.5) -> requests.Session:\n","    \"\"\"Create a requests session with robust retry strategy.\"\"\"\n","    session = requests.Session()\n","    retries = Retry(\n","        total=total_retries,\n","        connect=total_retries,\n","        read=total_retries,\n","        status=total_retries,\n","        backoff_factor=backoff,\n","        status_forcelist=(429, 500, 502, 503, 504),\n","        allowed_methods=frozenset([\"GET\"]),\n","        raise_on_status=False,\n","    )\n","    adapter = HTTPAdapter(max_retries=retries, pool_connections=20, pool_maxsize=20)\n","    session.mount(\"http://\", adapter)\n","    session.mount(\"https://\", adapter)\n","    session.headers.update({\n","        # User-Agent realistis agar tidak langsung diblokir\n","        \"User-Agent\": (\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n","                       \"AppleWebKit/605.1.15 (KHTML, like Gecko) Version/18.6 Safari/605.1.15\"),\n","        \"Accept-Language\": \"id,en;q=0.9\",\n","        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n","        \"Cache-Control\": \"no-cache\",\n","    })\n","    return session\n","\n","def _safe_get_text(el) -> str:\n","    return (el.get_text(strip=True) if el else \"\").strip()\n","\n","def _first(soup: BeautifulSoup, selectors: List[str]):\n","    for sel in selectors:\n","        found = soup.select_one(sel)\n","        if found:\n","            return found\n","    return None\n","\n","def _extract_cards(soup: BeautifulSoup) -> List[BeautifulSoup]:\n","    \"\"\"\n","    Kumpulkan node 'kartu' hasil dari berbagai kemungkinan struktur.\n","    \"\"\"\n","    candidates = []\n","    # Pola umum daftar hasil (ul > li > article / div)\n","    candidates += soup.select(\"ul li article\")\n","    candidates += soup.select(\"ul li div\")\n","    candidates += soup.select(\"div.list, div.list-content, div.media, article\")\n","    # Dedup by id() pointer\n","    uniq = []\n","    seen = set()\n","    for c in candidates:\n","        if id(c) not in seen:\n","            uniq.append(c)\n","            seen.add(id(c))\n","    return uniq\n","\n","def _guess_anchor(card: BeautifulSoup):\n","    \"\"\"\n","    Cari anchor (link) utama ke artikel dari sebuah 'card'.\n","    \"\"\"\n","    # Cari <a> dengan href domain cnbcindonesia & path mengandung /news/ atau kategori lain\n","    anchors = card.select(\"a[href]\")\n","    for a in anchors:\n","        href = a.get(\"href\", \"\")\n","        if \"cnbcindonesia.com\" in href and re.search(r\"/(news|market|tech|syariah|lifestyle|investment|entrepreneur|opini)/\", href):\n","            return a\n","    # fallback: ambil anchor pertama yang mengandung judul\n","    title_like = card.select_one(\"a[title], h2 a, h3 a\")\n","    return title_like or (anchors[0] if anchors else None)\n","\n","def _parse_date_author_from_detail(session: requests.Session, url: str, timeout: int = 12) -> Dict[str, str]:\n","    \"\"\"\n","    Buka halaman detail artikel untuk mengambil tanggal & penulis dari meta tag.\n","    \"\"\"\n","    out = {\"tanggal_berita\": \"\", \"penulis_berita\": \"\"}\n","    try:\n","        r = session.get(url, timeout=timeout)\n","        r.raise_for_status()\n","        dsoup = BeautifulSoup(r.text, \"lxml\")\n","        # Meta tanggal (beberapa kemungkinan)\n","        date_meta = _first(dsoup, [\n","            'meta[property=\"article:published_time\"]',\n","            'meta[name=\"date\"]',\n","            'meta[itemprop=\"datePublished\"]',\n","            'time[datetime]'\n","        ])\n","        if date_meta:\n","            out[\"tanggal_berita\"] = date_meta.get(\"content\") or date_meta.get(\"datetime\") or _safe_get_text(date_meta)\n","\n","        # Penulis\n","        author_meta = _first(dsoup, [\n","            'meta[name=\"author\"]',\n","            'meta[property=\"article:author\"]',\n","            '[itemprop=\"author\"]',\n","            '.author, .byline, .reporter'\n","        ])\n","        if author_meta:\n","            out[\"penulis_berita\"] = author_meta.get(\"content\") or _safe_get_text(author_meta)\n","    except requests.RequestException as e:\n","        logging.warning(f\"Gagal ambil detail: {url} -> {e}\")\n","    except Exception as e:\n","        logging.warning(f\"Parse detail error: {url} -> {e}\")\n","    return out\n","\n","def scrape_cnbc_search(\n","    query: str = \"kemenkeu\",\n","    max_pages: int = 2,\n","    delay_sec: float = 0.6,\n","    fetch_detail: bool = True,\n","    stop_if_no_results: bool = True,\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Scrape hasil pencarian CNBC Indonesia untuk `query` dan kembalikan DataFrame:\n","    kolom: judul_berita, tanggal_berita, penulis_berita, url_berita\n","\n","    Args:\n","        query: kata kunci pencarian.\n","        max_pages: jumlah halaman pencarian yang akan di-scrape.\n","        delay_sec: jeda antar request untuk sopan & menghindari rate limit.\n","        fetch_detail: kalau True, buka halaman artikel untuk tanggal/penulis akurat.\n","        stop_if_no_results: berhenti lebih awal bila suatu halaman tidak punya hasil.\n","\n","    Returns:\n","        pandas.DataFrame\n","    \"\"\"\n","    session = _build_session()\n","    rows: List[Dict[str, str]] = []\n","    for page in range(1, max_pages + 1):\n","        params = {\"query\": query}\n","        if page > 1:\n","            params[\"page\"] = page\n","\n","        url = CNBC_SEARCH_URL\n","        try:\n","            resp = session.get(url, params=params, timeout=15)\n","            # Jika respon mengarah ke challenge (403/503), raise_for_status tidak selalu error.\n","            if resp.status_code >= 400:\n","                logging.warning(f\"Halaman {page} -> HTTP {resp.status_code}, mencoba lanjut...\")\n","            html = resp.text\n","        except requests.RequestException as e:\n","            logging.error(f\"Gagal meminta halaman pencarian (page {page}): {e}\")\n","            if stop_if_no_results:\n","                break\n","            else:\n","                continue\n","\n","        soup = BeautifulSoup(html, \"lxml\")\n","\n","        cards = _extract_cards(soup)\n","        page_rows = []\n","        for card in cards:\n","            try:\n","                a = _guess_anchor(card)\n","                if not a:\n","                    continue\n","                href = a.get(\"href\", \"\").strip()\n","                title = (a.get(\"title\") or _safe_get_text(a)).strip()\n","                if not href or not title:\n","                    continue\n","                # Normalisasi URL relatif -> absolut\n","                if href.startswith(\"/\"):\n","                    href = \"https://www.cnbcindonesia.com\" + href\n","\n","                # Ambil tanggal/penulis dari list jika tersedia (fallback)\n","                # CNBC sering menaruh tanggal/penulis di elemen sekitar judul\n","                date_el = _first(card, [\".date\", \".tanggal\", \"time\", \".publish-date\"])\n","                author_el = _first(card, [\".author\", \".byline\", \".reporter\", \"[itemprop='author']\"])\n","                tanggal = _safe_get_text(date_el)\n","                penulis = _safe_get_text(author_el)\n","\n","                # Jika fetch_detail diaktifkan atau data masih kosong, buka halaman detail\n","                if fetch_detail and (not tanggal or not penulis):\n","                    detail = _parse_date_author_from_detail(session, href)\n","                    tanggal = detail.get(\"tanggal_berita\") or tanggal\n","                    penulis = detail.get(\"penulis_berita\") or penulis\n","\n","                page_rows.append({\n","                    \"judul_berita\": title,\n","                    \"tanggal_berita\": tanggal,\n","                    \"penulis_berita\": penulis,\n","                    \"url_berita\": href\n","                })\n","            except Exception as e:\n","                # Tangkap error parsing di level kartu agar kartu lain tetap diproses\n","                logging.warning(f\"Error parsing card di page {page}: {e}\")\n","                continue\n","\n","        # Filter duplikat berdasarkan URL\n","        before = len(page_rows)\n","        seen_urls = set()\n","        deduped = []\n","        for r in page_rows:\n","            if r[\"url_berita\"] not in seen_urls:\n","                deduped.append(r)\n","                seen_urls.add(r[\"url_berita\"])\n","        after = len(deduped)\n","        if before != after:\n","            logging.info(f\"Page {page}: {before - after} duplikat dihapus.\")\n","\n","        rows.extend(deduped)\n","\n","        logging.info(f\"Page {page}: {len(deduped)} artikel ditemukan.\")\n","        # Berhenti bila halaman tampak kosong\n","        if stop_if_no_results and len(deduped) == 0:\n","            logging.info(\"Tidak ada hasil di halaman ini. Stop lebih awal.\")\n","            break\n","\n","        time.sleep(delay_sec)\n","\n","    # DataFrame akhir\n","    df = pd.DataFrame(rows, columns=[\"judul_berita\", \"tanggal_berita\", \"penulis_berita\", \"url_berita\"]).drop_duplicates(subset=[\"url_berita\"])\n","    return df"],"metadata":{"id":"34ei9k2CkU23","executionInfo":{"status":"ok","timestamp":1758111015376,"user_tz":-420,"elapsed":113,"user":{"displayName":"Monitoring Berita","userId":"16755502473357078001"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    TOPIC = \"kemenkeu\"\n","    df = scrape_cnbc_search(query=TOPIC, max_pages=2, delay_sec=0.8, fetch_detail=True)\n","    date = time.strftime(\"%Y%m%d\")\n","    df.to_excel(cwd + f\"/daftar_berita/cnbc/{TOPIC}_{date}.xlsx\", index=False)\n","    # Simpan jika perlu:\n","    # df.to_csv(\"cnbc_kemenkeu.csv\", index=False, encoding=\"utf-8\")"],"metadata":{"id":"6prWbIK6khAc","executionInfo":{"status":"ok","timestamp":1758111040993,"user_tz":-420,"elapsed":14039,"user":{"displayName":"Monitoring Berita","userId":"16755502473357078001"}}},"execution_count":10,"outputs":[]}]}