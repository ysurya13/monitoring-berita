{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nl15yqDXsRcB"
   },
   "source": [
    "# SETTING ENVIRONMENT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1597,
     "status": "ok",
     "timestamp": 1758706386928,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "f8BqutX0SOd1",
    "outputId": "7c8350af-4d49-4ce6-b4e5-d931f77f4a62"
   },
   "outputs": [],
   "source": [
    "\"\"\"# mount the colab with google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1758706386929,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "eLOguPRQSRLN"
   },
   "outputs": [],
   "source": [
    "# set folder tempat kerja (current working directory)\n",
    "import os\n",
    "\n",
    "cwd = \"/Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Pekerjaan BMN/05. 2025/98_monitoring_berita/monitoring-berita\"\n",
    "# cwd = '/content/drive/MyDrive/Monitoring Berita'\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-yckWeo9oMl"
   },
   "source": [
    "# MAIN CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SikthOGY9vWX"
   },
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4703,
     "status": "ok",
     "timestamp": 1758706391627,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "Dj9OlaIX9eZf"
   },
   "outputs": [],
   "source": [
    "# # Scraping Artikel Berita Multisumber (Fleksibel)\n",
    "# - Membaca list URL dari Excel\n",
    "# - Ekstraksi teks artikel bersih (tanpa iklan/link terkait)\n",
    "# - Error handling kuat, retry, timeout\n",
    "# - Site-specific extractor + fallback readability/trafilatura + heuristik generik\n",
    "# - Output dataframe: url_berita, artikel_berita, source_domain, status, error\n",
    "\n",
    "# %%\n",
    "!pip -q install beautifulsoup4 lxml html5lib requests-html readability-lxml trafilatura tqdm pandas openpyxl regex > /dev/null\n",
    "\n",
    "# --- Sel 1 (tambahkan paket) ---\n",
    "!pip -q install cloudscraper httpx > /dev/null\n",
    "\n",
    "\n",
    "# %%\n",
    "import re\n",
    "import regex as reg\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Tuple, Callable, Dict\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Optional imports (fallback parsers)\n",
    "try:\n",
    "    from readability import Document\n",
    "    HAS_READABILITY = True\n",
    "except Exception:\n",
    "    HAS_READABILITY = False\n",
    "\n",
    "try:\n",
    "    import trafilatura\n",
    "    HAS_TRAFILATURA = True\n",
    "except Exception:\n",
    "    HAS_TRAFILATURA = False\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(\"news-scraper\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alOHs_ZY9xBp"
   },
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 451,
     "status": "ok",
     "timestamp": 1758706392077,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "LbKEmBgO9rBU"
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "# ======== PARAMETER YANG MUDAH DIUBAH ========\n",
    "\n",
    "# config path\n",
    "from pathlib import Path\n",
    "\n",
    "# CONFIG_PATH = Path(\"/content/drive/MyDrive/Monitoring Berita/config.json\")\n",
    "\n",
    "CONFIG_PATH = Path(f\"{cwd}/config.json\")\n",
    "\n",
    "with open(CONFIG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            cfg = json.load(f)\n",
    "\n",
    "# INPUT_EXCEL_PATH = cfg[\"labelled_data_xlsx\"]\n",
    "INPUT_CSV_PATH = cfg[\"last_output_path\"]\n",
    "INPUT_URL_COLUMN = \"url_berita\"\n",
    "\n",
    "# Batasi jumlah URL saat uji (None untuk semua)\n",
    "MAX_URLS: Optional[int] = None\n",
    "\n",
    "# Timeout & retry\n",
    "REQUEST_TIMEOUT = 20\n",
    "MAX_RETRIES = 2\n",
    "RETRY_SLEEP = 1.5\n",
    "\n",
    "# User-Agent khusus agar tidak diblok mudah\n",
    "DEFAULT_HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
    "        \"(KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# Pola frasa yang ingin dihapus dari artikel (iklan/clickbait/relate links)\n",
    "CLEANUP_PATTERNS = [\n",
    "    r\"\\bBaca juga\\b.*\", r\"\\bSimak juga\\b.*\", r\"\\bADVERTISEMENT\\b.*\",\n",
    "    r\"\\bIklan\\b.*\", r\"\\bIklan Layanan\\b.*\", r\"\\bInfografis\\b.*\",\n",
    "    r\"\\bTonton juga\\b.*\", r\"\\bVideo:\\b.*\", r\"\\b[Gg]rafis\\b.*\",\n",
    "    r\"\\b[Gg]allery\\b.*\", r\"\\bArtikel ini telah\\b.*\", r\"\\bEditor:\\b.*$\",\n",
    "    r\"^\\s*—\\s*$\", r\"^\\s*-\\s*$\", r\"\\b[ \\t]*[•\\-\\*]\\s*$\"\n",
    "]\n",
    "\n",
    "# Tag/kelas yang menandai elemen non-konten yang harus dibuang\n",
    "JUNK_SELECTORS = [\n",
    "    \"[class*=iklan]\", \"[class*=ads]\", \"[id*=ads]\", \"[id*=banner]\", \"[class*=related]\",\n",
    "    \"[class*=tag]\", \"[class*=breadcrumb]\", \"script\", \"style\", \"noscript\", \"iframe\",\n",
    "    \"[class*=share]\", \"[class*=social]\", \"[class*=recommend]\", \"[class*=promo]\",\n",
    "    \"[class*=copyright]\", \"[class*=author]\", \"[class*=metadata]\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weVk6VCw9y3X"
   },
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1758706392090,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "MxFtkgnZ9yck"
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "session = requests.Session()\n",
    "session.headers.update(DEFAULT_HEADERS)\n",
    "\n",
    "def fetch(url: str, timeout: int = REQUEST_TIMEOUT, max_retries: int = MAX_RETRIES) -> Optional[requests.Response]:\n",
    "    \"\"\"\n",
    "    Fetch URL dengan retry sederhana dan timeout.\n",
    "    \"\"\"\n",
    "    last_err = None\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            resp = session.get(url, timeout=timeout, allow_redirects=True)\n",
    "            if 200 <= resp.status_code < 300:\n",
    "                return resp\n",
    "            else:\n",
    "                last_err = RuntimeError(f\"HTTP {resp.status_code}\")\n",
    "                logger.warning(f\"Attempt {attempt}: {url} -> {resp.status_code}\")\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            logger.warning(f\"Attempt {attempt} failed for {url}: {e}\")\n",
    "        time.sleep(RETRY_SLEEP * attempt)\n",
    "    logger.error(f\"Failed to fetch {url}: {last_err}\")\n",
    "    return None\n",
    "\n",
    "def normalize_whitespace(text: str) -> str:\n",
    "    text = reg.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = reg.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def remove_cleanup_patterns(text: str, patterns: List[str]) -> str:\n",
    "    out = text\n",
    "    for pat in patterns:\n",
    "        out = re.sub(pat, \"\", out, flags=re.IGNORECASE)\n",
    "    # Buang baris kosong sisa\n",
    "    out = \"\\n\".join([ln.strip() for ln in out.splitlines() if ln.strip()])\n",
    "    return normalize_whitespace(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1758706392113,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "ApSGHWC6C0rc"
   },
   "outputs": [],
   "source": [
    "# --- Sel 3 (ganti sesi & fetch) ---\n",
    "import random\n",
    "import httpx\n",
    "import cloudscraper\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Headers \"benar-bener browser\"\n",
    "BROWSER_HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "        \"(KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36\"\n",
    "    ),\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"id-ID,id;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "    \"Cache-Control\": \"no-cache\",\n",
    "    \"Pragma\": \"no-cache\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "}\n",
    "\n",
    "def make_session():\n",
    "    # cloudscraper biasanya lolos 403 di situs yang pakai anti-bot\n",
    "    try:\n",
    "        s = cloudscraper.create_scraper(\n",
    "            browser={\"browser\": \"chrome\", \"platform\": \"windows\", \"mobile\": False}\n",
    "        )\n",
    "        s.headers.update(BROWSER_HEADERS)\n",
    "        return s\n",
    "    except Exception:\n",
    "        s = requests.Session()\n",
    "        s.headers.update(BROWSER_HEADERS)\n",
    "        return s\n",
    "\n",
    "session = make_session()\n",
    "\n",
    "def jitter_sleep(base=REQUEST_TIMEOUT * 0.05):\n",
    "    time.sleep(base + random.random() * 0.8)\n",
    "\n",
    "def as_amp(url: str) -> str:\n",
    "    # Coba bentuk /amp di Tribunnews (tanpa query).\n",
    "    # https://www.tribunnews.com/... -> https://www.tribunnews.com/.../amp\n",
    "    u = url.split(\"?\")[0].rstrip(\"/\")\n",
    "    return u + \"/amp\"\n",
    "\n",
    "def fetch(url: str, timeout: int = REQUEST_TIMEOUT, max_retries: int = MAX_RETRIES) -> Optional[requests.Response]:\n",
    "    \"\"\"\n",
    "    Fetch dengan retry + header lengkap; khusus Tribunnews:\n",
    "      - set Referer: Google\n",
    "      - jika 403: coba versi AMP\n",
    "    \"\"\"\n",
    "    last_err = None\n",
    "    parsed = urlparse(url)\n",
    "    is_tribun = \"tribunnews.com\" in (parsed.netloc or \"\")\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            headers = BROWSER_HEADERS.copy()\n",
    "            # Referer membantu untuk situs yang protektif\n",
    "            headers[\"Referer\"] = \"https://www.google.com/\"\n",
    "            resp = session.get(url, headers=headers, timeout=timeout, allow_redirects=True)\n",
    "            if 200 <= resp.status_code < 300:\n",
    "                return resp\n",
    "\n",
    "            # Kalau 403 dan Tribun, langsung coba AMP di attempt berikutnya\n",
    "            if is_tribun and resp.status_code in (401, 403):\n",
    "                logger.warning(f\"Attempt {attempt}: {url} -> {resp.status_code}. Trying AMP...\")\n",
    "                amp_url = as_amp(url)\n",
    "                try:\n",
    "                    resp2 = session.get(amp_url, headers=headers, timeout=timeout, allow_redirects=True)\n",
    "                    if 200 <= resp2.status_code < 300:\n",
    "                        # inject URL supaya downstream tahu url asli (opsional)\n",
    "                        resp2.url = url\n",
    "                        return resp2\n",
    "                    else:\n",
    "                        last_err = RuntimeError(f\"AMP HTTP {resp2.status_code}\")\n",
    "                except Exception as e2:\n",
    "                    last_err = e2\n",
    "            else:\n",
    "                last_err = RuntimeError(f\"HTTP {resp.status_code}\")\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            logger.warning(f\"Attempt {attempt} failed for {url}: {e}\")\n",
    "\n",
    "        # sedikit jeda supaya tidak dicap bot\n",
    "        jitter_sleep(0.6)\n",
    "    logger.error(f\"Failed to fetch {url}: {last_err}\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHSDUrPB94_z"
   },
   "source": [
    "## 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1758706392177,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "V0Oasond9132"
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "CANDIDATE_CONTENT_SELECTORS = [\n",
    "    \"article\",\n",
    "    \"[class*=content]\",\n",
    "    \"[class*=article]\",\n",
    "    \"[class*=read__content]\",\n",
    "    \"[class*=detail__body]\",\n",
    "    \"[class*=entry-content]\",\n",
    "    \"[id*=content]\", \"[id*=article]\",\n",
    "    \"[itemprop='articleBody']\",\n",
    "]\n",
    "\n",
    "def clean_soup(soup: BeautifulSoup) -> None:\n",
    "    # Hapus elemen junk\n",
    "    for sel in JUNK_SELECTORS:\n",
    "        for el in soup.select(sel):\n",
    "            el.decompose()\n",
    "\n",
    "def join_block_text(container: Tag) -> str:\n",
    "    # Ambil <p>, <h2/3/4>, <li> sebagai paragraf/kalimat\n",
    "    parts: List[str] = []\n",
    "    for el in container.descendants:\n",
    "        if isinstance(el, Tag):\n",
    "            if el.name in {\"p\", \"h2\", \"h3\", \"h4\", \"li\"}:\n",
    "                txt = el.get_text(separator=\" \", strip=True)\n",
    "                if txt and len(txt) > 2:\n",
    "                    parts.append(txt)\n",
    "        elif isinstance(el, NavigableString):\n",
    "            # Abaikan NavigableString langsung agar tidak duplikat\n",
    "            pass\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "def generic_extract(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    clean_soup(soup)\n",
    "\n",
    "    # 1) cari tag <article> atau kandidat body konten\n",
    "    for sel in CANDIDATE_CONTENT_SELECTORS:\n",
    "        nodes = soup.select(sel)\n",
    "        for node in nodes:\n",
    "            text = join_block_text(node)\n",
    "            if text and len(text) > 300:  # ambang minimal isi artikel\n",
    "                return text\n",
    "\n",
    "    # 2) fallback: ambil konten terpanjang dari beberapa kandidat\n",
    "    candidates = []\n",
    "    for node in soup.find_all([\"article\", \"div\", \"section\"], limit=50):\n",
    "        text = join_block_text(node)\n",
    "        if text:\n",
    "            candidates.append((len(text), text))\n",
    "    if candidates:\n",
    "        candidates.sort(reverse=True, key=lambda x: x[0])\n",
    "        return candidates[0][1]\n",
    "\n",
    "    # 3) fallback terakhir: keseluruhan dokumen (p, h2, li)\n",
    "    body = soup.body or soup\n",
    "    text = join_block_text(body)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MjvftAI97p5"
   },
   "source": [
    "## 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1758706392221,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "M7QzWB2E96C_"
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "def extract_detik(soup: BeautifulSoup) -> Optional[str]:\n",
    "    clean_soup(soup)\n",
    "    # Detik sering pakai class berikut:\n",
    "    for sel in [\n",
    "        \"article.detail__article\",\n",
    "        \"div.detail__body-text\",\n",
    "        \"[class*=detail__body]\",\n",
    "        \"[class*=itp_bodycontent]\"\n",
    "    ]:\n",
    "        nodes = soup.select(sel)\n",
    "        for n in nodes:\n",
    "            txt = join_block_text(n)\n",
    "            if txt and len(txt) > 200:\n",
    "                return txt\n",
    "    return None\n",
    "\n",
    "def extract_cnbcindo(soup: BeautifulSoup) -> Optional[str]:\n",
    "    clean_soup(soup)\n",
    "    for sel in [\n",
    "        \"article\",\n",
    "        \"div.detail_text\",\n",
    "        \"[class*=detail__body]\"\n",
    "    ]:\n",
    "        nodes = soup.select(sel)\n",
    "        for n in nodes:\n",
    "            txt = join_block_text(n)\n",
    "            if txt and len(txt) > 200:\n",
    "                return txt\n",
    "    return None\n",
    "\n",
    "def extract_kompas(soup: BeautifulSoup) -> Optional[str]:\n",
    "    clean_soup(soup)\n",
    "    for sel in [\n",
    "        \"div.read__content\",\n",
    "        \"article\",\n",
    "        \"[class*=read__content]\",\n",
    "    ]:\n",
    "        nodes = soup.select(sel)\n",
    "        for n in nodes:\n",
    "            txt = join_block_text(n)\n",
    "            if txt and len(txt) > 200:\n",
    "                return txt\n",
    "    return None\n",
    "\n",
    "def extract_liputan6(soup: BeautifulSoup) -> Optional[str]:\n",
    "    clean_soup(soup)\n",
    "    for sel in [\n",
    "        \"div.article-content-body__item-content\",  # paragraf-paragraf\n",
    "        \"div.article-content-body\",\n",
    "        \"article\"\n",
    "    ]:\n",
    "        nodes = soup.select(sel)\n",
    "        if nodes:\n",
    "            # gabungkan semua paragraf dari beberapa item-content\n",
    "            text_parts = []\n",
    "            for n in nodes:\n",
    "                t = join_block_text(n)\n",
    "                if t:\n",
    "                    text_parts.append(t)\n",
    "            txt = \"\\n\".join(text_parts)\n",
    "            if txt and len(txt) > 200:\n",
    "                return txt\n",
    "    return None\n",
    "\n",
    "def extract_tribun(soup: BeautifulSoup) -> Optional[str]:\n",
    "    clean_soup(soup)\n",
    "    for sel in [\n",
    "        \"div.side-article txt-article\",\n",
    "        \"div.txt-article\",\n",
    "        \"div#articlebody\",\n",
    "        \"div#article\"\n",
    "    ]:\n",
    "        nodes = soup.select(sel)\n",
    "        for n in nodes:\n",
    "            txt = join_block_text(n)\n",
    "            if txt and len(txt) > 200:\n",
    "                return txt\n",
    "    # Lainnya:\n",
    "    nodes = soup.select(\"div > p\")\n",
    "    if nodes:\n",
    "        txt = \"\\n\".join([n.get_text(\" \", strip=True) for n in nodes])\n",
    "        if len(txt) > 200:\n",
    "            return txt\n",
    "    return None\n",
    "\n",
    "def extract_merdeka(soup: BeautifulSoup) -> Optional[str]:\n",
    "    clean_soup(soup)\n",
    "    for sel in [\n",
    "        \"div.article-content\",\n",
    "        \"div.kanal-content\",\n",
    "        \"article\"\n",
    "    ]:\n",
    "        nodes = soup.select(sel)\n",
    "        for n in nodes:\n",
    "            txt = join_block_text(n)\n",
    "            if txt and len(txt) > 200:\n",
    "                return txt\n",
    "    return None\n",
    "\n",
    "def extract_antaranews(soup: BeautifulSoup) -> Optional[str]:\n",
    "    clean_soup(soup)\n",
    "    for sel in [\n",
    "        \"div.post-content\",\n",
    "        \"div#content\",\n",
    "        \"article\"\n",
    "    ]:\n",
    "        nodes = soup.select(sel)\n",
    "        for n in nodes:\n",
    "            txt = join_block_text(n)\n",
    "            if txt and len(txt) > 200:\n",
    "                return txt\n",
    "    return None\n",
    "\n",
    "DOMAIN_EXTRACTORS: Dict[str, Callable[[BeautifulSoup], Optional[str]]] = {\n",
    "    \"detik.com\": extract_detik,\n",
    "    \"cnbcindonesia.com\": extract_cnbcindo,\n",
    "    \"kompas.com\": extract_kompas,\n",
    "    \"liputan6.com\": extract_liputan6,\n",
    "    \"tribunnews.com\": extract_tribun,\n",
    "    \"merdeka.com\": extract_merdeka,\n",
    "    \"antaranews.com\": extract_antaranews,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1758706392222,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "6y8-wmqODCIF"
   },
   "outputs": [],
   "source": [
    "def extract_tribun(soup: BeautifulSoup) -> Optional[str]:\n",
    "    clean_soup(soup)\n",
    "    # Versi desktop lama/baru\n",
    "    for sel in [\n",
    "        \"div.txt-article\", \"div#article\",\n",
    "        \"div#articlebody\", \"div.side-article.txt-article\",\n",
    "        \"article\", \"[class*=read__content]\", \"[class*=detail__body]\"\n",
    "    ]:\n",
    "        for n in soup.select(sel):\n",
    "            txt = join_block_text(n)\n",
    "            if txt and len(txt) > 200:\n",
    "                return txt\n",
    "    # fallback p\n",
    "    ps = soup.select(\"article p, div p\")\n",
    "    if ps:\n",
    "        txt = \"\\n\".join([p.get_text(\" \", strip=True) for p in ps])\n",
    "        if len(txt) > 200:\n",
    "            return txt\n",
    "    return None\n",
    "\n",
    "def extract_tribun_amp(soup: BeautifulSoup) -> Optional[str]:\n",
    "    clean_soup(soup)\n",
    "    # AMP biasanya lebih bersih\n",
    "    for sel in [\n",
    "        \"div.read__content\", \"div.read__content--body\",\n",
    "        \"article\", \"[itemprop='articleBody']\"\n",
    "    ]:\n",
    "        for n in soup.select(sel):\n",
    "            txt = join_block_text(n)\n",
    "            if txt and len(txt) > 200:\n",
    "                return txt\n",
    "    ps = soup.select(\"article p, div p\")\n",
    "    if ps:\n",
    "        txt = \"\\n\".join([p.get_text(' ', strip=True) for p in ps])\n",
    "        if len(txt) > 200:\n",
    "            return txt\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1758706392223,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "d-f5WdZjDNZK"
   },
   "outputs": [],
   "source": [
    "DOMAIN_EXTRACTORS.update({\n",
    "    \"tribunnews.com\": extract_tribun,  # desktop\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5v-1yumQ98-P"
   },
   "source": [
    "## 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1758706392224,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "3xTLyjnF9-AI"
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "@dataclass\n",
    "class ExtractResult:\n",
    "    url: str\n",
    "    domain: str\n",
    "    text: Optional[str]\n",
    "    status: str\n",
    "    error: Optional[str]\n",
    "\n",
    "def extract_with_layers(url: str) -> ExtractResult:\n",
    "    \"\"\"\n",
    "    Layered extraction:\n",
    "      1) site-specific extractor (jika domain dikenali)\n",
    "      2) trafilatura (jika tersedia)\n",
    "      3) readability-lxml (jika tersedia)\n",
    "      4) generic heuristic\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    domain = parsed.netloc.lower()\n",
    "    domain_root = \".\".join(domain.split(\".\")[-3:]) if domain.count(\".\") >= 2 else domain\n",
    "\n",
    "    # Fetch HTML\n",
    "    resp = fetch(url)\n",
    "    if resp is None:\n",
    "        return ExtractResult(url, domain, None, \"fetch_failed\", \"Request failed\")\n",
    "\n",
    "    html = resp.text\n",
    "\n",
    "    # 1) Site-specific (desktop)\n",
    "    try:\n",
    "        for known in DOMAIN_EXTRACTORS:\n",
    "            if known in domain:\n",
    "                soup = BeautifulSoup(html, \"lxml\")\n",
    "                text = DOMAIN_EXTRACTORS[known](soup)\n",
    "                if text and len(text) > 150:\n",
    "                    text = remove_cleanup_patterns(text, CLEANUP_PATTERNS)\n",
    "                    return ExtractResult(url, domain, text, \"ok_site_specific\", None)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Site-specific extractor error for {url}: {e}\")\n",
    "\n",
    "    # 1b) Jika Tribun, coba ekstraktor AMP bila konten belum didapat\n",
    "    if \"tribunnews.com\" in domain:\n",
    "        try:\n",
    "            # ketika fetch() berhasil via AMP, resp.text yang kita punya bisa AMP atau desktop;\n",
    "            # kalau masih desktop & kosong, paksa unduh AMP untuk parsing\n",
    "            amp_html = None\n",
    "            if \"/amp\" in resp.url or \"amp\" in (resp.headers.get(\"content-location\", \"\") or \"\").lower():\n",
    "                amp_html = html\n",
    "            else:\n",
    "                amp_resp = fetch(as_amp(url))\n",
    "                if amp_resp is not None and 200 <= amp_resp.status_code < 300:\n",
    "                    amp_html = amp_resp.text\n",
    "            if amp_html:\n",
    "                soup_amp = BeautifulSoup(amp_html, \"lxml\")\n",
    "                text_amp = extract_tribun_amp(soup_amp)\n",
    "                if text_amp and len(text_amp) > 150:\n",
    "                    text_amp = remove_cleanup_patterns(text_amp, CLEANUP_PATTERNS)\n",
    "                    return ExtractResult(url, domain, text_amp, \"ok_tribun_amp\", None)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Tribun AMP extractor error for {url}: {e}\")\n",
    "\n",
    "\n",
    "    # 1) Site-specific\n",
    "    try:\n",
    "        for known in DOMAIN_EXTRACTORS:\n",
    "            if known in domain:\n",
    "                soup = BeautifulSoup(html, \"lxml\")\n",
    "                text = DOMAIN_EXTRACTORS[known](soup)\n",
    "                if text and len(text) > 150:\n",
    "                    text = remove_cleanup_patterns(text, CLEANUP_PATTERNS)\n",
    "                    return ExtractResult(url, domain, text, \"ok_site_specific\", None)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Site-specific extractor error for {url}: {e}\")\n",
    "\n",
    "    # 2) Trafilatura  — perbaiki kompatibilitas argumen timeout\n",
    "    if HAS_TRAFILATURA:\n",
    "        try:\n",
    "            downloaded = None\n",
    "            try:\n",
    "                import inspect\n",
    "                if \"timeout\" in inspect.signature(trafilatura.fetch_url).parameters:\n",
    "                    # versi baru: dukung timeout\n",
    "                    downloaded = trafilatura.fetch_url(url, timeout=REQUEST_TIMEOUT)\n",
    "                else:\n",
    "                    # versi lama: tidak ada timeout\n",
    "                    downloaded = trafilatura.fetch_url(url)\n",
    "            except Exception:\n",
    "                # fallback paling aman\n",
    "                downloaded = trafilatura.fetch_url(url)\n",
    "\n",
    "            if downloaded:\n",
    "                t_text = trafilatura.extract(\n",
    "                    downloaded,\n",
    "                    include_comments=False,\n",
    "                    include_tables=False,\n",
    "                    favor_recall=True,   # sedikit lebih longgar ambil teks\n",
    "                )\n",
    "                if t_text and len(t_text) > 150:\n",
    "                    t_text = remove_cleanup_patterns(t_text, CLEANUP_PATTERNS)\n",
    "                    return ExtractResult(url, domain, t_text, \"ok_trafilatura\", None)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Trafilatura extractor error for {url}: {e}\")\n",
    "\n",
    "\n",
    "    # 3) Readability\n",
    "    if HAS_READABILITY:\n",
    "        try:\n",
    "            doc = Document(html)\n",
    "            readable_html = doc.summary(html_partial=True)\n",
    "            soup = BeautifulSoup(readable_html, \"lxml\")\n",
    "            clean_soup(soup)\n",
    "            r_text = join_block_text(soup)\n",
    "            if r_text and len(r_text) > 150:\n",
    "                r_text = remove_cleanup_patterns(r_text, CLEANUP_PATTERNS)\n",
    "                return ExtractResult(url, domain, r_text, \"ok_readability\", None)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Readability extractor error for {url}: {e}\")\n",
    "\n",
    "    # 4) Generic heuristic\n",
    "    try:\n",
    "        g_text = generic_extract(html)\n",
    "        g_text = remove_cleanup_patterns(g_text, CLEANUP_PATTERNS)\n",
    "        if g_text and len(g_text) > 100:\n",
    "            return ExtractResult(url, domain, g_text, \"ok_generic\", None)\n",
    "        else:\n",
    "            return ExtractResult(url, domain, None, \"empty_after_generic\", None)\n",
    "    except Exception as e:\n",
    "        return ExtractResult(url, domain, None, \"generic_failed\", str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s10M5kbW-DNc"
   },
   "source": [
    "## 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315,
     "referenced_widgets": [
      "220218e84e7c4b5a988c5e53254cfbe9",
      "504fdeb14baf42ea8a18d6cc32baef23",
      "f3544fe90d974bfc945d30ac5e432956",
      "fc8314b543284cfda1090b53a276c097",
      "45135c05354c4242b7fd198658454bb2",
      "7eab9712b4da4180b083c35b4bcdac6c",
      "46667759861a4475a5069cb4b6847c4e",
      "b10cecf9ab3c4b9aa47e9720522de598",
      "4a3447c430df4a1ba7428d92a5bbafea",
      "af2282eee0f248e39d69f6a931f8a861",
      "584b6c4ef81a4eb8bc902c112d7fe4fd"
     ]
    },
    "executionInfo": {
     "elapsed": 73434,
     "status": "ok",
     "timestamp": 1758706465658,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "rV5jB-jc-BMU",
    "outputId": "22c850c2-8268-4d70-b1aa-2536a113c2c9"
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "# === Baca daftar URL dari Excel===\n",
    "try:\n",
    "    df_in = pd.read_csv(INPUT_CSV_PATH) # 🚀 limit ke 100 baris\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Gagal membaca Excel input: {INPUT_CSV_PATH} -> {e}\")\n",
    "\n",
    "if INPUT_URL_COLUMN not in df_in.columns:\n",
    "    raise ValueError(f\"Kolom '{INPUT_URL_COLUMN}' tidak ditemukan di file Excel.\")\n",
    "\n",
    "urls = df_in[INPUT_URL_COLUMN].dropna().astype(str).str.strip().unique().tolist()\n",
    "\n",
    "logger.info(f\"Total URL untuk diproses (dibatasi 100): {len(urls)}\")\n",
    "\n",
    "results: List[ExtractResult] = []\n",
    "for url in tqdm(urls, desc=\"Scraping artikel\"):\n",
    "    try:\n",
    "        res = extract_with_layers(url)\n",
    "        results.append(res)\n",
    "    except Exception as e:\n",
    "        parsed = urlparse(url)\n",
    "        results.append(ExtractResult(url, parsed.netloc.lower(), None, \"fatal_error\", str(e)))\n",
    "\n",
    "# === Buat dataframe hasil scraping ===\n",
    "df_scraped = pd.DataFrame([{\n",
    "    \"url_berita\": r.url,\n",
    "    \"source_domain\": r.domain,\n",
    "    \"artikel_berita\": r.text if r.text else \"\",\n",
    "    \"status\": r.status,\n",
    "    \"error\": r.error if r.error else \"\",\n",
    "} for r in results])\n",
    "\n",
    "# === Gabungkan dengan df_in, tambahkan kolom baru ===\n",
    "df_output = df_in.merge(df_scraped, on=\"url_berita\", how=\"left\")\n",
    "\n",
    "# === Bersihkan artikel_berita ===\n",
    "def final_cleanup(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = normalize_whitespace(text)\n",
    "    lines = [ln for ln in text.splitlines() if len(ln.strip()) > 2]\n",
    "    return \"\\n\".join(lines).strip()\n",
    "\n",
    "df_output[\"artikel_berita\"] = df_output[\"artikel_berita\"].apply(final_cleanup)\n",
    "df_output[\"is_empty\"] = df_output[\"artikel_berita\"].str.len().fillna(0).lt(60)\n",
    "\n",
    "# === Ringkas hasil ===\n",
    "summary = df_output[\"status\"].value_counts(dropna=False)\n",
    "logger.info(f\"\\nSummary status:\\n{summary}\")\n",
    "\n",
    "df_output.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxCAw02V-UNM"
   },
   "source": [
    "## 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 189
    },
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1758706465903,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "ljgWZvHQ-guQ",
    "outputId": "c4647f65-0b45-4d6f-eb9f-b8c0aaba96bf"
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "# Simpan sebagai CSV/Excel untuk integrasi lanjutan\n",
    "DATE_TAG = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "OUTPUT_CSV = f\"{cwd}/hasil_baca_berita/hasil_scraping_artikel_{DATE_TAG}.csv\"\n",
    "# OUTPUT_XLSX = f\"{cwd}/hasil_baca_berita/hasil_scraping_artikel_{DATE_TAG}.xlsx\"\n",
    "\n",
    "try:\n",
    "    df_output.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "    # df_output.to_excel(OUTPUT_XLSX, index=False)\n",
    "    logger.info(f\"Tersimpan: {OUTPUT_CSV}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Gagal menyimpan output: {e}\")\n",
    "\n",
    "df_output.tail(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1758706465906,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "OXkWq-Kq-kTp"
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def update_config(path: Path, new_values: dict):\n",
    "    \"\"\"Update config.json hanya pada key tertentu tanpa menimpa keseluruhan isi.\"\"\"\n",
    "    data = {}\n",
    "    if path.exists():\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Gagal membaca config lama: {e}\")\n",
    "            data = {}\n",
    "\n",
    "    # update hanya key yang diberikan\n",
    "    data.update(new_values)\n",
    "\n",
    "    try:\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "        logger.info(f\"Berhasil update config.json di {path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Gagal menyimpan config.json: {e}\")\n",
    "\n",
    "# Simpan OUTPUT_CSV & OUTPUT_XLSX ke config dengan nama yang lebih jelas\n",
    "update_config(CONFIG_PATH, {\n",
    "    \"last_output_path\": OUTPUT_CSV,\n",
    "})\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNOC2ysORhDQCWdElJ2IoaU",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "220218e84e7c4b5a988c5e53254cfbe9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_504fdeb14baf42ea8a18d6cc32baef23",
       "IPY_MODEL_f3544fe90d974bfc945d30ac5e432956",
       "IPY_MODEL_fc8314b543284cfda1090b53a276c097"
      ],
      "layout": "IPY_MODEL_45135c05354c4242b7fd198658454bb2"
     }
    },
    "45135c05354c4242b7fd198658454bb2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46667759861a4475a5069cb4b6847c4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4a3447c430df4a1ba7428d92a5bbafea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "504fdeb14baf42ea8a18d6cc32baef23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7eab9712b4da4180b083c35b4bcdac6c",
      "placeholder": "​",
      "style": "IPY_MODEL_46667759861a4475a5069cb4b6847c4e",
      "value": "Scraping artikel: 100%"
     }
    },
    "584b6c4ef81a4eb8bc902c112d7fe4fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7eab9712b4da4180b083c35b4bcdac6c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af2282eee0f248e39d69f6a931f8a861": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b10cecf9ab3c4b9aa47e9720522de598": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f3544fe90d974bfc945d30ac5e432956": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b10cecf9ab3c4b9aa47e9720522de598",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4a3447c430df4a1ba7428d92a5bbafea",
      "value": 100
     }
    },
    "fc8314b543284cfda1090b53a276c097": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_af2282eee0f248e39d69f6a931f8a861",
      "placeholder": "​",
      "style": "IPY_MODEL_584b6c4ef81a4eb8bc902c112d7fe4fd",
      "value": " 100/100 [01:13&lt;00:00,  1.87it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
