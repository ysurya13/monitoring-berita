{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nl15yqDXsRcB"
   },
   "source": [
    "# SETTING ENVIRONMENT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1597,
     "status": "ok",
     "timestamp": 1758706386928,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "f8BqutX0SOd1",
    "outputId": "7c8350af-4d49-4ce6-b4e5-d931f77f4a62"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# mount the colab with google drive\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# mount the colab with google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1758706386929,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "eLOguPRQSRLN"
   },
   "outputs": [],
   "source": [
    "# set folder tempat kerja (current working directory)\n",
    "import os\n",
    "\n",
    "cwd = \"/Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Pekerjaan BMN/05. 2025/98_monitoring_berita/monitoring-berita\"\n",
    "# cwd = '/content/drive/MyDrive/Monitoring Berita'\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-yckWeo9oMl"
   },
   "source": [
    "# MAIN CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SikthOGY9vWX"
   },
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 4703,
     "status": "ok",
     "timestamp": 1758706391627,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "Dj9OlaIX9eZf"
   },
   "outputs": [],
   "source": [
    "# # Scraping Artikel Berita Multisumber (Fleksibel)\n",
    "# - Membaca list URL dari Excel\n",
    "# - Ekstraksi teks artikel bersih (tanpa iklan/link terkait)\n",
    "# - Error handling kuat, retry, timeout\n",
    "# - Site-specific extractor + fallback readability/trafilatura + heuristik generik\n",
    "# - Output dataframe: url_berita, artikel_berita, source_domain, status, error\n",
    "\n",
    "# %%\n",
    "!pip -q install beautifulsoup4 lxml html5lib requests-html readability-lxml trafilatura tqdm pandas openpyxl regex > /dev/null\n",
    "\n",
    "# --- Sel 1 (tambahkan paket) ---\n",
    "!pip -q install cloudscraper httpx > /dev/null\n",
    "\n",
    "\n",
    "# %%\n",
    "import re\n",
    "import regex as reg\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Tuple, Callable, Dict\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Optional imports (fallback parsers)\n",
    "try:\n",
    "    from readability import Document\n",
    "    HAS_READABILITY = True\n",
    "except Exception:\n",
    "    HAS_READABILITY = False\n",
    "\n",
    "try:\n",
    "    import trafilatura\n",
    "    HAS_TRAFILATURA = True\n",
    "except Exception:\n",
    "    HAS_TRAFILATURA = False\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    ")\n",
    "logger = logging.getLogger(\"news-scraper\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alOHs_ZY9xBp"
   },
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 451,
     "status": "ok",
     "timestamp": 1758706392077,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "LbKEmBgO9rBU"
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "# ======== PARAMETER YANG MUDAH DIUBAH ========\n",
    "\n",
    "# config path\n",
    "from pathlib import Path\n",
    "\n",
    "# CONFIG_PATH = Path(\"/content/drive/MyDrive/Monitoring Berita/config.json\")\n",
    "\n",
    "CONFIG_PATH = Path(f\"{cwd}/config.json\")\n",
    "\n",
    "with open(CONFIG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            cfg = json.load(f)\n",
    "\n",
    "# INPUT_EXCEL_PATH = cfg[\"labelled_data_xlsx\"]\n",
    "INPUT_CSV_PATH = cfg[\"last_konsolidasi_path\"]\n",
    "INPUT_URL_COLUMN = \"url_berita\"\n",
    "\n",
    "# Batasi jumlah URL saat uji (None untuk semua)\n",
    "MAX_URLS: Optional[int] = None\n",
    "\n",
    "# Timeout & retry (per attempt)\n",
    "REQUEST_TIMEOUT = 8  # dikurangi agar tidak memakan seluruh budget 10 detik\n",
    "MAX_RETRIES = 2\n",
    "RETRY_SLEEP = 1.0\n",
    "\n",
    "# Total batas waktu proses ekstraksi per URL (detik)\n",
    "PER_ARTICLE_TIMEOUT = 10\n",
    "\n",
    "# User-Agent khusus agar tidak diblok mudah\n",
    "DEFAULT_HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
    "        \"(KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# Pola frasa yang ingin dihapus dari artikel (iklan/clickbait/relate links)\n",
    "CLEANUP_PATTERNS = [\n",
    "    r\"\\bBaca juga\\b.*\", r\"\\bSimak juga\\b.*\", r\"\\bADVERTISEMENT\\b.*\",\n",
    "    r\"\\bIklan\\b.*\", r\"\\bIklan Layanan\\b.*\", r\"\\bInfografis\\b.*\",\n",
    "    r\"\\bTonton juga\\b.*\", r\"\\bVideo:\\b.*\", r\"\\b[Gg]rafis\\b.*\",\n",
    "    r\"\\b[Gg]allery\\b.*\", r\"\\bArtikel ini telah\\b.*\", r\"\\bEditor:\\b.*$\",\n",
    "    r\"^\\s*—\\s*$\", r\"^\\s*-\\s*$\", r\"\\b[ \\t]*[•\\-\\*]\\s*$\"\n",
    "]\n",
    "\n",
    "# Tag/kelas yang menandai elemen non-konten yang harus dibuang\n",
    "JUNK_SELECTORS = [\n",
    "    \"[class*=iklan]\", \"[class*=ads]\", \"[id*=ads]\", \"[id*=banner]\", \"[class*=related]\",\n",
    "    \"[class*=tag]\", \"[class*=breadcrumb]\", \"script\", \"style\", \"noscript\", \"iframe\",\n",
    "    \"[class*=share]\", \"[class*=social]\", \"[class*=recommend]\", \"[class*=promo]\",\n",
    "    \"[class*=copyright]\", \"[class*=author]\", \"[class*=metadata]\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weVk6VCw9y3X"
   },
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1758706392090,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "MxFtkgnZ9yck"
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "session = requests.Session()\n",
    "session.headers.update(DEFAULT_HEADERS)\n",
    "\n",
    "def fetch(url: str, timeout: int = REQUEST_TIMEOUT, max_retries: int = MAX_RETRIES) -> Optional[requests.Response]:\n",
    "    \"\"\"\n",
    "    Fetch URL dengan retry sederhana dan timeout.\n",
    "    \"\"\"\n",
    "    last_err = None\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            resp = session.get(url, timeout=timeout, allow_redirects=True)\n",
    "            if 200 <= resp.status_code < 300:\n",
    "                return resp\n",
    "            else:\n",
    "                last_err = RuntimeError(f\"HTTP {resp.status_code}\")\n",
    "                logger.warning(f\"Attempt {attempt}: {url} -> {resp.status_code}\")\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            logger.warning(f\"Attempt {attempt} failed for {url}: {e}\")\n",
    "        time.sleep(RETRY_SLEEP * attempt)\n",
    "    logger.error(f\"Failed to fetch {url}: {last_err}\")\n",
    "    return None\n",
    "\n",
    "def normalize_whitespace(text: str) -> str:\n",
    "    text = reg.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = reg.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def remove_cleanup_patterns(text: str, patterns: List[str]) -> str:\n",
    "    out = text\n",
    "    for pat in patterns:\n",
    "        out = re.sub(pat, \"\", out, flags=re.IGNORECASE)\n",
    "    # Buang baris kosong sisa\n",
    "    out = \"\\n\".join([ln.strip() for ln in out.splitlines() if ln.strip()])\n",
    "    return normalize_whitespace(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1758706392113,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "ApSGHWC6C0rc"
   },
   "outputs": [],
   "source": [
    "# --- Sel 3 (ganti sesi & fetch) ---\n",
    "import random\n",
    "import httpx\n",
    "import cloudscraper\n",
    "from urllib.parse import urlparse\n",
    "import time as _time\n",
    "\n",
    "# Headers \"benar-bener browser\" (ditambah beberapa header network hints)\n",
    "BROWSER_HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "        \"(KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36\"\n",
    "    ),\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"id-ID,id;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "    \"Cache-Control\": \"no-cache\",\n",
    "    \"Pragma\": \"no-cache\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    \"Sec-Fetch-Dest\": \"document\",\n",
    "    \"Sec-Fetch-Mode\": \"navigate\",\n",
    "    \"Sec-Fetch-Site\": \"none\",\n",
    "    \"Sec-Fetch-User\": \"?1\",\n",
    "}\n",
    "\n",
    "DETIK_EXTRA_HEADERS = {\n",
    "    \"Authority\": \"www.detik.com\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "}\n",
    "\n",
    "UA_POOL = [\n",
    "    BROWSER_HEADERS[\"User-Agent\"],\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64; rv:130.0) Gecko/20100101 Firefox/130.0\",\n",
    "]\n",
    "\n",
    "\n",
    "def make_session():\n",
    "    # cloudscraper biasanya lolos 403 di situs yang pakai anti-bot\n",
    "    try:\n",
    "        s = cloudscraper.create_scraper(\n",
    "            browser={\"browser\": \"chrome\", \"platform\": \"windows\", \"mobile\": False}\n",
    "        )\n",
    "        s.headers.update(BROWSER_HEADERS)\n",
    "        return s\n",
    "    except Exception:\n",
    "        s = requests.Session()\n",
    "        s.headers.update(BROWSER_HEADERS)\n",
    "        return s\n",
    "\n",
    "session = make_session()\n",
    "\n",
    "\n",
    "def jitter_sleep(base=REQUEST_TIMEOUT * 0.05):\n",
    "    _time.sleep(base + random.random() * 0.5)\n",
    "\n",
    "\n",
    "def as_amp(url: str) -> str:\n",
    "    u = url.split(\"?\")[0].rstrip(\"/\")\n",
    "    return u + \"/amp\"\n",
    "\n",
    "\n",
    "def _attempt_requests(url, headers, timeout):\n",
    "    return session.get(url, headers=headers, timeout=timeout, allow_redirects=True)\n",
    "\n",
    "\n",
    "def _attempt_httpx(url, headers, timeout):\n",
    "    try:\n",
    "        with httpx.Client(follow_redirects=True, timeout=timeout, headers=headers, http2=True) as client:\n",
    "            return client.get(url)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def fetch(url: str, timeout: int = REQUEST_TIMEOUT, max_retries: int = MAX_RETRIES, deadline: float = None) -> Optional[requests.Response]:\n",
    "    \"\"\"\n",
    "    Fetch dengan retry + header lengkap dan menghormati *deadline* absolut (epoch detik).\n",
    "    Jika waktu sekarang melewati deadline sebelum attempt berikutnya, hentikan lebih awal.\n",
    "    \"\"\"\n",
    "    last_err = None\n",
    "    parsed = urlparse(url)\n",
    "    domain = parsed.netloc.lower()\n",
    "    is_tribun = \"tribunnews.com\" in domain\n",
    "    is_detik = \"detik.com\" in domain\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        if deadline and _time.time() > deadline:\n",
    "            last_err = TimeoutError(\"global deadline exceeded before attempt\")\n",
    "            break\n",
    "        try:\n",
    "            headers = BROWSER_HEADERS.copy()\n",
    "            headers[\"User-Agent\"] = random.choice(UA_POOL)\n",
    "            headers[\"Referer\"] = \"https://www.google.com/\"\n",
    "            if is_detik:\n",
    "                headers.update(DETIK_EXTRA_HEADERS)\n",
    "\n",
    "            per_attempt_timeout = min(timeout, max(2, int((deadline - _time.time()) if deadline else timeout))) if deadline else timeout\n",
    "            resp = _attempt_requests(url, headers=headers, timeout=per_attempt_timeout)\n",
    "            if resp is not None and 200 <= resp.status_code < 300:\n",
    "                return resp\n",
    "\n",
    "            status = resp.status_code if resp is not None else \"NO_RESP\"\n",
    "\n",
    "            if is_detik and (resp is None or status in (403, 503)) and (not deadline or _time.time() < deadline):\n",
    "                per_attempt_timeout = min(per_attempt_timeout, 5)\n",
    "                resp_hx = _attempt_httpx(url, headers=headers, timeout=per_attempt_timeout)\n",
    "                if resp_hx is not None and 200 <= resp_hx.status_code < 300:\n",
    "                    class SimpleResp:\n",
    "                        def __init__(self, r):\n",
    "                            self.text = r.text\n",
    "                            self.status_code = r.status_code\n",
    "                            self.url = str(r.url)\n",
    "                            self.headers = dict(r.headers)\n",
    "                    return SimpleResp(resp_hx)  # type: ignore\n",
    "\n",
    "            if is_tribun and resp is not None and resp.status_code in (401, 403) and (not deadline or _time.time() < deadline):\n",
    "                amp_url = as_amp(url)\n",
    "                try:\n",
    "                    per_attempt_timeout = min(per_attempt_timeout, 5)\n",
    "                    resp2 = _attempt_requests(amp_url, headers=headers, timeout=per_attempt_timeout)\n",
    "                    if resp2 is not None and 200 <= resp2.status_code < 300:\n",
    "                        resp2.url = url\n",
    "                        return resp2\n",
    "                    else:\n",
    "                        last_err = RuntimeError(f\"AMP HTTP {resp2.status_code if resp2 else 'none'}\")\n",
    "                except Exception as e2:\n",
    "                    last_err = e2\n",
    "            else:\n",
    "                last_err = RuntimeError(f\"HTTP {status}\")\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            logger.warning(f\"Attempt {attempt} failed for {url}: {e}\")\n",
    "\n",
    "        if attempt < max_retries and (not deadline or _time.time() + 0.5 < deadline):\n",
    "            jitter_sleep(0.4 + attempt * 0.2)\n",
    "\n",
    "    if last_err:\n",
    "        logger.error(f\"Failed to fetch {url}: {last_err}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHSDUrPB94_z"
   },
   "source": [
    "## 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1758706392177,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "V0Oasond9132"
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "CANDIDATE_CONTENT_SELECTORS = [\n",
    "    \"article\",\n",
    "    \"[class*=content]\",\n",
    "    \"[class*=article]\",\n",
    "    \"[class*=read__content]\",\n",
    "    \"[class*=detail__body]\",\n",
    "    \"[class*=entry-content]\",\n",
    "    \"[id*=content]\", \"[id*=article]\",\n",
    "    \"[itemprop='articleBody']\",\n",
    "]\n",
    "\n",
    "def clean_soup(soup: BeautifulSoup) -> None:\n",
    "    # Hapus elemen junk\n",
    "    for sel in JUNK_SELECTORS:\n",
    "        for el in soup.select(sel):\n",
    "            el.decompose()\n",
    "\n",
    "def join_block_text(container: Tag) -> str:\n",
    "    # Ambil <p>, <h2/3/4>, <li> sebagai paragraf/kalimat\n",
    "    parts: List[str] = []\n",
    "    for el in container.descendants:\n",
    "        if isinstance(el, Tag):\n",
    "            if el.name in {\"p\", \"h2\", \"h3\", \"h4\", \"li\"}:\n",
    "                txt = el.get_text(separator=\" \", strip=True)\n",
    "                if txt and len(txt) > 2:\n",
    "                    parts.append(txt)\n",
    "        elif isinstance(el, NavigableString):\n",
    "            # Abaikan NavigableString langsung agar tidak duplikat\n",
    "            pass\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "def generic_extract(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    clean_soup(soup)\n",
    "\n",
    "    # 1) cari tag <article> atau kandidat body konten\n",
    "    for sel in CANDIDATE_CONTENT_SELECTORS:\n",
    "        nodes = soup.select(sel)\n",
    "        for node in nodes:\n",
    "            text = join_block_text(node)\n",
    "            if text and len(text) > 300:  # ambang minimal isi artikel\n",
    "                return text\n",
    "\n",
    "    # 2) fallback: ambil konten terpanjang dari beberapa kandidat\n",
    "    candidates = []\n",
    "    for node in soup.find_all([\"article\", \"div\", \"section\"], limit=50):\n",
    "        text = join_block_text(node)\n",
    "        if text:\n",
    "            candidates.append((len(text), text))\n",
    "    if candidates:\n",
    "        candidates.sort(reverse=True, key=lambda x: x[0])\n",
    "        return candidates[0][1]\n",
    "\n",
    "    # 3) fallback terakhir: keseluruhan dokumen (p, h2, li)\n",
    "    body = soup.body or soup\n",
    "    text = join_block_text(body)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MjvftAI97p5"
   },
   "source": [
    "## 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1758706392221,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "M7QzWB2E96C_"
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "# Improved detik.com extraction + existing other extractors\n",
    "\n",
    "def extract_detik(soup: BeautifulSoup) -> Optional[str]:\n",
    "    \"\"\"Ekstraktor khusus detik.com dengan beberapa lapis fallback.\n",
    "\n",
    "    Strategi:\n",
    "    1. Hapus elemen iklan/junk (clean_soup)\n",
    "    2. Coba kumpulan selector spesifik (kelas bisa berubah urutannya)\n",
    "    3. Fallback gabungan semua <div> kandidat yang mengandung paragraf panjang\n",
    "    4. Fallback cari skrip __NEXT_DATA__ (jika arsitektur Next.js) dan ambil field konten\n",
    "    5. Fallback semua <p> dalam area utama dokumen\n",
    "    \"\"\"\n",
    "    clean_soup(soup)\n",
    "\n",
    "    candidate_selectors = [\n",
    "        \"article.detail__article\",\n",
    "        \"div.detail__body-text.itp_bodycontent\",\n",
    "        \"div.detail__body-text\",\n",
    "        \"div.itp_bodycontent\",\n",
    "        \"div.detail__body\",  # kadang dipakai\n",
    "        \"[class*=detail__body]\",\n",
    "        \"[class*=itp_bodycontent]\",\n",
    "        \"div#detikdetailtext\",\n",
    "    ]\n",
    "\n",
    "    for sel in candidate_selectors:\n",
    "        for node in soup.select(sel):\n",
    "            txt = join_block_text(node)\n",
    "            if txt and len(txt) > 200:\n",
    "                return txt\n",
    "\n",
    "    # Fallback: kumpulkan paragraf dari beberapa kandidat container besar\n",
    "    containers = soup.select(\"article, div#detikdetailtext, div.detail__body-text, div.itp_bodycontent\")\n",
    "    gathered: List[str] = []\n",
    "    for c in containers:\n",
    "        t = join_block_text(c)\n",
    "        if t and len(t) > 40:\n",
    "            gathered.append(t)\n",
    "    if gathered:\n",
    "        merged = \"\\n\".join(gathered)\n",
    "        if len(merged) > 200:\n",
    "            return merged\n",
    "\n",
    "    # Fallback: parsing __NEXT_DATA__ jika ada (arsitektur JS modern)\n",
    "    next_data = soup.find(\"script\", id=\"__NEXT_DATA__\")\n",
    "    if next_data and next_data.string:\n",
    "        try:\n",
    "            data = json.loads(next_data.string)\n",
    "            # Cari secara rekursif field bernuansa body\n",
    "            def walk(o):\n",
    "                if isinstance(o, dict):\n",
    "                    for k, v in o.items():\n",
    "                        kl = k.lower()\n",
    "                        if kl in {\"content\", \"body\", \"articlebody\", \"article_body\"} and isinstance(v, (str, list)):\n",
    "                            yield v\n",
    "                        else:\n",
    "                            yield from walk(v)\n",
    "                elif isinstance(o, list):\n",
    "                    for it in o:\n",
    "                        yield from walk(it)\n",
    "            texts = []\n",
    "            for val in walk(data):\n",
    "                if isinstance(val, str) and len(val) > 100:\n",
    "                    # Hilangkan tag HTML dasar jika ada\n",
    "                    cleaned = BeautifulSoup(val, \"lxml\").get_text(\" \", strip=True)\n",
    "                    texts.append(cleaned)\n",
    "                elif isinstance(val, list):\n",
    "                    joined = \" \".join([str(x) for x in val])\n",
    "                    if len(joined) > 100:\n",
    "                        texts.append(joined)\n",
    "            if texts:\n",
    "                uniq = []\n",
    "                seen = set()\n",
    "                for t in texts:\n",
    "                    if t not in seen:\n",
    "                        uniq.append(t)\n",
    "                        seen.add(t)\n",
    "                merged = \"\\n\".join(uniq)\n",
    "                if len(merged) > 200:\n",
    "                    return merged\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Fallback akhir: semua paragraf yang panjang di dokumen\n",
    "    paragraphs = [p.get_text(\" \", strip=True) for p in soup.find_all(\"p\")]\n",
    "    paragraphs = [p for p in paragraphs if len(p) > 40 and not p.lower().startswith(\"advertorial\")]  # buang advertorial\n",
    "    if paragraphs:\n",
    "        merged = \"\\n\".join(paragraphs)\n",
    "        if len(merged) > 200:\n",
    "            return merged\n",
    "    return None\n",
    "\n",
    "def extract_cnbcindo(soup: BeautifulSoup) -> Optional[str]:\n",
    "    clean_soup(soup)\n",
    "    for sel in [\n",
    "        \"article\",\n",
    "        \"div.detail_text\",\n",
    "        \"[class*=detail__body]\"\n",
    "    ]:\n",
    "        nodes = soup.select(sel)\n",
    "        for n in nodes:\n",
    "            txt = join_block_text(n)\n",
    "            if txt and len(txt) > 200:\n",
    "                return txt\n",
    "    return None\n",
    "\n",
    "def extract_kompas(soup: BeautifulSoup) -> Optional[str]:\n",
    "    clean_soup(soup)\n",
    "    for sel in [\n",
    "        \"div.read__content\",\n",
    "        \"article\",\n",
    "        \"[class*=read__content]\",\n",
    "    ]:\n",
    "        nodes = soup.select(sel)\n",
    "        for n in nodes:\n",
    "            txt = join_block_text(n)\n",
    "            if txt and len(txt) > 200:\n",
    "                return txt\n",
    "    return None\n",
    "\n",
    "def extract_liputan6(soup: BeautifulSoup) -> Optional[str]:\n",
    "    clean_soup(soup)\n",
    "    for sel in [\n",
    "        \"div.article-content-body__item-content\",  # paragraf-paragraf\n",
    "        \"div.article-content-body\",\n",
    "        \"article\"\n",
    "    ]:\n",
    "        nodes = soup.select(sel)\n",
    "        if nodes:\n",
    "            # gabungkan semua paragraf dari beberapa item-content\n",
    "            text_parts = []\n",
    "            for n in nodes:\n",
    "                t = join_block_text(n)\n",
    "                if t:\n",
    "                    text_parts.append(t)\n",
    "            txt = \"\\n\".join(text_parts)\n",
    "            if txt and len(txt) > 200:\n",
    "                return txt\n",
    "    return None\n",
    "\n",
    "def extract_tribun(soup: BeautifulSoup) -> Optional[str]:\n",
    "    clean_soup(soup)\n",
    "    for sel in [\n",
    "        \"div.side-article txt-article\",\n",
    "        \"div.txt-article\",\n",
    "        \"div#articlebody\",\n",
    "        \"div#article\"\n",
    "    ]:\n",
    "        nodes = soup.select(sel)\n",
    "        for n in nodes:\n",
    "            txt = join_block_text(n)\n",
    "            if txt and len(txt) > 200:\n",
    "                return txt\n",
    "    # Lainnya:\n",
    "    nodes = soup.select(\"div > p\")\n",
    "    if nodes:\n",
    "        txt = \"\\n\".join([n.get_text(\" \", strip=True) for n in nodes])\n",
    "        if len(txt) > 200:\n",
    "            return txt\n",
    "    return None\n",
    "\n",
    "def extract_merdeka(soup: BeautifulSoup) -> Optional[str]:\n",
    "    clean_soup(soup)\n",
    "    for sel in [\n",
    "        \"div.article-content\",\n",
    "        \"div.kanal-content\",\n",
    "        \"article\"\n",
    "    ]:\n",
    "        nodes = soup.select(sel)\n",
    "        for n in nodes:\n",
    "            txt = join_block_text(n)\n",
    "            if txt and len(txt) > 200:\n",
    "                return txt\n",
    "    return None\n",
    "\n",
    "def extract_antaranews(soup: BeautifulSoup) -> Optional[str]:\n",
    "    clean_soup(soup)\n",
    "    for sel in [\n",
    "        \"div.post-content\",\n",
    "        \"div#content\",\n",
    "        \"article\"\n",
    "    ]:\n",
    "        nodes = soup.select(sel)\n",
    "        for n in nodes:\n",
    "            txt = join_block_text(n)\n",
    "            if txt and len(txt) > 200:\n",
    "                return txt\n",
    "    return None\n",
    "\n",
    "DOMAIN_EXTRACTORS: Dict[str, Callable[[BeautifulSoup], Optional[str]]] = {\n",
    "    \"detik.com\": extract_detik,\n",
    "    \"cnbcindonesia.com\": extract_cnbcindo,\n",
    "    \"kompas.com\": extract_kompas,\n",
    "    \"liputan6.com\": extract_liputan6,\n",
    "    \"tribunnews.com\": extract_tribun,\n",
    "    \"merdeka.com\": extract_merdeka,\n",
    "    \"antaranews.com\": extract_antaranews,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1758706392222,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "6y8-wmqODCIF"
   },
   "outputs": [],
   "source": [
    "def extract_tribun(soup: BeautifulSoup) -> Optional[str]:\n",
    "    clean_soup(soup)\n",
    "    # Versi desktop lama/baru\n",
    "    for sel in [\n",
    "        \"div.txt-article\", \"div#article\",\n",
    "        \"div#articlebody\", \"div.side-article.txt-article\",\n",
    "        \"article\", \"[class*=read__content]\", \"[class*=detail__body]\"\n",
    "    ]:\n",
    "        for n in soup.select(sel):\n",
    "            txt = join_block_text(n)\n",
    "            if txt and len(txt) > 200:\n",
    "                return txt\n",
    "    # fallback p\n",
    "    ps = soup.select(\"article p, div p\")\n",
    "    if ps:\n",
    "        txt = \"\\n\".join([p.get_text(\" \", strip=True) for p in ps])\n",
    "        if len(txt) > 200:\n",
    "            return txt\n",
    "    return None\n",
    "\n",
    "def extract_tribun_amp(soup: BeautifulSoup) -> Optional[str]:\n",
    "    clean_soup(soup)\n",
    "    # AMP biasanya lebih bersih\n",
    "    for sel in [\n",
    "        \"div.read__content\", \"div.read__content--body\",\n",
    "        \"article\", \"[itemprop='articleBody']\"\n",
    "    ]:\n",
    "        for n in soup.select(sel):\n",
    "            txt = join_block_text(n)\n",
    "            if txt and len(txt) > 200:\n",
    "                return txt\n",
    "    ps = soup.select(\"article p, div p\")\n",
    "    if ps:\n",
    "        txt = \"\\n\".join([p.get_text(' ', strip=True) for p in ps])\n",
    "        if len(txt) > 200:\n",
    "            return txt\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1758706392223,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "d-f5WdZjDNZK"
   },
   "outputs": [],
   "source": [
    "DOMAIN_EXTRACTORS.update({\n",
    "    \"tribunnews.com\": extract_tribun,  # desktop\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5v-1yumQ98-P"
   },
   "source": [
    "## 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1758706392224,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "3xTLyjnF9-AI"
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "@dataclass\n",
    "class ExtractResult:\n",
    "    url: str\n",
    "    domain: str\n",
    "    text: Optional[str]\n",
    "    status: str\n",
    "    error: Optional[str]\n",
    "\n",
    "def extract_with_layers(url: str, per_article_timeout: int = PER_ARTICLE_TIMEOUT) -> ExtractResult:\n",
    "    \"\"\"\n",
    "    Layered extraction dengan batas waktu total per URL.\n",
    "    Deadline global dikontrol via per_article_timeout (detik).\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    deadline = start + per_article_timeout\n",
    "\n",
    "    parsed = urlparse(url)\n",
    "    domain = parsed.netloc.lower()\n",
    "\n",
    "    def remaining() -> float:\n",
    "        return max(0, deadline - time.time())\n",
    "\n",
    "    # 1) Fetch HTML (menghormati deadline)\n",
    "    resp = fetch(url, timeout=REQUEST_TIMEOUT, max_retries=MAX_RETRIES, deadline=deadline)\n",
    "    if resp is None:\n",
    "        if time.time() > deadline:\n",
    "            return ExtractResult(url, domain, None, \"timeout_fetch\", \"Exceeded per-article timeout during fetch\")\n",
    "        return ExtractResult(url, domain, None, \"fetch_failed\", \"Request failed\")\n",
    "\n",
    "    html = resp.text\n",
    "\n",
    "    # 2) Site-specific\n",
    "    try:\n",
    "        if time.time() > deadline:\n",
    "            return ExtractResult(url, domain, None, \"timeout_before_site_specific\", None)\n",
    "        for known in DOMAIN_EXTRACTORS:\n",
    "            if known in domain:\n",
    "                soup = BeautifulSoup(html, \"lxml\")\n",
    "                text = DOMAIN_EXTRACTORS[known](soup)\n",
    "                if time.time() > deadline:\n",
    "                    return ExtractResult(url, domain, None, \"timeout_site_specific\", None)\n",
    "                if text and len(text) > 150:\n",
    "                    text = remove_cleanup_patterns(text, CLEANUP_PATTERNS)\n",
    "                    return ExtractResult(url, domain, text, \"ok_site_specific\", None)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Site-specific extractor error for {url}: {e}\")\n",
    "\n",
    "    # 3) Tribun AMP khusus\n",
    "    if \"tribunnews.com\" in domain and time.time() < deadline:\n",
    "        try:\n",
    "            amp_html = None\n",
    "            if \"/amp\" in resp.url or \"amp\" in (resp.headers.get(\"content-location\", \"\") or \"\").lower():\n",
    "                amp_html = html\n",
    "            else:\n",
    "                rem = remaining()\n",
    "                if rem > 1.5:  # masih layak coba\n",
    "                    amp_resp = fetch(as_amp(url), timeout=min(REQUEST_TIMEOUT, int(rem)), max_retries=1, deadline=deadline)\n",
    "                    if amp_resp is not None and 200 <= amp_resp.status_code < 300:\n",
    "                        amp_html = amp_resp.text\n",
    "            if amp_html and time.time() < deadline:\n",
    "                soup_amp = BeautifulSoup(amp_html, \"lxml\")\n",
    "                text_amp = extract_tribun_amp(soup_amp)\n",
    "                if time.time() > deadline:\n",
    "                    return ExtractResult(url, domain, None, \"timeout_tribun_amp\", None)\n",
    "                if text_amp and len(text_amp) > 150:\n",
    "                    text_amp = remove_cleanup_patterns(text_amp, CLEANUP_PATTERNS)\n",
    "                    return ExtractResult(url, domain, text_amp, \"ok_tribun_amp\", None)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Tribun AMP extractor error for {url}: {e}\")\n",
    "\n",
    "    # 4) Trafilatura\n",
    "    if HAS_TRAFILATURA and time.time() < deadline:\n",
    "        try:\n",
    "            rem = remaining()\n",
    "            if rem > 1.5:\n",
    "                downloaded = None\n",
    "                try:\n",
    "                    import inspect\n",
    "                    if \"timeout\" in inspect.signature(trafilatura.fetch_url).parameters:\n",
    "                        downloaded = trafilatura.fetch_url(url, timeout=min(REQUEST_TIMEOUT, int(rem)))\n",
    "                    else:\n",
    "                        downloaded = trafilatura.fetch_url(url)\n",
    "                except Exception:\n",
    "                    downloaded = trafilatura.fetch_url(url)\n",
    "                if downloaded and time.time() < deadline:\n",
    "                    t_text = trafilatura.extract(\n",
    "                        downloaded,\n",
    "                        include_comments=False,\n",
    "                        include_tables=False,\n",
    "                        favor_recall=True,\n",
    "                    )\n",
    "                    if time.time() > deadline:\n",
    "                        return ExtractResult(url, domain, None, \"timeout_trafilatura\", None)\n",
    "                    if t_text and len(t_text) > 150:\n",
    "                        t_text = remove_cleanup_patterns(t_text, CLEANUP_PATTERNS)\n",
    "                        return ExtractResult(url, domain, t_text, \"ok_trafilatura\", None)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Trafilatura extractor error for {url}: {e}\")\n",
    "\n",
    "    # 5) Readability\n",
    "    if HAS_READABILITY and time.time() < deadline:\n",
    "        try:\n",
    "            rem = remaining()\n",
    "            if rem > 1.0:\n",
    "                doc = Document(html)\n",
    "                readable_html = doc.summary(html_partial=True)\n",
    "                soup = BeautifulSoup(readable_html, \"lxml\")\n",
    "                clean_soup(soup)\n",
    "                r_text = join_block_text(soup)\n",
    "                if time.time() > deadline:\n",
    "                    return ExtractResult(url, domain, None, \"timeout_readability\", None)\n",
    "                if r_text and len(r_text) > 150:\n",
    "                    r_text = remove_cleanup_patterns(r_text, CLEANUP_PATTERNS)\n",
    "                    return ExtractResult(url, domain, r_text, \"ok_readability\", None)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Readability extractor error for {url}: {e}\")\n",
    "\n",
    "    # 6) Generic heuristic\n",
    "    if time.time() < deadline:\n",
    "        try:\n",
    "            g_text = generic_extract(html)\n",
    "            if time.time() > deadline:\n",
    "                return ExtractResult(url, domain, None, \"timeout_generic\", None)\n",
    "            g_text = remove_cleanup_patterns(g_text, CLEANUP_PATTERNS)\n",
    "            if g_text and len(g_text) > 100:\n",
    "                return ExtractResult(url, domain, g_text, \"ok_generic\", None)\n",
    "            else:\n",
    "                return ExtractResult(url, domain, None, \"empty_after_generic\", None)\n",
    "        except Exception as e:\n",
    "            if time.time() > deadline:\n",
    "                return ExtractResult(url, domain, None, \"timeout_generic_exception\", str(e))\n",
    "            return ExtractResult(url, domain, None, \"generic_failed\", str(e))\n",
    "\n",
    "    # Jika semua tahap habis waktu\n",
    "    return ExtractResult(url, domain, None, \"timeout_no_content\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s10M5kbW-DNc"
   },
   "source": [
    "## 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315,
     "referenced_widgets": [
      "220218e84e7c4b5a988c5e53254cfbe9",
      "504fdeb14baf42ea8a18d6cc32baef23",
      "f3544fe90d974bfc945d30ac5e432956",
      "fc8314b543284cfda1090b53a276c097",
      "45135c05354c4242b7fd198658454bb2",
      "7eab9712b4da4180b083c35b4bcdac6c",
      "46667759861a4475a5069cb4b6847c4e",
      "b10cecf9ab3c4b9aa47e9720522de598",
      "4a3447c430df4a1ba7428d92a5bbafea",
      "af2282eee0f248e39d69f6a931f8a861",
      "584b6c4ef81a4eb8bc902c112d7fe4fd"
     ]
    },
    "executionInfo": {
     "elapsed": 73434,
     "status": "ok",
     "timestamp": 1758706465658,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "rV5jB-jc-BMU",
    "outputId": "22c850c2-8268-4d70-b1aa-2536a113c2c9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-01 20:41:56,845 | INFO | Total URL untuk diproses: 46 (limit=None)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ffe118a71f46dfb2a8d7eb0970c095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scraping artikel:   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-01 20:42:02,580 | ERROR | not a 200 response: 502 for URL https://kumparan.com/kumparanbisnis/purbaya-diminta-gempur-rokok-ilegal-bea-cukai-kejar-sampai-ke-e-commerce-25wTqv6LRis\n",
      "2025-10-01 20:42:07,329 | ERROR | not a 200 response: 502 for URL https://kumparan.com/kumparanbisnis/ketua-komisi-xi-dpr-minta-purbaya-gempur-praktik-rokok-ilegal-25wOoSLlaNg\n",
      "2025-10-01 20:42:11,431 | ERROR | not a 200 response: 403 for URL https://kaltimpost.jawapos.com/nasional/2386640196/purbaya-tegas-larang-rokok-ilegal-mulai-1-oktober-2025-warung-hingga-tokopedia-wajib-patuh\n",
      "2025-10-01 20:42:11,666 | ERROR | not a 200 response: 502 for URL https://kumparan.com/kumparanbisnis/purbaya-pastikan-sidak-rokok-impor-ilegal-tak-ganggu-operasional-25vaWNIlzKQ\n",
      "2025-10-01 20:42:35,345 | WARNING | Attempt 1 failed for https://rri.co.id/daerah/1874681/sebanyak-3-6-juta-batang-rokok-ilegal-kembali-diamankan: HTTPSConnectionPool(host='rri.co.id', port=443): Read timed out. (read timeout=8)\n",
      "2025-10-01 20:42:55,904 | ERROR | not a 200 response: 404 for URL https://rri.co.id/info-pemda/1865499/pedagang-diedukasi-rokok-ilegal-diamankan\n",
      "2025-10-01 20:43:03,577 | INFO | \n",
      "Summary status:\n",
      "status\n",
      "ok_trafilatura         34\n",
      "ok_site_specific        6\n",
      "empty_after_generic     3\n",
      "ok_readability          1\n",
      "ok_generic              1\n",
      "timeout_no_content      1\n",
      "Name: count, dtype: int64\n",
      "2025-10-01 20:43:03,578 | INFO | Timeout sample rows: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>judul_berita</th>\n",
       "      <th>url_berita</th>\n",
       "      <th>tanggal_berita</th>\n",
       "      <th>source_domain</th>\n",
       "      <th>artikel_berita</th>\n",
       "      <th>status</th>\n",
       "      <th>error</th>\n",
       "      <th>is_empty</th>\n",
       "      <th>elapsed_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Purbaya Bakal Sikat Rokok Ilegal, Seberapa Par...</td>\n",
       "      <td>https://ekbis.sindonews.com/read/1626051/34/pu...</td>\n",
       "      <td>2025-09-28T16:00:00+07:00</td>\n",
       "      <td>ekbis.sindonews.com</td>\n",
       "      <td>Purbaya Bakal Sikat Rokok Ilegal, Seberapa Par...</td>\n",
       "      <td>ok_trafilatura</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kiai Jatim Dukung Menkeu Purbaya Berantas Roko...</td>\n",
       "      <td>https://surabaya.kompas.com/read/2025/10/01/11...</td>\n",
       "      <td>2025-10-01T11:13:00+07:00</td>\n",
       "      <td>surabaya.kompas.com</td>\n",
       "      <td>TUBAN, KOMPAS.com - Pengasuh Pondok Pesantren ...</td>\n",
       "      <td>ok_site_specific</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rencana Menkeu Purbaya Buat Para Penjual Rokok...</td>\n",
       "      <td>https://pasardana.id/news/2025/9/29/rencana-me...</td>\n",
       "      <td>2025-09-29T00:19:00+07:00</td>\n",
       "      <td>pasardana.id</td>\n",
       "      <td>Rencana Menkeu Purbaya Buat Para Penjual Rokok...</td>\n",
       "      <td>ok_trafilatura</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Penindakan Rokok Ilegal Bakal Semakin Diperket...</td>\n",
       "      <td>https://www.pajak.com/pajak/penindakan-rokok-i...</td>\n",
       "      <td>2025-09-29T14:00:01+07:00</td>\n",
       "      <td>www.pajak.com</td>\n",
       "      <td>Penindakan Rokok Ilegal Bakal Semakin Diperket...</td>\n",
       "      <td>ok_trafilatura</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lawan Produk Ilegal, Menkeu Purbaya Tak Naikka...</td>\n",
       "      <td>https://www.metrotvnews.com/play/KYVC4EaR-lawa...</td>\n",
       "      <td>2025-09-30T20:35:45+07:00</td>\n",
       "      <td>www.metrotvnews.com</td>\n",
       "      <td>30 September 2025 20:35\\nMenteri Keuangan (Men...</td>\n",
       "      <td>ok_trafilatura</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        judul_berita  \\\n",
       "0  Purbaya Bakal Sikat Rokok Ilegal, Seberapa Par...   \n",
       "1  Kiai Jatim Dukung Menkeu Purbaya Berantas Roko...   \n",
       "2  Rencana Menkeu Purbaya Buat Para Penjual Rokok...   \n",
       "3  Penindakan Rokok Ilegal Bakal Semakin Diperket...   \n",
       "4  Lawan Produk Ilegal, Menkeu Purbaya Tak Naikka...   \n",
       "\n",
       "                                          url_berita  \\\n",
       "0  https://ekbis.sindonews.com/read/1626051/34/pu...   \n",
       "1  https://surabaya.kompas.com/read/2025/10/01/11...   \n",
       "2  https://pasardana.id/news/2025/9/29/rencana-me...   \n",
       "3  https://www.pajak.com/pajak/penindakan-rokok-i...   \n",
       "4  https://www.metrotvnews.com/play/KYVC4EaR-lawa...   \n",
       "\n",
       "              tanggal_berita        source_domain  \\\n",
       "0  2025-09-28T16:00:00+07:00  ekbis.sindonews.com   \n",
       "1  2025-10-01T11:13:00+07:00  surabaya.kompas.com   \n",
       "2  2025-09-29T00:19:00+07:00         pasardana.id   \n",
       "3  2025-09-29T14:00:01+07:00        www.pajak.com   \n",
       "4  2025-09-30T20:35:45+07:00  www.metrotvnews.com   \n",
       "\n",
       "                                      artikel_berita            status error  \\\n",
       "0  Purbaya Bakal Sikat Rokok Ilegal, Seberapa Par...    ok_trafilatura         \n",
       "1  TUBAN, KOMPAS.com - Pengasuh Pondok Pesantren ...  ok_site_specific         \n",
       "2  Rencana Menkeu Purbaya Buat Para Penjual Rokok...    ok_trafilatura         \n",
       "3  Penindakan Rokok Ilegal Bakal Semakin Diperket...    ok_trafilatura         \n",
       "4  30 September 2025 20:35\\nMenteri Keuangan (Men...    ok_trafilatura         \n",
       "\n",
       "   is_empty  elapsed_flag  \n",
       "0     False         False  \n",
       "1     False         False  \n",
       "2     False         False  \n",
       "3     False         False  \n",
       "4     False         False  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "# === Baca daftar URL dari Excel===\n",
    "try:\n",
    "    df_in = pd.read_csv(INPUT_CSV_PATH) # 🚀 limit ke 100 baris\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Gagal membaca Excel input: {INPUT_CSV_PATH} -> {e}\")\n",
    "\n",
    "if INPUT_URL_COLUMN not in df_in.columns:\n",
    "    raise ValueError(f\"Kolom '{INPUT_URL_COLUMN}' tidak ditemukan di file Excel.\")\n",
    "\n",
    "urls = df_in[INPUT_URL_COLUMN].dropna().astype(str).str.strip().unique().tolist()\n",
    "if MAX_URLS is not None:\n",
    "    urls = urls[:MAX_URLS]\n",
    "\n",
    "logger.info(f\"Total URL untuk diproses: {len(urls)} (limit={MAX_URLS})\")\n",
    "\n",
    "results: List[ExtractResult] = []\n",
    "for url in tqdm(urls, desc=\"Scraping artikel\"):\n",
    "    try:\n",
    "        res = extract_with_layers(url, per_article_timeout=PER_ARTICLE_TIMEOUT)\n",
    "        results.append(res)\n",
    "    except Exception as e:\n",
    "        parsed = urlparse(url)\n",
    "        results.append(ExtractResult(url, parsed.netloc.lower(), None, \"fatal_error\", str(e)))\n",
    "\n",
    "# === Buat dataframe hasil scraping ===\n",
    "df_scraped = pd.DataFrame([{\n",
    "    \"url_berita\": r.url,\n",
    "    \"source_domain\": r.domain,\n",
    "    \"artikel_berita\": r.text if r.text else \"\",\n",
    "    \"status\": r.status,\n",
    "    \"error\": r.error if r.error else \"\",\n",
    "} for r in results])\n",
    "\n",
    "# === Gabungkan dengan df_in, tambahkan kolom baru ===\n",
    "df_output = df_in.merge(df_scraped, on=\"url_berita\", how=\"left\")\n",
    "\n",
    "# === Bersihkan artikel_berita ===\n",
    "def final_cleanup(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = normalize_whitespace(text)\n",
    "    lines = [ln for ln in text.splitlines() if len(ln.strip()) > 2]\n",
    "    return \"\\n\".join(lines).strip()\n",
    "\n",
    "df_output[\"artikel_berita\"] = df_output[\"artikel_berita\"].apply(final_cleanup)\n",
    "df_output[\"is_empty\"] = df_output[\"artikel_berita\"].str.len().fillna(0).lt(60)\n",
    "\n",
    "df_output[\"elapsed_flag\"] = df_output[\"status\"].str.startswith(\"timeout\")\n",
    "\n",
    "# === Ringkas hasil ===\n",
    "summary = df_output[\"status\"].value_counts(dropna=False)\n",
    "logger.info(f\"\\nSummary status:\\n{summary}\")\n",
    "\n",
    "# Tampilkan beberapa baris yang timeout untuk inspeksi cepat\n",
    "timeout_preview = df_output[df_output[\"elapsed_flag\"]].head(5)\n",
    "logger.info(f\"Timeout sample rows: {len(timeout_preview)}\")\n",
    "\n",
    "df_output.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxCAw02V-UNM"
   },
   "source": [
    "## 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 189
    },
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1758706465903,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "ljgWZvHQ-guQ",
    "outputId": "c4647f65-0b45-4d6f-eb9f-b8c0aaba96bf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-01 20:43:03,600 | INFO | Tersimpan: /Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Pekerjaan BMN/05. 2025/98_monitoring_berita/monitoring-berita/hasil_baca_berita/hasil_scraping_artikel_20251001_204303.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>judul_berita</th>\n",
       "      <th>url_berita</th>\n",
       "      <th>tanggal_berita</th>\n",
       "      <th>source_domain</th>\n",
       "      <th>artikel_berita</th>\n",
       "      <th>status</th>\n",
       "      <th>error</th>\n",
       "      <th>is_empty</th>\n",
       "      <th>elapsed_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Ulama Jatim Dukung Menkeu Berantas Rokok Ilega...</td>\n",
       "      <td>https://www.detik.com/jatim/berita/d-8137673/u...</td>\n",
       "      <td>2025-09-30T15:10:55+07:00</td>\n",
       "      <td>www.detik.com</td>\n",
       "      <td>Pengasuh Pondok Pesantren Langitan Tuban, KH M...</td>\n",
       "      <td>ok_site_specific</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Bea Cukai Labuan Bajo gencarkan penindakan rok...</td>\n",
       "      <td>https://kupang.antaranews.com/berita/170337/be...</td>\n",
       "      <td>2025-09-30T19:12:10+07:00</td>\n",
       "      <td>kupang.antaranews.com</td>\n",
       "      <td>Labuan Bajo (ANTARA) - Bea Cukai Labuan Bajo, ...</td>\n",
       "      <td>ok_site_specific</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Pemkab Pasuruan Stop Peredaran Rokok Ilegal - ...</td>\n",
       "      <td>https://kabarbaik.co/pemkab-pasuruan-stop-pere...</td>\n",
       "      <td>2025-09-29T14:22:22+07:00</td>\n",
       "      <td>kabarbaik.co</td>\n",
       "      <td>Cek Berita dan Artikel kabarbaik.co yang lain ...</td>\n",
       "      <td>ok_trafilatura</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         judul_berita  \\\n",
       "43  Ulama Jatim Dukung Menkeu Berantas Rokok Ilega...   \n",
       "44  Bea Cukai Labuan Bajo gencarkan penindakan rok...   \n",
       "45  Pemkab Pasuruan Stop Peredaran Rokok Ilegal - ...   \n",
       "\n",
       "                                           url_berita  \\\n",
       "43  https://www.detik.com/jatim/berita/d-8137673/u...   \n",
       "44  https://kupang.antaranews.com/berita/170337/be...   \n",
       "45  https://kabarbaik.co/pemkab-pasuruan-stop-pere...   \n",
       "\n",
       "               tanggal_berita          source_domain  \\\n",
       "43  2025-09-30T15:10:55+07:00          www.detik.com   \n",
       "44  2025-09-30T19:12:10+07:00  kupang.antaranews.com   \n",
       "45  2025-09-29T14:22:22+07:00           kabarbaik.co   \n",
       "\n",
       "                                       artikel_berita            status error  \\\n",
       "43  Pengasuh Pondok Pesantren Langitan Tuban, KH M...  ok_site_specific         \n",
       "44  Labuan Bajo (ANTARA) - Bea Cukai Labuan Bajo, ...  ok_site_specific         \n",
       "45  Cek Berita dan Artikel kabarbaik.co yang lain ...    ok_trafilatura         \n",
       "\n",
       "    is_empty  elapsed_flag  \n",
       "43     False         False  \n",
       "44     False         False  \n",
       "45     False         False  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "# Simpan sebagai CSV/Excel untuk integrasi lanjutan\n",
    "DATE_TAG = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "OUTPUT_CSV = f\"{cwd}/hasil_baca_berita/hasil_scraping_artikel_{DATE_TAG}.csv\"\n",
    "# OUTPUT_XLSX = f\"{cwd}/hasil_baca_berita/hasil_scraping_artikel_{DATE_TAG}.xlsx\"\n",
    "\n",
    "try:\n",
    "    df_output.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "    # df_output.to_excel(OUTPUT_XLSX, index=False)\n",
    "    logger.info(f\"Tersimpan: {OUTPUT_CSV}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Gagal menyimpan output: {e}\")\n",
    "\n",
    "df_output.tail(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1758706465906,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "OXkWq-Kq-kTp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-01 20:43:03,611 | INFO | Berhasil update config.json di /Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Pekerjaan BMN/05. 2025/98_monitoring_berita/monitoring-berita/config.json\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def update_config(path: Path, new_values: dict):\n",
    "    \"\"\"Update config.json hanya pada key tertentu tanpa menimpa keseluruhan isi.\"\"\"\n",
    "    data = {}\n",
    "    if path.exists():\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Gagal membaca config lama: {e}\")\n",
    "            data = {}\n",
    "\n",
    "    # update hanya key yang diberikan\n",
    "    data.update(new_values)\n",
    "\n",
    "    try:\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "        logger.info(f\"Berhasil update config.json di {path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Gagal menyimpan config.json: {e}\")\n",
    "\n",
    "# Simpan OUTPUT_CSV & OUTPUT_XLSX ke config dengan nama yang lebih jelas\n",
    "update_config(CONFIG_PATH, {\n",
    "    \"last_baca_berita_path\": OUTPUT_CSV,\n",
    "})\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNOC2ysORhDQCWdElJ2IoaU",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "220218e84e7c4b5a988c5e53254cfbe9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_504fdeb14baf42ea8a18d6cc32baef23",
       "IPY_MODEL_f3544fe90d974bfc945d30ac5e432956",
       "IPY_MODEL_fc8314b543284cfda1090b53a276c097"
      ],
      "layout": "IPY_MODEL_45135c05354c4242b7fd198658454bb2"
     }
    },
    "45135c05354c4242b7fd198658454bb2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46667759861a4475a5069cb4b6847c4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4a3447c430df4a1ba7428d92a5bbafea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "504fdeb14baf42ea8a18d6cc32baef23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7eab9712b4da4180b083c35b4bcdac6c",
      "placeholder": "​",
      "style": "IPY_MODEL_46667759861a4475a5069cb4b6847c4e",
      "value": "Scraping artikel: 100%"
     }
    },
    "584b6c4ef81a4eb8bc902c112d7fe4fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7eab9712b4da4180b083c35b4bcdac6c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af2282eee0f248e39d69f6a931f8a861": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b10cecf9ab3c4b9aa47e9720522de598": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f3544fe90d974bfc945d30ac5e432956": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b10cecf9ab3c4b9aa47e9720522de598",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4a3447c430df4a1ba7428d92a5bbafea",
      "value": 100
     }
    },
    "fc8314b543284cfda1090b53a276c097": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_af2282eee0f248e39d69f6a931f8a861",
      "placeholder": "​",
      "style": "IPY_MODEL_584b6c4ef81a4eb8bc902c112d7fe4fd",
      "value": " 100/100 [01:13&lt;00:00,  1.87it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
