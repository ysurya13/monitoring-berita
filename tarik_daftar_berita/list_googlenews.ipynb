{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4cc9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set folder tempat kerja (current working directory)\n",
    "import os\n",
    "cwd = '/Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Pekerjaan BMN/05. 2025/98_monitoring_berita/monitoring-berita'\n",
    "# cwd = '/content/drive/MyDrive/Monitoring Berita'\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e522ef7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check library, if not found install it\n",
    "!pip install  googlenewsdecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb78487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Tarik daftar berita via Google News RSS.\n",
    "\n",
    "Menghasilkan DataFrame dengan kolom sama seperti scraper lain:\n",
    "    query, judul_berita, tanggal_berita, penulis_berita, url_berita\n",
    "\n",
    "Sumber: Google News RSS (hl=id, gl=ID)\n",
    "Catatan:\n",
    " - Google News tidak selalu menyediakan penulis, hanya sumber (media). Itu kita mapping ke penulis_berita.\n",
    " - Tanggal di <pubDate> adalah GMT. Kita konversi ke zona Asia/Jakarta dan format \"%Y-%m-%d %H:%M:%S\".\n",
    " - Kembali ditambahkan filter tanggal: hanya tanggal (YYYY-MM-DD) yang ada di config['search_date'] yang diikutkan jika daftar itu tidak kosong.\n",
    " - Pembatas jumlah item per query diterapkan SETELAH filter tanggal (agar slot diisi item relevan tanggal target).\n",
    "\n",
    "Pemakaian:\n",
    "    python list_berita_google_news_rss.py  # hasil akan tersimpan ke daftar_berita/google_news_rss.xlsx\n",
    "\n",
    "Opsi lingkungan (opsional melalui variabel environment):\n",
    "    GNEWS_TIME_WINDOW_DAYS  (default 7)  -> batas pencarian relatif (when:7d) agar cakupan feed cukup.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "from typing import List, Dict, Optional\n",
    "import datetime as dt\n",
    "import zoneinfo\n",
    "import re\n",
    "import html\n",
    "import urllib.parse as urlparse\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from googlenewsdecoder import gnewsdecoder\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Konfigurasi logging\n",
    "# --------------------------------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"[%(levelname)s] %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(\"google_news_rss\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Parameter umum\n",
    "# --------------------------------------------------\n",
    "JAKARTA_TZ = zoneinfo.ZoneInfo(\"Asia/Jakarta\")\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:125.0) Gecko/20100101 Firefox/125.0\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36\",\n",
    "]\n",
    "REQUEST_DELAY_RANGE = (0.8, 1.6)\n",
    "RETRY_TOTAL = 3\n",
    "TIME_WINDOW_DAYS = int(os.environ.get(\"GNEWS_TIME_WINDOW_DAYS\", \"7\"))\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Util: HTTP fetch dengan retry sederhana\n",
    "# --------------------------------------------------\n",
    "\n",
    "def fetch_url(url: str, timeout: float = 15.0) -> str:\n",
    "    last_err: Optional[Exception] = None\n",
    "    for attempt in range(1, RETRY_TOTAL + 1):\n",
    "        try:\n",
    "            headers = {\"User-Agent\": random.choice(USER_AGENTS)}\n",
    "            r = requests.get(url, headers=headers, timeout=timeout)\n",
    "            if r.status_code >= 400:\n",
    "                raise RuntimeError(f\"Status {r.status_code}\")\n",
    "            return r.text\n",
    "        except Exception as e:  # noqa: BLE001\n",
    "            last_err = e\n",
    "            logger.warning(\"Percobaan %s gagal (%s): %s\", attempt, url, e)\n",
    "            time.sleep(0.5 * attempt)\n",
    "    raise RuntimeError(f\"Gagal fetch setelah {RETRY_TOTAL} percobaan: {last_err}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Build URL Google News RSS\n",
    "# --------------------------------------------------\n",
    "\n",
    "def build_google_news_rss_url(query: str, time_window_days: int = TIME_WINDOW_DAYS) -> str:\n",
    "    q = query.strip()\n",
    "    encoded = urlparse.quote_plus(f\"{q} when:{time_window_days}d\")\n",
    "    base = \"https://news.google.com/rss/search\"\n",
    "    return f\"{base}?q={encoded}&hl=id&gl=ID&ceid=ID:id\"\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Parse RSS XML ke list item\n",
    "# --------------------------------------------------\n",
    "\n",
    "def parse_rss_items(xml_text: str) -> List[Dict[str, str]]:\n",
    "    cleaned = re.sub(r\"[\\x00-\\x08\\x0b-\\x0c\\x0e-\\x1f]\", \"\", xml_text)\n",
    "    root = ET.fromstring(cleaned)\n",
    "    channel = root.find(\"channel\")\n",
    "    if channel is None:\n",
    "        return []\n",
    "    items_out: List[Dict[str, str]] = []\n",
    "    for item in channel.findall(\"item\"):\n",
    "        title_el = item.find(\"title\")\n",
    "        link_el = item.find(\"link\")\n",
    "        pub_el = item.find(\"pubDate\")\n",
    "        source_el = item.find(\"source\")\n",
    "\n",
    "        title = html.unescape(title_el.text.strip()) if title_el is not None and title_el.text else \"\"\n",
    "        link = link_el.text.strip() if link_el is not None and link_el.text else \"\"\n",
    "        pub_raw = pub_el.text.strip() if pub_el is not None and pub_el.text else \"\"\n",
    "        source = source_el.text.strip() if source_el is not None and source_el.text else \"\"\n",
    "\n",
    "        final_url = resolve_final_article_url(link)\n",
    "\n",
    "        items_out.append({\n",
    "            \"judul_berita\": title,\n",
    "            \"url_berita\": final_url,\n",
    "            \"pub_raw\": pub_raw,\n",
    "            \"penulis_berita\": source,\n",
    "        })\n",
    "    return items_out\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Resolve final URL dari link news.google.com jika ada parameter url=...\n",
    "# --------------------------------------------------\n",
    "\n",
    "def resolve_final_article_url(link: str) -> str:\n",
    "    if not link:\n",
    "        return link\n",
    "    try:\n",
    "        if \"news.google.com\" in link and \"url=\" in link:\n",
    "            parsed = urlparse.urlparse(link)\n",
    "            qs = urlparse.parse_qs(parsed.query)\n",
    "            if \"url\" in qs and qs[\"url\"]:\n",
    "                return qs[\"url\"][0]\n",
    "    except Exception:  # noqa: BLE001\n",
    "        return link\n",
    "    return link\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Convert pubDate -> datetime lokal & format string\n",
    "# --------------------------------------------------\n",
    "RFC_PARSE_FORMATS = [\n",
    "    \"%a, %d %b %Y %H:%M:%S %Z\",\n",
    "    \"%a, %d %b %Y %H:%M:%S %z\",\n",
    "]\n",
    "\n",
    "\n",
    "def parse_pubdate(pub_raw: str) -> Optional[dt.datetime]:\n",
    "    if not pub_raw:\n",
    "        return None\n",
    "    for fmt in RFC_PARSE_FORMATS:\n",
    "        try:\n",
    "            dt_obj = dt.datetime.strptime(pub_raw, fmt)\n",
    "            if dt_obj.tzinfo is None:\n",
    "                dt_obj = dt_obj.replace(tzinfo=dt.timezone.utc)\n",
    "            return dt_obj.astimezone(JAKARTA_TZ)\n",
    "        except Exception:  # noqa: BLE001\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Ambil berita untuk satu query (filter tanggal + batasi max item)\n",
    "# --------------------------------------------------\n",
    "\n",
    "def scrape_google_news_query(query: str, max_items: int, date_filters: List[str], delay_range=REQUEST_DELAY_RANGE) -> pd.DataFrame:\n",
    "    url = build_google_news_rss_url(query)\n",
    "    logger.info(\"[QUERY] %s => %s\", query, url)\n",
    "    try:\n",
    "        xml_text = fetch_url(url)\n",
    "    except Exception as e:  # noqa: BLE001\n",
    "        logger.error(\"Gagal fetch query '%s': %s\", query, e)\n",
    "        return pd.DataFrame(columns=[\"query\", \"judul_berita\", \"tanggal_berita\", \"penulis_berita\", \"url_berita\"])\n",
    "\n",
    "    raw_items = parse_rss_items(xml_text)\n",
    "\n",
    "    # Transform + filter tanggal jika disediakan\n",
    "    out_items: List[Dict[str, str]] = []\n",
    "    date_set = set(d.strip() for d in date_filters if d.strip()) if date_filters else None\n",
    "    for it in raw_items:\n",
    "        pub_dt = parse_pubdate(it.get(\"pub_raw\", \"\"))\n",
    "        tanggal_fmt = pub_dt.strftime(\"%Y-%m-%d %H:%M:%S\") if pub_dt else \"\"\n",
    "        date_only = tanggal_fmt[:10] if tanggal_fmt else None\n",
    "        if date_set is not None and date_only not in date_set:\n",
    "            continue\n",
    "        out_items.append({\n",
    "            \"query\": query,\n",
    "            \"judul_berita\": it.get(\"judul_berita\", \"\"),\n",
    "            \"tanggal_berita\": tanggal_fmt,\n",
    "            \"penulis_berita\": it.get(\"penulis_berita\", \"\"),\n",
    "            \"url_berita\": it.get(\"url_berita\", \"\"),\n",
    "        })\n",
    "\n",
    "    # Batasi setelah filter\n",
    "    if max_items > 0 and len(out_items) > max_items:\n",
    "        out_items = out_items[: max_items]\n",
    "\n",
    "    df = pd.DataFrame(out_items, columns=[\"query\", \"judul_berita\", \"tanggal_berita\", \"penulis_berita\", \"url_berita\"])\n",
    "    if not df.empty:\n",
    "        df = df.drop_duplicates(subset=[\"url_berita\"]).reset_index(drop=True)\n",
    "    time.sleep(random.uniform(*delay_range))\n",
    "    return df\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Ambil berita untuk banyak query\n",
    "# --------------------------------------------------\n",
    "\n",
    "def scrape_google_news_queries(queries: List[str], max_items: int, date_filters: List[str]) -> pd.DataFrame:\n",
    "    all_df: List[pd.DataFrame] = []\n",
    "    for q in queries:\n",
    "        df_q = scrape_google_news_query(q, max_items=max_items, date_filters=date_filters)\n",
    "        all_df.append(df_q)\n",
    "    if not all_df:\n",
    "        return pd.DataFrame(columns=[\"query\", \"judul_berita\", \"tanggal_berita\", \"penulis_berita\", \"url_berita\"])\n",
    "    df = pd.concat(all_df, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Main CLI (aman untuk notebook & script)\n",
    "# --------------------------------------------------\n",
    "\n",
    "def main():  # noqa: D401\n",
    "    if \"__file__\" in globals():\n",
    "        base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\n",
    "    else:\n",
    "        base_dir = os.getcwd()\n",
    "\n",
    "    config_path = os.path.join(base_dir, \"config.json\")\n",
    "    if not os.path.exists(config_path):\n",
    "        raise SystemExit(f\"config.json tidak ditemukan di {base_dir}\")\n",
    "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    queries = config.get(\"keywords\", [])\n",
    "    date_filters = config.get(\"search_date\", [])  # daftar tanggal (YYYY-MM-DD)\n",
    "    # ubah untuk hasil akhir\n",
    "    max_items = 100\n",
    "    if not queries:\n",
    "        raise SystemExit(\"keywords kosong di config.json\")\n",
    "\n",
    "    logger.info(\n",
    "        \"Mulai scrape Google News RSS | queries=%s | max_items_per_query=%s | date_filters=%s\",\n",
    "        len(queries), max_items, date_filters if date_filters else \"(tidak ada, ambil semua)\"\n",
    "    )\n",
    "    df = scrape_google_news_queries(queries, max_items=max_items, date_filters=date_filters)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def convert_link():\n",
    "    if \"__file__\" in globals():\n",
    "        base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\n",
    "    else:\n",
    "        base_dir = os.getcwd()\n",
    "\n",
    "    df = main()\n",
    "\n",
    "    interval_time = 1  # interval is optional, default is None\n",
    "    daftar_berita = df['url_berita']\n",
    "    \n",
    "    decoded = []\n",
    "    for source_url in daftar_berita:\n",
    "        try:\n",
    "            decoded_url = gnewsdecoder(source_url, interval=interval_time)\n",
    "\n",
    "            if decoded_url.get(\"status\"):\n",
    "                print(\"Decoded URL:\", decoded_url[\"decoded_url\"])\n",
    "            else:\n",
    "                print(\"Error:\", decoded_url[\"message\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {e}\")\n",
    "        decoded.append(decoded_url[\"decoded_url\"])\n",
    "    df['url_berita'] = decoded\n",
    "    out_dir = os.path.join(base_dir, \"daftar_berita\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_xlsx = os.path.join(out_dir, \"google_news_rss.xlsx\")\n",
    "    df.to_excel(out_xlsx, index=False)\n",
    "    logger.info(\"Selesai. Tersimpan: %s (%s baris)\", out_xlsx, len(df))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    convert_link()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
