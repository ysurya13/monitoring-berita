{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "750397cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset execution state untuk mencegah duplikasi\n",
    "import sys\n",
    "if hasattr(sys.modules[__name__], '_already_executed'):\n",
    "    print(\"Mencegah eksekusi ganda - gunakan restart kernel jika diperlukan\")\n",
    "else:\n",
    "    sys.modules[__name__]._already_executed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c4cc9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set folder tempat kerja (current working directory)\n",
    "import os\n",
    "cwd = '/Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Pekerjaan BMN/05. 2025/98_monitoring_berita/monitoring-berita'\n",
    "# cwd = '/content/drive/MyDrive/Monitoring Berita'\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e522ef7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mengecek dependencies...\n",
      "‚úì googlenewsdecoder sudah terinstall\n",
      "‚úì tqdm sudah terinstall\n",
      "\n",
      "üéâ Semua dependencies siap!\n"
     ]
    }
   ],
   "source": [
    "# Check and install required libraries only if not already installed\n",
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def check_and_install_package(package_name, import_name=None):\n",
    "    \"\"\"Check if package is installed, if not install it\"\"\"\n",
    "    if import_name is None:\n",
    "        import_name = package_name\n",
    "    \n",
    "    try:\n",
    "        importlib.import_module(import_name)\n",
    "        print(f\"‚úì {package_name} sudah terinstall\")\n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(f\"‚ö† {package_name} belum terinstall, menginstall...\")\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
    "            print(f\"‚úì {package_name} berhasil diinstall\")\n",
    "            return True\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"‚úó Gagal menginstall {package_name}\")\n",
    "            return False\n",
    "\n",
    "# Check required packages\n",
    "packages_to_check = [\n",
    "    (\"googlenewsdecoder\", \"googlenewsdecoder\"),\n",
    "    (\"tqdm\", \"tqdm\")\n",
    "]\n",
    "\n",
    "print(\"Mengecek dependencies...\")\n",
    "all_installed = True\n",
    "for package, import_name in packages_to_check:\n",
    "    if not check_and_install_package(package, import_name):\n",
    "        all_installed = False\n",
    "\n",
    "if all_installed:\n",
    "    print(\"\\nüéâ Semua dependencies siap!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Ada masalah dengan instalasi dependencies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb78487d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Google News untuk 3 queries dengan max 200 artikel per query\n",
      "Filter tanggal: 2025-10-02, 2025-10-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching: kilang: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:05<00:00,  1.86s/query, Articles=174]\n",
      "Removing duplicates: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 576.79step/s, Author+Date+Title]\n",
      "Fetching: kilang: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:05<00:00,  1.86s/query, Articles=174]\n",
      "Removing duplicates: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 576.79step/s, Author+Date+Title]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplikasi dihapus: 174 ‚Üí 162 artikel (dihapus: 12)\n",
      "\n",
      "Memulai decode 162 Google News URLs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decode URLs:  20%|‚ñà‚ñâ        | 32/162 [00:59<04:01,  1.86s/url, ‚úì=32, ‚úó=0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 363\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;18m__name__\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_conversion_done\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 363\u001b[0m         convert_link()\n\u001b[1;32m    364\u001b[0m         sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;18m__name__\u001b[39m]\u001b[38;5;241m.\u001b[39m_conversion_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[19], line 328\u001b[0m, in \u001b[0;36mconvert_link\u001b[0;34m()\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, source_url \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(daftar_berita):\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m         decoded_url \u001b[38;5;241m=\u001b[39m gnewsdecoder(source_url, interval\u001b[38;5;241m=\u001b[39minterval_time)\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m decoded_url\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    331\u001b[0m             decoded\u001b[38;5;241m.\u001b[39mappend(decoded_url[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoded_url\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/googlenewsdecoder/__init__.py:25\u001b[0m, in \u001b[0;36mgnewsdecoder\u001b[0;34m(source_url, interval, proxy)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03mDecodes a Google News article URL into its original source URL.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03mThis is a convenience function that uses the GoogleDecoder class internally.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m          otherwise 'status' and 'message'.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     24\u001b[0m decoder \u001b[38;5;241m=\u001b[39m GoogleDecoder(proxy\u001b[38;5;241m=\u001b[39mproxy)\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decoder\u001b[38;5;241m.\u001b[39mdecode_google_news_url(source_url, interval\u001b[38;5;241m=\u001b[39minterval)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/googlenewsdecoder/new_decoderv2.py:193\u001b[0m, in \u001b[0;36mGoogleDecoder.decode_google_news_url\u001b[0;34m(self, source_url, interval)\u001b[0m\n\u001b[1;32m    187\u001b[0m     decoded_url_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_url(\n\u001b[1;32m    188\u001b[0m         decoding_params_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msignature\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    189\u001b[0m         decoding_params_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    190\u001b[0m         decoding_params_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase64_str\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    191\u001b[0m     )\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m interval:\n\u001b[0;32m--> 193\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(interval)\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoded_url_response\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"Tarik daftar berita via Google News RSS.\n",
    "\n",
    "Menghasilkan DataFrame dengan kolom sama seperti scraper lain:\n",
    "    query, judul_berita, tanggal_berita, penulis_berita, url_berita\n",
    "\n",
    "Sumber: Google News RSS (hl=id, gl=ID)\n",
    "Catatan:\n",
    " - Google News tidak selalu menyediakan penulis, hanya sumber (media). Itu kita mapping ke penulis_berita.\n",
    " - Tanggal di <pubDate> adalah GMT. Kita konversi ke zona Asia/Jakarta dan format \"%Y-%m-%d %H:%M:%S\".\n",
    " - Kembali ditambahkan filter tanggal: hanya tanggal (YYYY-MM-DD) yang ada di config['search_date'] yang diikutkan jika daftar itu tidak kosong.\n",
    " - Pembatas jumlah item per query diterapkan SETELAH filter tanggal (agar slot diisi item relevan tanggal target).\n",
    "\n",
    "Pemakaian:\n",
    "    python list_berita_google_news_rss.py  # hasil akan tersimpan ke daftar_berita/google_news_rss.xlsx\n",
    "\n",
    "Opsi lingkungan (opsional melalui variabel environment):\n",
    "    GNEWS_TIME_WINDOW_DAYS  (default 7)  -> batas pencarian relatif (when:7d) agar cakupan feed cukup.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from typing import List, Dict, Optional\n",
    "import datetime as dt\n",
    "import zoneinfo\n",
    "import re\n",
    "import html\n",
    "import urllib.parse as urlparse\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "from googlenewsdecoder import gnewsdecoder\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Parameter umum\n",
    "# --------------------------------------------------\n",
    "JAKARTA_TZ = zoneinfo.ZoneInfo(\"Asia/Jakarta\")\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:125.0) Gecko/20100101 Firefox/125.0\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36\",\n",
    "]\n",
    "REQUEST_DELAY_RANGE = (0.8, 1.6)\n",
    "RETRY_TOTAL = 3\n",
    "TIME_WINDOW_DAYS = int(os.environ.get(\"GNEWS_TIME_WINDOW_DAYS\", \"7\"))\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Util: HTTP fetch dengan retry sederhana\n",
    "# --------------------------------------------------\n",
    "\n",
    "def fetch_url(url: str, timeout: float = 15.0) -> str:\n",
    "    last_err: Optional[Exception] = None\n",
    "    for attempt in range(1, RETRY_TOTAL + 1):\n",
    "        try:\n",
    "            headers = {\"User-Agent\": random.choice(USER_AGENTS)}\n",
    "            r = requests.get(url, headers=headers, timeout=timeout)\n",
    "            if r.status_code >= 400:\n",
    "                raise RuntimeError(f\"Status {r.status_code}\")\n",
    "            return r.text\n",
    "        except Exception as e:  # noqa: BLE001\n",
    "            last_err = e\n",
    "            time.sleep(0.5 * attempt)\n",
    "    raise RuntimeError(f\"Gagal fetch setelah {RETRY_TOTAL} percobaan: {last_err}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Build URL Google News RSS\n",
    "# --------------------------------------------------\n",
    "\n",
    "def build_google_news_rss_url(query: str, time_window_days: int = TIME_WINDOW_DAYS) -> str:\n",
    "    q = query.strip()\n",
    "    encoded = urlparse.quote_plus(f\"{q} when:{time_window_days}d\")\n",
    "    base = \"https://news.google.com/rss/search\"\n",
    "    return f\"{base}?q={encoded}&hl=id&gl=ID&ceid=ID:id\"\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Parse RSS XML ke list item\n",
    "# --------------------------------------------------\n",
    "\n",
    "def parse_rss_items(xml_text: str) -> List[Dict[str, str]]:\n",
    "    cleaned = re.sub(r\"[\\x00-\\x08\\x0b-\\x0c\\x0e-\\x1f]\", \"\", xml_text)\n",
    "    root = ET.fromstring(cleaned)\n",
    "    channel = root.find(\"channel\")\n",
    "    if channel is None:\n",
    "        return []\n",
    "    items_out: List[Dict[str, str]] = []\n",
    "    for item in channel.findall(\"item\"):\n",
    "        title_el = item.find(\"title\")\n",
    "        link_el = item.find(\"link\")\n",
    "        pub_el = item.find(\"pubDate\")\n",
    "        source_el = item.find(\"source\")\n",
    "\n",
    "        title = html.unescape(title_el.text.strip()) if title_el is not None and title_el.text else \"\"\n",
    "        link = link_el.text.strip() if link_el is not None and link_el.text else \"\"\n",
    "        pub_raw = pub_el.text.strip() if pub_el is not None and pub_el.text else \"\"\n",
    "        source = source_el.text.strip() if source_el is not None and source_el.text else \"\"\n",
    "\n",
    "        final_url = resolve_final_article_url(link)\n",
    "\n",
    "        items_out.append({\n",
    "            \"judul_berita\": title,\n",
    "            \"url_berita\": final_url,\n",
    "            \"pub_raw\": pub_raw,\n",
    "            \"penulis_berita\": source,\n",
    "        })\n",
    "    return items_out\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Resolve final URL dari link news.google.com jika ada parameter url=...\n",
    "# --------------------------------------------------\n",
    "\n",
    "def resolve_final_article_url(link: str) -> str:\n",
    "    if not link:\n",
    "        return link\n",
    "    try:\n",
    "        if \"news.google.com\" in link and \"url=\" in link:\n",
    "            parsed = urlparse.urlparse(link)\n",
    "            qs = urlparse.parse_qs(parsed.query)\n",
    "            if \"url\" in qs and qs[\"url\"]:\n",
    "                return qs[\"url\"][0]\n",
    "    except Exception:  # noqa: BLE001\n",
    "        return link\n",
    "    return link\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Convert pubDate -> datetime lokal & format string\n",
    "# --------------------------------------------------\n",
    "RFC_PARSE_FORMATS = [\n",
    "    \"%a, %d %b %Y %H:%M:%S %Z\",\n",
    "    \"%a, %d %b %Y %H:%M:%S %z\",\n",
    "]\n",
    "\n",
    "\n",
    "def parse_pubdate(pub_raw: str) -> Optional[dt.datetime]:\n",
    "    if not pub_raw:\n",
    "        return None\n",
    "    for fmt in RFC_PARSE_FORMATS:\n",
    "        try:\n",
    "            dt_obj = dt.datetime.strptime(pub_raw, fmt)\n",
    "            if dt_obj.tzinfo is None:\n",
    "                dt_obj = dt_obj.replace(tzinfo=dt.timezone.utc)\n",
    "            return dt_obj.astimezone(JAKARTA_TZ)\n",
    "        except Exception:  # noqa: BLE001\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Ambil berita untuk satu query (filter tanggal + batasi max item)\n",
    "# --------------------------------------------------\n",
    "\n",
    "def scrape_google_news_query(query: str, max_items: int, date_filters: List[str], delay_range=REQUEST_DELAY_RANGE, pbar=None) -> pd.DataFrame:\n",
    "    url = build_google_news_rss_url(query)\n",
    "    if pbar:\n",
    "        pbar.set_description(f\"Fetching: {query}\")\n",
    "    \n",
    "    try:\n",
    "        xml_text = fetch_url(url)\n",
    "    except Exception as e:  # noqa: BLE001\n",
    "        return pd.DataFrame(columns=[\"query\", \"judul_berita\", \"tanggal_berita\", \"penulis_berita\", \"url_berita\"])\n",
    "\n",
    "    raw_items = parse_rss_items(xml_text)\n",
    "\n",
    "    # Transform + filter tanggal jika disediakan\n",
    "    out_items: List[Dict[str, str]] = []\n",
    "    date_set = set(d.strip() for d in date_filters if d.strip()) if date_filters else None\n",
    "    for it in raw_items:\n",
    "        pub_dt = parse_pubdate(it.get(\"pub_raw\", \"\"))\n",
    "        tanggal_fmt = pub_dt.strftime(\"%Y-%m-%d %H:%M:%S\") if pub_dt else \"\"\n",
    "        date_only = tanggal_fmt[:10] if tanggal_fmt else None\n",
    "        if date_set is not None and date_only not in date_set:\n",
    "            continue\n",
    "        out_items.append({\n",
    "            \"query\": query,\n",
    "            \"judul_berita\": it.get(\"judul_berita\", \"\"),\n",
    "            \"tanggal_berita\": tanggal_fmt,\n",
    "            \"penulis_berita\": it.get(\"penulis_berita\", \"\"),\n",
    "            \"url_berita\": it.get(\"url_berita\", \"\"),\n",
    "        })\n",
    "\n",
    "    # Batasi setelah filter\n",
    "    if max_items > 0 and len(out_items) > max_items:\n",
    "        out_items = out_items[: max_items]\n",
    "\n",
    "    df = pd.DataFrame(out_items, columns=[\"query\", \"judul_berita\", \"tanggal_berita\", \"penulis_berita\", \"url_berita\"])\n",
    "    # Hapus duplikat dasar per query (hanya URL yang sama)\n",
    "    if not df.empty:\n",
    "        df = df.drop_duplicates(subset=[\"url_berita\"]).reset_index(drop=True)\n",
    "    time.sleep(random.uniform(*delay_range))\n",
    "    return df\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Ambil berita untuk banyak query\n",
    "# --------------------------------------------------\n",
    "\n",
    "def scrape_google_news_queries(queries: List[str], max_items: int, date_filters: List[str]) -> pd.DataFrame:\n",
    "    all_df: List[pd.DataFrame] = []\n",
    "    \n",
    "    # Progress bar untuk scraping queries\n",
    "    with tqdm(total=len(queries), desc=\"Scraping queries\", unit=\"query\") as pbar:\n",
    "        for q in queries:\n",
    "            df_q = scrape_google_news_query(q, max_items=max_items, date_filters=date_filters, pbar=pbar)\n",
    "            all_df.append(df_q)\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({'Articles': sum(len(df) for df in all_df)})\n",
    "    \n",
    "    if not all_df:\n",
    "        return pd.DataFrame(columns=[\"query\", \"judul_berita\", \"tanggal_berita\", \"penulis_berita\", \"url_berita\"])\n",
    "    df = pd.concat(all_df, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Fungsi untuk menghapus duplikat menyeluruh\n",
    "# --------------------------------------------------\n",
    "\n",
    "def remove_duplicates_comprehensive(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Menghapus duplikat secara menyeluruh berdasarkan beberapa kriteria:\n",
    "    1. URL berita yang sama\n",
    "    2. Judul berita yang sangat mirip (untuk menangani judul dengan sedikit variasi)\n",
    "    3. Kombinasi penulis dan tanggal yang sama dengan judul mirip\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame dengan kolom [query, judul_berita, tanggal_berita, penulis_berita, url_berita]\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame yang sudah dibersihkan dari duplikat\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    \n",
    "    initial_count = len(df)\n",
    "    \n",
    "    # Progress bar untuk deduplication\n",
    "    with tqdm(total=4, desc=\"Removing duplicates\", unit=\"step\") as pbar:\n",
    "        # 1. Hapus duplikat berdasarkan URL yang sama (antar query bisa ada duplikat)\n",
    "        pbar.set_postfix_str(\"URLs\")\n",
    "        df_cleaned = df.drop_duplicates(subset=[\"url_berita\"], keep='first').reset_index(drop=True)\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # 2. Hapus duplikat berdasarkan judul yang sangat mirip\n",
    "        pbar.set_postfix_str(\"Titles\")\n",
    "        df_cleaned['judul_normalized'] = df_cleaned['judul_berita'].str.lower().str.strip()\n",
    "        df_cleaned['judul_normalized'] = df_cleaned['judul_normalized'].str.replace(r'[^\\w\\s]', ' ', regex=True)\n",
    "        df_cleaned['judul_normalized'] = df_cleaned['judul_normalized'].str.replace(r'\\s+', ' ', regex=True)\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Hapus duplikat berdasarkan judul yang sudah dinormalisasi\n",
    "        pbar.set_postfix_str(\"Normalized titles\")\n",
    "        df_cleaned = df_cleaned.drop_duplicates(subset=[\"judul_normalized\"], keep='first').reset_index(drop=True)\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # 3. Hapus duplikat berdasarkan kombinasi penulis + tanggal dengan judul sangat mirip\n",
    "        pbar.set_postfix_str(\"Author+Date+Title\")\n",
    "        df_cleaned['tanggal_only'] = df_cleaned['tanggal_berita'].str[:10]  # ambil YYYY-MM-DD saja\n",
    "        df_cleaned = df_cleaned.drop_duplicates(subset=[\"penulis_berita\", \"tanggal_only\", \"judul_normalized\"], keep='first').reset_index(drop=True)\n",
    "        \n",
    "        # Hapus kolom bantuan yang tidak diperlukan\n",
    "        df_cleaned = df_cleaned.drop(columns=[\"judul_normalized\", \"tanggal_only\"])\n",
    "        pbar.update(1)\n",
    "    \n",
    "    final_count = len(df_cleaned)\n",
    "    removed_count = initial_count - final_count\n",
    "    print(f\"Duplikasi dihapus: {initial_count} ‚Üí {final_count} artikel (dihapus: {removed_count})\")\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Main CLI (aman untuk notebook & script)\n",
    "# --------------------------------------------------\n",
    "\n",
    "def main():  # noqa: D401\n",
    "    if \"__file__\" in globals():\n",
    "        base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\n",
    "    else:\n",
    "        base_dir = os.getcwd()\n",
    "\n",
    "    config_path = os.path.join(base_dir, \"config.json\")\n",
    "    if not os.path.exists(config_path):\n",
    "        raise SystemExit(f\"config.json tidak ditemukan di {base_dir}\")\n",
    "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    queries = config.get(\"keywords\", [])\n",
    "    date_filters = config.get(\"search_date\", [])  # daftar tanggal (YYYY-MM-DD)\n",
    "    # ubah untuk hasil akhir\n",
    "    max_items = 200\n",
    "    if not queries:\n",
    "        raise SystemExit(\"keywords kosong di config.json\")\n",
    "\n",
    "    print(f\"Scraping Google News untuk {len(queries)} queries dengan max {max_items} artikel per query\")\n",
    "    if date_filters:\n",
    "        print(f\"Filter tanggal: {', '.join(date_filters)}\")\n",
    "    \n",
    "    df = scrape_google_news_queries(queries, max_items=max_items, date_filters=date_filters)\n",
    "    \n",
    "    # Hapus duplikat secara menyeluruh sebelum proses selanjutnya\n",
    "    df = remove_duplicates_comprehensive(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_link():\n",
    "    if \"__file__\" in globals():\n",
    "        base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\n",
    "    else:\n",
    "        base_dir = os.getcwd()\n",
    "\n",
    "    df = main()\n",
    "\n",
    "    interval_time = 1  # interval is optional, default is None\n",
    "    daftar_berita = df['url_berita']\n",
    "    \n",
    "    print(f\"\\nMemulai decode {len(daftar_berita)} Google News URLs...\")\n",
    "    \n",
    "    decoded = []\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    # Progress bar untuk proses decode URL\n",
    "    with tqdm(total=len(daftar_berita), desc=\"Decode URLs\", unit=\"url\") as pbar:\n",
    "        for i, source_url in enumerate(daftar_berita):\n",
    "            try:\n",
    "                decoded_url = gnewsdecoder(source_url, interval=interval_time)\n",
    "                \n",
    "                if decoded_url.get(\"status\"):\n",
    "                    decoded.append(decoded_url[\"decoded_url\"])\n",
    "                    success_count += 1\n",
    "                    pbar.set_postfix({'‚úì': success_count, '‚úó': error_count})\n",
    "                else:\n",
    "                    decoded.append(source_url)  # Gunakan URL asli jika decode gagal\n",
    "                    error_count += 1\n",
    "                    pbar.set_postfix({'‚úì': success_count, '‚úó': error_count})\n",
    "                    \n",
    "            except Exception as e:\n",
    "                decoded.append(source_url)  # Gunakan URL asli jika ada exception\n",
    "                error_count += 1\n",
    "                pbar.set_postfix({'‚úì': success_count, '‚úó': error_count})\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Update DataFrame dengan URL yang sudah di-decode\n",
    "    df['url_berita'] = decoded\n",
    "    \n",
    "    # Simpan hasil\n",
    "    out_dir = os.path.join(base_dir, \"daftar_berita\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_xlsx = os.path.join(out_dir, \"google_news_rss.xlsx\")\n",
    "    df.to_excel(out_xlsx, index=False)\n",
    "    \n",
    "    print(f\"\\nSelesai! File disimpan: {out_xlsx}\")\n",
    "    print(f\"Total artikel: {len(df)}\")\n",
    "    print(f\"URL decode: {success_count} berhasil, {error_count} gagal\")\n",
    "\n",
    "\n",
    "# Jalankan hanya jika belum pernah dieksekusi\n",
    "if not hasattr(sys.modules[__name__], '_conversion_done'):\n",
    "    if __name__ == \"__main__\":\n",
    "        convert_link()\n",
    "        sys.modules[__name__]._conversion_done = True\n",
    "else:\n",
    "    print(\"Sudah dijalankan sebelumnya. Restart kernel jika ingin menjalankan ulang.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
