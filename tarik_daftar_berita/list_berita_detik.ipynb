{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1579,
     "status": "ok",
     "timestamp": 1758171474611,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "92tZqTz-E00W",
    "outputId": "4666c754-82dd-4e8a-b261-166130091910"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# mount the colab with google drive\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# mount the colab with google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_qX1v6HMFSaL"
   },
   "outputs": [],
   "source": [
    "# set folder tempat kerja (current working directory)\n",
    "\n",
    "import os\n",
    "\n",
    "# cwd = '/content/drive/MyDrive/Monitoring Berita'\n",
    "\n",
    "cwd = '/Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Pekerjaan BMN/05. 2025/98_monitoring_berita/monitoring-berita'\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JY9GIerCFUt1"
   },
   "outputs": [],
   "source": [
    "# === Sel 1: Import & Logging Config ===\n",
    "\n",
    "# %%\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "from urllib.parse import urljoin\n",
    "from datetime import datetime\n",
    "\n",
    "# Konfigurasi logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\"\n",
    ")\n",
    "logger = logging.getLogger(\"detik_scraper\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-gclN6OfnEuw"
   },
   "outputs": [],
   "source": [
    "# --- Sel 2: Parameter (mudah diubah) ---\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"config.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Kata kunci topik untuk analisis relevansi judul\n",
    "topic_keywords = config[\"keywords\"]\n",
    "\n",
    "# Daftar tanggal (YYYY-MM-DD). Akan di-convert ke DD-MM-YYYY untuk pencocokan di halaman.\n",
    "dates = config[\"search_date\"]\n",
    "\n",
    "# Maksimum halaman per tanggal (akan berhenti lebih awal jika halaman kosong)\n",
    "max_pages_per_date = config[\"max_page_length\"]*3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jnMgWG2BFyO0"
   },
   "outputs": [],
   "source": [
    "# === Sel 3 (REVISI): Kelas DetikNewsScraper dengan stop-on-no-new ===\n",
    "\n",
    "# %%\n",
    "class DetikNewsScraper:\n",
    "    \"\"\"\n",
    "    Scraper indeks harian Detik News.\n",
    "    Contoh indeks:\n",
    "      https://news.detik.com/berita/indeks?page=2&date=09/17/2025\n",
    "    Pola:\n",
    "      https://news.detik.com/indeks?page={page}&date={MM}/{DD}/{YYYY}\n",
    "    \"\"\"\n",
    "\n",
    "    BASE_INDEX = \"https://news.detik.com/berita/indeks\"\n",
    "    BASE_DOMAIN = \"https://news.detik.com/\"\n",
    "\n",
    "    def __init__(self, timeout=10):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            \"User-Agent\": (\n",
    "                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                \"Chrome/125.0.0.0 Safari/537.36\"\n",
    "            ),\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "            \"Accept-Language\": \"id,en-US;q=0.9,en;q=0.8\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "        })\n",
    "        self.timeout = timeout\n",
    "\n",
    "    # -------------------- Util & HTTP --------------------\n",
    "    def _safe_get(self, url, max_retries=3):\n",
    "        delay = 1.0\n",
    "        last_exc = None\n",
    "        for attempt in range(1, max_retries + 1):\n",
    "            try:\n",
    "                resp = self.session.get(url, timeout=self.timeout)\n",
    "                resp.raise_for_status()\n",
    "                return resp\n",
    "            except (requests.exceptions.Timeout, requests.exceptions.HTTPError) as e:\n",
    "                status = getattr(e.response, \"status_code\", None)\n",
    "                if isinstance(e, requests.exceptions.Timeout) or (status and 500 <= status < 600):\n",
    "                    logger.warning(f\"[{attempt}/{max_retries}] Error {type(e).__name__} untuk {url}. Retry dalam {int(delay)}s.\")\n",
    "                    last_exc = e\n",
    "                    time.sleep(delay)\n",
    "                    delay *= 2\n",
    "                    continue\n",
    "                else:\n",
    "                    logger.error(f\"HTTP error non-retryable untuk {url}: {e}\")\n",
    "                    raise\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                logger.error(f\"RequestException untuk {url}: {e}\")\n",
    "                last_exc = e\n",
    "                break\n",
    "        if last_exc:\n",
    "            raise last_exc\n",
    "\n",
    "    @staticmethod\n",
    "    def _norm_text(s):\n",
    "        if not s:\n",
    "            return \"\"\n",
    "        s = s.lower()\n",
    "        s = re.sub(r\"[^\\w\\s]\", \" \", s, flags=re.UNICODE)\n",
    "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "        return s\n",
    "\n",
    "    # -------------------- URL Builder --------------------\n",
    "    def _build_index_url(self, date_str: str, page: int) -> str:\n",
    "        try:\n",
    "            dt = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            raise ValueError(f\"Tanggal tidak valid: {date_str}. Gunakan format YYYY-MM-DD.\")\n",
    "        date_for_query = dt.strftime(\"%m/%d/%Y\")\n",
    "        return f\"{self.BASE_INDEX}?page={page}&date={date_for_query}\"\n",
    "\n",
    "    # -------------------- Parsing List Page --------------------\n",
    "    def _parse_list_page(self, html: str):\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        candidates = []\n",
    "        candidates.extend(soup.select(\"article.list-content__item\"))\n",
    "        candidates.extend(soup.select(\"div.list-content__item\"))\n",
    "        candidates.extend(soup.select(\"li\"))\n",
    "\n",
    "        uniq = []\n",
    "        seen = set()\n",
    "        for tag in candidates:\n",
    "            key = id(tag)\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                uniq.append(tag)\n",
    "\n",
    "        items = []\n",
    "        for tag in uniq:\n",
    "            a = tag.select_one(\"a[href]\")\n",
    "            title_node = (\n",
    "                tag.select_one(\".media__title\") or\n",
    "                tag.select_one(\".title\") or\n",
    "                tag.select_one(\"h3\") or\n",
    "                (a and a)\n",
    "            )\n",
    "            if a and title_node:\n",
    "                items.append(tag)\n",
    "        return items\n",
    "\n",
    "    # -------------------- Ekstrak Data per Item --------------------\n",
    "    def _extract_article_data(self, tag, fallback_date: str, topic_keywords: list[str]) -> dict | None:\n",
    "        try:\n",
    "            a = tag.select_one(\"a[href]\")\n",
    "            if not a:\n",
    "                return None\n",
    "\n",
    "            url = a.get(\"href\", \"\").strip()\n",
    "            if not url:\n",
    "                return None\n",
    "            url = urljoin(self.BASE_DOMAIN, url)\n",
    "\n",
    "            title_node = (\n",
    "                tag.select_one(\".media__title\") or\n",
    "                tag.select_one(\".title\") or\n",
    "                tag.select_one(\"h3\") or\n",
    "                a\n",
    "            )\n",
    "            title = title_node.get_text(strip=True) if title_node else \"\"\n",
    "            if not title:\n",
    "                return None\n",
    "\n",
    "            date_text = \"\"\n",
    "            date_node = (\n",
    "                tag.select_one(\".media__date\") or\n",
    "                tag.select_one(\".list-content__date\") or\n",
    "                tag.select_one(\".date\") or\n",
    "                tag.select_one(\"time\")\n",
    "            )\n",
    "            if date_node:\n",
    "                date_text = date_node.get_text(\" \", strip=True)\n",
    "\n",
    "            tanggal_berita = date_text if date_text else fallback_date\n",
    "\n",
    "            penulis = \"Tidak Diketahui\"\n",
    "            author_node = (\n",
    "                tag.select_one(\".media__author\") or\n",
    "                tag.select_one(\".author\") or\n",
    "                tag.select_one(\"[rel='author']\")\n",
    "            )\n",
    "            if author_node:\n",
    "                pn = author_node.get_text(\" \", strip=True)\n",
    "                if pn:\n",
    "                    penulis = pn\n",
    "\n",
    "            norm_title = self._norm_text(title)\n",
    "            found = []\n",
    "            for kw in topic_keywords:\n",
    "                if not kw:\n",
    "                    continue\n",
    "                kw_norm = self._norm_text(kw)\n",
    "                if kw_norm and re.search(rf\"\\b{re.escape(kw_norm)}\\b\", norm_title):\n",
    "                    found.append(kw)\n",
    "\n",
    "            return {\n",
    "                \"judul_berita\": title,\n",
    "                \"tanggal_berita\": tanggal_berita,\n",
    "                \"penulis_berita\": penulis,\n",
    "                \"url_berita\": url,\n",
    "                \"relevan\": bool(found),\n",
    "                \"keywords_found\": \", \".join(found)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Gagal parse 1 item: {e}\")\n",
    "            return None\n",
    "\n",
    "    # -------------------- Scrape per tanggal --------------------\n",
    "    def scrape_date(self, date_str: str, max_pages: int, topic_keywords: list[str]) -> pd.DataFrame:\n",
    "        all_rows = []\n",
    "        seen_urls = set()  # track URL unik per tanggal\n",
    "        fallback_date_iso = date_str\n",
    "\n",
    "        for page in range(1, max_pages + 1):\n",
    "            url = self._build_index_url(date_str, page)\n",
    "            try:\n",
    "                logger.info(f\"Proses tanggal {date_str} | Halaman {page} | URL: {url}\")\n",
    "                resp = self._safe_get(url)\n",
    "                html = resp.text\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Gagal memuat halaman indeks: {e}. Lewati halaman ini.\")\n",
    "                continue\n",
    "\n",
    "            items = self._parse_list_page(html)\n",
    "            logger.info(f\"  Ditemukan {len(items)} item artikel pada halaman ini.\")\n",
    "\n",
    "            if not items:\n",
    "                logger.info(\"  Halaman tidak memuat artikel lagi. Hentikan iterasi tanggal ini.\")\n",
    "                break\n",
    "\n",
    "            before = len(seen_urls)\n",
    "            added_this_page = 0\n",
    "            dup_this_page = 0\n",
    "\n",
    "            for tag in items:\n",
    "                row = self._extract_article_data(tag, fallback_date_iso, topic_keywords)\n",
    "                if not row:\n",
    "                    continue\n",
    "                u = row[\"url_berita\"]\n",
    "                if u in seen_urls:\n",
    "                    dup_this_page += 1\n",
    "                    continue\n",
    "                seen_urls.add(u)\n",
    "                all_rows.append(row)\n",
    "                added_this_page += 1\n",
    "\n",
    "            after = len(seen_urls)\n",
    "            logger.info(f\"  Baru: {added_this_page} | Duplikat: {dup_this_page} | Total unik {after}.\")\n",
    "\n",
    "            # === KUNCI: break jika halaman ini tidak menambah apa-apa (indikasi batas nyata tercapai) ===\n",
    "            if added_this_page == 0:\n",
    "                logger.info(\"  Tidak ada URL baru pada halaman ini (konten berulang). Stop iterasi tanggal ini.\")\n",
    "                break\n",
    "\n",
    "            time.sleep(1)\n",
    "\n",
    "        if not all_rows:\n",
    "            logger.warning(f\"Tidak ada data untuk tanggal {date_str}\")\n",
    "            return pd.DataFrame(columns=[\"judul_berita\",\"tanggal_berita\",\"penulis_berita\",\"url_berita\",\"relevan\",\"keywords_found\"])\n",
    "\n",
    "        df = pd.DataFrame(all_rows, columns=[\"judul_berita\",\"tanggal_berita\",\"penulis_berita\",\"url_berita\",\"relevan\",\"keywords_found\"])\n",
    "        return df\n",
    "\n",
    "    # -------------------- Scrape banyak tanggal --------------------\n",
    "    def scrape_many(self, dates: list[str], max_pages: int, topic_keywords: list[str]) -> pd.DataFrame:\n",
    "        frames = []\n",
    "        for d in dates:\n",
    "            try:\n",
    "                df_d = self.scrape_date(d, max_pages, topic_keywords)\n",
    "                frames.append(df_d)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Gagal memproses tanggal {d}: {e}\")\n",
    "        if not frames:\n",
    "            return pd.DataFrame(columns=[\"judul_berita\",\"tanggal_berita\",\"penulis_berita\",\"url_berita\",\"relevan\",\"keywords_found\"])\n",
    "        out = pd.concat(frames, ignore_index=True)\n",
    "        # Tetap drop duplikat antar tanggal (kadang lintas kanal sama)\n",
    "        out = out.drop_duplicates(subset=[\"url_berita\"]).reset_index(drop=True)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 543
    },
    "executionInfo": {
     "elapsed": 25176,
     "status": "ok",
     "timestamp": 1758171499842,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "C9jDUyeUF3oZ",
    "outputId": "128e4b45-7ac7-4fb4-8e1e-b8137588dca2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:53:07 | INFO | Proses tanggal 2025-09-24 | Halaman 1 | URL: https://news.detik.com/berita/indeks?page=1&date=09/24/2025\n",
      "07:53:08 | INFO |   Ditemukan 168 item artikel pada halaman ini.\n",
      "07:53:08 | INFO |   Baru: 141 | Duplikat: 26 | Total unik 141.\n",
      "07:53:09 | INFO | Proses tanggal 2025-09-24 | Halaman 2 | URL: https://news.detik.com/berita/indeks?page=2&date=09/24/2025\n",
      "07:53:09 | INFO |   Ditemukan 168 item artikel pada halaman ini.\n",
      "07:53:09 | INFO |   Baru: 0 | Duplikat: 167 | Total unik 141.\n",
      "07:53:09 | INFO |   Tidak ada URL baru pada halaman ini (konten berulang). Stop iterasi tanggal ini.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>judul_berita</th>\n",
       "      <th>tanggal_berita</th>\n",
       "      <th>penulis_berita</th>\n",
       "      <th>url_berita</th>\n",
       "      <th>relevan</th>\n",
       "      <th>keywords_found</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Prabowo Bertemu Presiden FIFA di AS, Perkuat K...</td>\n",
       "      <td>Rabu, 24 Sep 2025 23:44 WIB</td>\n",
       "      <td>Tidak Diketahui</td>\n",
       "      <td>https://news.detik.com/berita/d-8129051/prabow...</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wawalkot Tangsel Jawab Kritikan Leony soal Ang...</td>\n",
       "      <td>Rabu, 24 Sep 2025 23:40 WIB</td>\n",
       "      <td>Tidak Diketahui</td>\n",
       "      <td>https://news.detik.com/berita/d-8129050/wawalk...</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Penjelasan Jasa Marga soal Macet Parah di Jaka...</td>\n",
       "      <td>Rabu, 24 Sep 2025 23:24 WIB</td>\n",
       "      <td>Tidak Diketahui</td>\n",
       "      <td>https://news.detik.com/berita/d-8129043/penjel...</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mendes Minta 2 Desa di Bogor Tak Dilelang: Mer...</td>\n",
       "      <td>Rabu, 24 Sep 2025 23:13 WIB</td>\n",
       "      <td>Tidak Diketahui</td>\n",
       "      <td>https://news.detik.com/berita/d-8129026/mendes...</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Buka IBF 2025, Fadli Zon Tegaskan Pentingnya L...</td>\n",
       "      <td>Rabu, 24 Sep 2025 23:03 WIB</td>\n",
       "      <td>Tidak Diketahui</td>\n",
       "      <td>https://news.detik.com/berita/d-8129016/buka-i...</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Disclaimer</td>\n",
       "      <td>2025-09-24</td>\n",
       "      <td>Tidak Diketahui</td>\n",
       "      <td>https://www.detik.com/disclaimer</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Insertlive</td>\n",
       "      <td>2025-09-24</td>\n",
       "      <td>Tidak Diketahui</td>\n",
       "      <td>https://www.insertlive.com/</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>Beautynesia</td>\n",
       "      <td>2025-09-24</td>\n",
       "      <td>Tidak Diketahui</td>\n",
       "      <td>https://beautynesia.id</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>Female Daily</td>\n",
       "      <td>2025-09-24</td>\n",
       "      <td>Tidak Diketahui</td>\n",
       "      <td>https://femaledaily.com</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>CXO Media</td>\n",
       "      <td>2025-09-24</td>\n",
       "      <td>Tidak Diketahui</td>\n",
       "      <td>https://www.cxomedia.id</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          judul_berita  \\\n",
       "0    Prabowo Bertemu Presiden FIFA di AS, Perkuat K...   \n",
       "1    Wawalkot Tangsel Jawab Kritikan Leony soal Ang...   \n",
       "2    Penjelasan Jasa Marga soal Macet Parah di Jaka...   \n",
       "3    Mendes Minta 2 Desa di Bogor Tak Dilelang: Mer...   \n",
       "4    Buka IBF 2025, Fadli Zon Tegaskan Pentingnya L...   \n",
       "..                                                 ...   \n",
       "136                                         Disclaimer   \n",
       "137                                         Insertlive   \n",
       "138                                        Beautynesia   \n",
       "139                                       Female Daily   \n",
       "140                                          CXO Media   \n",
       "\n",
       "                  tanggal_berita   penulis_berita  \\\n",
       "0    Rabu, 24 Sep 2025 23:44 WIB  Tidak Diketahui   \n",
       "1    Rabu, 24 Sep 2025 23:40 WIB  Tidak Diketahui   \n",
       "2    Rabu, 24 Sep 2025 23:24 WIB  Tidak Diketahui   \n",
       "3    Rabu, 24 Sep 2025 23:13 WIB  Tidak Diketahui   \n",
       "4    Rabu, 24 Sep 2025 23:03 WIB  Tidak Diketahui   \n",
       "..                           ...              ...   \n",
       "136                   2025-09-24  Tidak Diketahui   \n",
       "137                   2025-09-24  Tidak Diketahui   \n",
       "138                   2025-09-24  Tidak Diketahui   \n",
       "139                   2025-09-24  Tidak Diketahui   \n",
       "140                   2025-09-24  Tidak Diketahui   \n",
       "\n",
       "                                            url_berita  relevan keywords_found  \n",
       "0    https://news.detik.com/berita/d-8129051/prabow...    False                 \n",
       "1    https://news.detik.com/berita/d-8129050/wawalk...    False                 \n",
       "2    https://news.detik.com/berita/d-8129043/penjel...    False                 \n",
       "3    https://news.detik.com/berita/d-8129026/mendes...    False                 \n",
       "4    https://news.detik.com/berita/d-8129016/buka-i...    False                 \n",
       "..                                                 ...      ...            ...  \n",
       "136                   https://www.detik.com/disclaimer    False                 \n",
       "137                        https://www.insertlive.com/    False                 \n",
       "138                             https://beautynesia.id    False                 \n",
       "139                            https://femaledaily.com    False                 \n",
       "140                            https://www.cxomedia.id    False                 \n",
       "\n",
       "[141 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Ringkasan ---\n",
      "Total artikel: 141\n",
      "Artikel relevan: 0\n",
      "Tidak ada judul relevan untuk ditampilkan.\n"
     ]
    }
   ],
   "source": [
    "# === Sel 4: Eksekusi Scraping & Ringkasan ===\n",
    "\n",
    "# %%\n",
    "scraper = DetikNewsScraper(timeout=10)\n",
    "df = scraper.scrape_many(dates=dates, max_pages=max_pages_per_date, topic_keywords=topic_keywords)\n",
    "\n",
    "# Tampilkan DataFrame (kolom harus persis sesuai spesifikasi)\n",
    "expected_cols = [\"judul_berita\",\"tanggal_berita\",\"penulis_berita\",\"url_berita\",\"relevan\",\"keywords_found\"]\n",
    "df = df.reindex(columns=expected_cols) if not df.empty else pd.DataFrame(columns=expected_cols)\n",
    "display(df)\n",
    "\n",
    "# Ringkasan\n",
    "total = len(df)\n",
    "relevant_count = int(df[\"relevan\"].sum()) if not df.empty else 0\n",
    "print(\"\\n--- Ringkasan ---\")\n",
    "print(f\"Total artikel: {total}\")\n",
    "print(f\"Artikel relevan: {relevant_count}\")\n",
    "\n",
    "if relevant_count > 0:\n",
    "    contoh = df[df[\"relevan\"]].head(5)[\"judul_berita\"].tolist()\n",
    "    print(\"5 contoh judul relevan:\")\n",
    "    for i, j in enumerate(contoh, 1):\n",
    "        print(f\"{i}. {j}\")\n",
    "else:\n",
    "    print(\"Tidak ada judul relevan untuk ditampilkan.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7kvcowCWqOgl"
   },
   "outputs": [],
   "source": [
    "df.to_excel(cwd + f\"/daftar_berita/detik_index.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOq6i2kuTjif+e2IwAIo6gG",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
