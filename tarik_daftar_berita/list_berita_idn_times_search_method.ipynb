{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1776,
     "status": "ok",
     "timestamp": 1758094211068,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "972eE2voiEGz",
    "outputId": "495712ca-7967-4ff6-9e53-9aa0c962ec6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from google.colab import drive\\ndrive.mount('/content/drive')\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mount the colab with google drive\n",
    "\"\"\"from google.colab import drive\n",
    "drive.mount('/content/drive')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "o3dmlHyCi6-E"
   },
   "outputs": [],
   "source": [
    "# set folder tempat kerja (current working directory)\n",
    "import os\n",
    "# cwd = '/content/drive/MyDrive/Monitoring Berita'\n",
    "cwd = '/Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Pekerjaan BMN/05. 2025/98_monitoring_berita'\n",
    "os.chdir(cwd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vWSdWYJlilxS"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Scrape daftar pemberitaan IDN Times dari halaman pencarian:\n",
    "https://www.idntimes.com/search?q=<topik>\n",
    "\n",
    "Keluaran: pandas DataFrame dengan kolom:\n",
    "['judul_berita', 'tanggal_berita', 'penulis_berita', 'url_berita']\n",
    "\n",
    "Prasyarat:\n",
    "pip install requests beautifulsoup4 pandas lxml\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "from typing import List, Dict, Optional\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# =======================\n",
    "# Konfigurasi umum\n",
    "# =======================\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/127.0 Safari/537.36\"\n",
    "    ),\n",
    "    \"Accept-Language\": \"id,en;q=0.9\",\n",
    "}\n",
    "TIMEOUT = 12  # detik\n",
    "RETRY = 2     # jumlah retry per request\n",
    "SLEEP_BETWEEN_REQUESTS = 0.5  # detik - kurangi beban server\n",
    "\n",
    "# Pola tanggal seperti \"09 Sep 2025, 22:13 WIB\" (fallback bila meta tak ada)\n",
    "DATE_REGEX = re.compile(\n",
    "    r\"\\b(\\d{1,2}\\s(?:Jan|Feb|Mar|Apr|Mei|Jun|Jul|Agu|Sep|Okt|Nov|Des)\\s\\d{4},\\s\\d{2}:\\d{2}\\sWIB)\\b\",\n",
    "    flags=re.IGNORECASE,\n",
    ")\n",
    "\n",
    "# =======================\n",
    "# Utilitas HTTP\n",
    "# =======================\n",
    "def http_get(url: str) -> Optional[requests.Response]:\n",
    "    \"\"\"GET dengan retry & timeout. Return Response atau None jika gagal total.\"\"\"\n",
    "    last_err = None\n",
    "    for attempt in range(1, RETRY + 2):\n",
    "        try:\n",
    "            resp = requests.get(url, headers=HEADERS, timeout=TIMEOUT)\n",
    "            # IDN kadang redirect ke App/banner; kita tetap terima 200 saja\n",
    "            if resp.status_code == 200:\n",
    "                return resp\n",
    "            last_err = f\"HTTP {resp.status_code}\"\n",
    "        except requests.RequestException as e:\n",
    "            last_err = str(e)\n",
    "        # tunggu sebelum retry\n",
    "        time.sleep(0.6 * attempt)\n",
    "    logging.warning(\"Gagal GET %s: %s\", url, last_err)\n",
    "    return None\n",
    "\n",
    "# =======================\n",
    "# Parsing hasil pencarian\n",
    "# =======================\n",
    "def extract_article_links_from_search(html: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Ambil link artikel dari HTML pencarian.\n",
    "    Filter: hanya domain idntimes.com dan path yang mengarah ke artikel (ada kategori).\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    links = set()\n",
    "\n",
    "    # Heuristik: link artikel biasanya memiliki pola /<kategori>/<subkategori>/<slug>...\n",
    "    # dan bukan link ke ekosistem lain (Popbela, Duniaku, dll).\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        if not href.startswith(\"https://www.idntimes.com/\"):\n",
    "            continue\n",
    "        # Buang halaman yang jelas bukan artikel (homepage, tag, search, sitemap, dsb.)\n",
    "        if any(bad in href for bad in [\"/search\", \"/tag/\", \"/sitemap\", \"/index\", \"/about\", \"/contact\"]):\n",
    "            continue\n",
    "        # Pastikan ada kategori di path (mis. /business/economy/..., /news/...)\n",
    "        # Contoh kategori utama IDN: business, news, life, hype, sport, tech, travel, dll.\n",
    "        path = href.replace(\"https://www.idntimes.com\", \"\")\n",
    "        if path.count(\"/\") >= 3:\n",
    "            links.add(href)\n",
    "\n",
    "    return list(links)\n",
    "\n",
    "def paginate_search_urls(query: str, max_pages: int = 1) -> List[str]:\n",
    "    \"\"\"\n",
    "    Bentuk daftar URL pencarian. IDN Times biasanya mendukung ?q=<term>&page=<n>.\n",
    "    Jika struktur berubah, tetap aman (hanya halaman 1 yang dipakai).\n",
    "    \"\"\"\n",
    "    q = quote_plus(query.strip())\n",
    "    base = f\"https://www.idntimes.com/search?q={q}\"\n",
    "    urls = [base]\n",
    "    for p in range(2, max_pages + 1):\n",
    "        urls.append(f\"{base}&page={p}\")\n",
    "    return urls\n",
    "\n",
    "# =======================\n",
    "# Parsing halaman artikel\n",
    "# =======================\n",
    "def parse_article_page(url: str) -> Dict[str, Optional[str]]:\n",
    "    \"\"\"\n",
    "    Ambil judul, tanggal, penulis dari halaman artikel.\n",
    "    Strategi:\n",
    "      1) Coba cari dari elemen visible: <h1>, teks tanggal, penulis dekat H1.\n",
    "      2) Fallback cari dari meta tag umum: <meta name=\"author\">,\n",
    "         <meta property=\"article:published_time\">, JSON-LD jika tersedia.\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        \"judul_berita\": None,\n",
    "        \"tanggal_berita\": None,\n",
    "        \"penulis_berita\": None,\n",
    "        \"url_berita\": url,\n",
    "        \"error\": None,  # simpan error untuk logging (tidak ikut ke DataFrame final)\n",
    "    }\n",
    "\n",
    "    resp = http_get(url)\n",
    "    if not resp:\n",
    "        data[\"error\"] = \"Gagal memuat artikel\"\n",
    "        return data\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "\n",
    "    # --- Judul (H1) ---\n",
    "    h1 = soup.find(\"h1\")\n",
    "    if h1 and h1.get_text(strip=True):\n",
    "        data[\"judul_berita\"] = h1.get_text(strip=True)\n",
    "\n",
    "    # --- Tanggal (visible text dekat header) ---\n",
    "    # Cari pola tanggal standar yang sering tampil di dekat judul.\n",
    "    m = DATE_REGEX.search(soup.get_text(\" \", strip=True))\n",
    "    if m:\n",
    "        data[\"tanggal_berita\"] = m.group(1)\n",
    "\n",
    "    # --- Penulis (sering berupa teks 'Ridwan Aji Pitoko' dekat header) ---\n",
    "    # Heuristik: cari anchor/span yang mirip 'author' di sekitar H1.\n",
    "    author = None\n",
    "\n",
    "    # 1) Meta name=author\n",
    "    meta_author = soup.find(\"meta\", attrs={\"name\": \"author\"})\n",
    "    if meta_author and meta_author.get(\"content\"):\n",
    "        author = meta_author[\"content\"].strip()\n",
    "\n",
    "    # 2) Cari link/teks berulang yang tampak seperti nama penulis (huruf & spasi)\n",
    "    if not author:\n",
    "        # Banyak halaman meletakkan penulis dalam anchor/spans dekat judul\n",
    "        header_block = h1.find_parent() if h1 else None\n",
    "        candidates = []\n",
    "        if header_block:\n",
    "            candidates = header_block.find_all([\"a\", \"span\"], string=True)\n",
    "        else:\n",
    "            candidates = soup.find_all([\"a\", \"span\"], string=True)\n",
    "\n",
    "        for el in candidates:\n",
    "            text = el.get_text(strip=True)\n",
    "            # heuristik sederhana: 2-4 kata, huruf/karakter nama\n",
    "            if 2 <= len(text.split()) <= 5 and re.match(r\"^[A-Za-zÀ-ÖØ-öø-ÿ'.\\-\\s]+$\", text):\n",
    "                # Hindari kata kategori umum (Business, News, Life, dsb.)\n",
    "                if text.lower() not in {\"home\", \"news\", \"business\", \"economy\", \"life\", \"sport\", \"tech\", \"travel\"}:\n",
    "                    author = text\n",
    "                    break\n",
    "\n",
    "    # 3) Fallback: cari schema.org JSON-LD bila ada\n",
    "    if not author or not data[\"tanggal_berita\"]:\n",
    "        for script in soup.find_all(\"script\", type=\"application/ld+json\"):\n",
    "            try:\n",
    "                import json\n",
    "                j = json.loads(script.string or \"{}\")\n",
    "                # Bisa berupa dict atau list\n",
    "                objs = j if isinstance(j, list) else [j]\n",
    "                for obj in objs:\n",
    "                    if not isinstance(obj, dict):\n",
    "                        continue\n",
    "                    if not author:\n",
    "                        a = obj.get(\"author\")\n",
    "                        if isinstance(a, dict) and a.get(\"name\"):\n",
    "                            author = a[\"name\"]\n",
    "                        elif isinstance(a, list) and a and isinstance(a[0], dict) and a[0].get(\"name\"):\n",
    "                            author = a[0][\"name\"]\n",
    "                    if not data[\"tanggal_berita\"]:\n",
    "                        dp = obj.get(\"datePublished\") or obj.get(\"dateCreated\")\n",
    "                        if dp:\n",
    "                            data[\"tanggal_berita\"] = dp\n",
    "            except Exception:\n",
    "                # Abaikan JSON-LD yang tidak valid\n",
    "                pass\n",
    "\n",
    "    # 4) Meta OpenGraph waktu publish\n",
    "    if not data[\"tanggal_berita\"]:\n",
    "        og_pub = soup.find(\"meta\", attrs={\"property\": \"article:published_time\"})\n",
    "        if og_pub and og_pub.get(\"content\"):\n",
    "            data[\"tanggal_berita\"] = og_pub[\"content\"].strip()\n",
    "\n",
    "    data[\"penulis_berita\"] = author\n",
    "    # Validasi minimal\n",
    "    if not data[\"judul_berita\"]:\n",
    "        data[\"error\"] = \"Judul tidak ditemukan\"\n",
    "    return data\n",
    "\n",
    "# =======================\n",
    "# Pipeline utama\n",
    "# =======================\n",
    "def scrape_idntimes_search(topic: str, max_pages: int = 1, limit: Optional[int] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ambil daftar artikel untuk 'topic' dari halaman pencarian IDN Times.\n",
    "    - max_pages: jumlah halaman pencarian yang dipindai (default 1).\n",
    "    - limit: batasi jumlah artikel yang diproses (None = semua).\n",
    "    \"\"\"\n",
    "    if not topic or not topic.strip():\n",
    "        raise ValueError(\"Parameter 'topic' tidak boleh kosong.\")\n",
    "\n",
    "    search_urls = paginate_search_urls(topic, max_pages=max_pages)\n",
    "    all_links: List[str] = []\n",
    "\n",
    "    for su in search_urls:\n",
    "        resp = http_get(su)\n",
    "        if not resp:\n",
    "            logging.warning(\"Lewati halaman pencarian (gagal dimuat): %s\", su)\n",
    "            continue\n",
    "        links = extract_article_links_from_search(resp.text)\n",
    "        # Hindari duplikat & pertahankan urutan relatif\n",
    "        for lk in links:\n",
    "            if lk not in all_links:\n",
    "                all_links.append(lk)\n",
    "        time.sleep(SLEEP_BETWEEN_REQUESTS)\n",
    "\n",
    "    if not all_links:\n",
    "        logging.warning(\"Tidak ada link artikel ditemukan untuk topik '%s'.\", topic)\n",
    "\n",
    "    if limit is not None:\n",
    "        all_links = all_links[:max(0, int(limit))]\n",
    "\n",
    "    rows: List[Dict] = []\n",
    "    for i, url in enumerate(all_links, 1):\n",
    "        try:\n",
    "            row = parse_article_page(url)\n",
    "            if row.get(\"error\"):\n",
    "                logging.warning(\"Artikel bermasalah (%s): %s\", row[\"error\"], url)\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"judul_berita\": row.get(\"judul_berita\"),\n",
    "                    \"tanggal_berita\": row.get(\"tanggal_berita\"),\n",
    "                    \"penulis_berita\": row.get(\"penulis_berita\"),\n",
    "                    \"url_berita\": row.get(\"url_berita\"),\n",
    "                }\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.exception(\"Gagal memproses artikel #%d: %s | err=%s\", i, url, e)\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"judul_berita\": None,\n",
    "                    \"tanggal_berita\": None,\n",
    "                    \"penulis_berita\": None,\n",
    "                    \"url_berita\": url,\n",
    "                }\n",
    "            )\n",
    "        time.sleep(SLEEP_BETWEEN_REQUESTS)\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"judul_berita\", \"tanggal_berita\", \"penulis_berita\", \"url_berita\"])\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9k9FBHBknQBn"
   },
   "outputs": [],
   "source": [
    "# --- Sel 2: Parameter (mudah diubah) ---\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"config.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Kata kunci topik untuk analisis relevansi judul\n",
    "topic_keywords = config[\"keywords\"]\n",
    "\n",
    "# Daftar tanggal (YYYY-MM-DD). Akan di-convert ke DD-MM-YYYY untuk pencocokan di halaman.\n",
    "dates = config[\"search_date\"]\n",
    "\n",
    "# Maksimum halaman per tanggal (akan berhenti lebih awal jika halaman kosong)\n",
    "max_pages_per_date = config[\"max_page_length\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bF9v75-DiwEZ"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "    )\n",
    "\n",
    "    df_list = []\n",
    "    for topik in topic_keywords:\n",
    "        df1 = scrape_idntimes_search(topik, max_pages=3, limit=50)\n",
    "        df_list.append(df1)\n",
    "\n",
    "    df = pd.concat(df_list)\n",
    "    date = pd.Timestamp.now().strftime(\"%Y%m%d\")\n",
    "    df.to_excel(cwd + f\"/daftar_berita/idn_times.xlsx\", index=False)\n",
    "    # Simpan kalau perlu:\n",
    "    # df.to_csv(f\"idntimes_{topik}.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNelSsPAjGNdOSk8KpKCZ1v",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
