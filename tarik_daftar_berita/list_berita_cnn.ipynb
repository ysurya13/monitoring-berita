{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nl15yqDXsRcB"
   },
   "source": [
    "# SETTING ENVIRONMENT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27633,
     "status": "ok",
     "timestamp": 1758676480067,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "f8BqutX0SOd1",
    "outputId": "c3e7af85-08a1-409d-a9a5-53dc4efe08f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# mount the colab with google drive\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# mount the colab with google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1758676480314,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "eLOguPRQSRLN"
   },
   "outputs": [],
   "source": [
    "# set folder tempat kerja (current working directory)\n",
    "import os\n",
    "# cwd = '/content/drive/MyDrive/Monitoring Berita'\n",
    "cwd = '/Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Pekerjaan BMN/05. 2025/98_monitoring_berita'\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2Ya6gVVSHYl"
   },
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "executionInfo": {
     "elapsed": 17093,
     "status": "ok",
     "timestamp": 1758691998207,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "zya6CDYGWeUI",
    "outputId": "6b6f169b-3670-4a46-fc62-ffed29a7ded5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-26 07:45:15,071 | INFO | Project root: /Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Pekerjaan BMN/05. 2025/98_monitoring_berita\n",
      "2025-09-26 07:45:15,071 | INFO | Keywords: ['purbaya', 'menkeu', 'banggar', 'apbn']\n",
      "2025-09-26 07:45:15,072 | INFO | Using TAG mode for: purbaya\n",
      "2025-09-26 07:45:15,071 | INFO | Keywords: ['purbaya', 'menkeu', 'banggar', 'apbn']\n",
      "2025-09-26 07:45:15,072 | INFO | Using TAG mode for: purbaya\n",
      "2025-09-26 07:45:19,885 | INFO | [tag:purbaya] page 1 -> 36 items\n",
      "2025-09-26 07:45:19,885 | INFO | [tag:purbaya] page 1 -> 36 items\n",
      "2025-09-26 07:45:24,642 | INFO | [tag:purbaya] page 2 -> 34 items\n",
      "2025-09-26 07:45:24,642 | INFO | [tag:purbaya] page 2 -> 34 items\n",
      "2025-09-26 07:45:29,090 | INFO | [tag:purbaya] page 3 -> 28 items\n",
      "2025-09-26 07:45:29,090 | INFO | [tag:purbaya] page 3 -> 28 items\n",
      "2025-09-26 07:45:30,677 | INFO | Using TAG mode for: menkeu\n",
      "2025-09-26 07:45:30,677 | INFO | Using TAG mode for: menkeu\n",
      "2025-09-26 07:45:34,558 | INFO | [tag:menkeu] page 1 -> 34 items\n",
      "2025-09-26 07:45:34,558 | INFO | [tag:menkeu] page 1 -> 34 items\n",
      "2025-09-26 07:45:38,882 | INFO | [tag:menkeu] page 2 -> 34 items\n",
      "2025-09-26 07:45:38,882 | INFO | [tag:menkeu] page 2 -> 34 items\n",
      "2025-09-26 07:45:44,038 | INFO | [tag:menkeu] page 3 -> 36 items\n",
      "2025-09-26 07:45:44,038 | INFO | [tag:menkeu] page 3 -> 36 items\n",
      "2025-09-26 07:45:45,382 | INFO | Using TAG mode for: banggar\n",
      "2025-09-26 07:45:45,382 | INFO | Using TAG mode for: banggar\n",
      "2025-09-26 07:45:49,245 | INFO | [tag:banggar] page 1 -> 34 items\n",
      "2025-09-26 07:45:49,245 | INFO | [tag:banggar] page 1 -> 34 items\n",
      "2025-09-26 07:45:54,370 | INFO | [tag:banggar] page 2 -> 34 items\n",
      "2025-09-26 07:45:54,370 | INFO | [tag:banggar] page 2 -> 34 items\n",
      "2025-09-26 07:45:59,956 | INFO | [tag:banggar] page 3 -> 34 items\n",
      "2025-09-26 07:45:59,956 | INFO | [tag:banggar] page 3 -> 34 items\n",
      "2025-09-26 07:46:01,365 | INFO | Using TAG mode for: apbn\n",
      "2025-09-26 07:46:01,365 | INFO | Using TAG mode for: apbn\n",
      "2025-09-26 07:46:05,514 | INFO | [tag:apbn] page 1 -> 36 items\n",
      "2025-09-26 07:46:05,514 | INFO | [tag:apbn] page 1 -> 36 items\n",
      "2025-09-26 07:46:10,085 | INFO | [tag:apbn] page 2 -> 34 items\n",
      "2025-09-26 07:46:10,085 | INFO | [tag:apbn] page 2 -> 34 items\n",
      "2025-09-26 07:46:15,335 | INFO | [tag:apbn] page 3 -> 34 items\n",
      "2025-09-26 07:46:15,335 | INFO | [tag:apbn] page 3 -> 34 items\n",
      "2025-09-26 07:46:16,263 | INFO | Total unique links: 137\n",
      "2025-09-26 07:46:16,269 | INFO | Saved: /Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Pekerjaan BMN/05. 2025/98_monitoring_berita/daftar_berita/cnn_links.csv\n",
      "2025-09-26 07:46:16,263 | INFO | Total unique links: 137\n",
      "2025-09-26 07:46:16,269 | INFO | Saved: /Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Pekerjaan BMN/05. 2025/98_monitoring_berita/daftar_berita/cnn_links.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>judul_berita</th>\n",
       "      <th>url_berita</th>\n",
       "      <th>keyword</th>\n",
       "      <th>page</th>\n",
       "      <th>published_at</th>\n",
       "      <th>scraped_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Keluar</td>\n",
       "      <td>https://connect.detik.com/oauth/signout?redire...</td>\n",
       "      <td>purbaya</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-09-26 07:45:15.469785+07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DAFTAR</td>\n",
       "      <td>https://connect.detik.com/accounts/register?cl...</td>\n",
       "      <td>purbaya</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-09-26 07:45:15.509931+07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Politik</td>\n",
       "      <td>https://www.cnnindonesia.com/nasional/politik</td>\n",
       "      <td>purbaya</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-09-26 07:45:15.656043+07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hukum &amp; Kriminal</td>\n",
       "      <td>https://www.cnnindonesia.com/nasional/hukum-kr...</td>\n",
       "      <td>purbaya</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-09-26 07:45:15.745989+07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Peristiwa</td>\n",
       "      <td>https://www.cnnindonesia.com/nasional/peristiwa</td>\n",
       "      <td>purbaya</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-09-26 07:45:15.910123+07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Pemilu</td>\n",
       "      <td>https://www.cnnindonesia.com/nasional/pemilu</td>\n",
       "      <td>purbaya</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-09-26 07:45:16.041971+07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Info Politik</td>\n",
       "      <td>https://www.cnnindonesia.com/nasional/info-pol...</td>\n",
       "      <td>purbaya</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-09-26 07:45:16.176766+07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Asia Pasifik</td>\n",
       "      <td>https://www.cnnindonesia.com/internasional/asi...</td>\n",
       "      <td>purbaya</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-09-26 07:45:16.280720+07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Timur Tengah</td>\n",
       "      <td>https://www.cnnindonesia.com/internasional/tim...</td>\n",
       "      <td>purbaya</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-09-26 07:45:16.348669+07:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Eropa Amerika</td>\n",
       "      <td>https://www.cnnindonesia.com/internasional/ero...</td>\n",
       "      <td>purbaya</td>\n",
       "      <td>1</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-09-26 07:45:16.480125+07:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       judul_berita                                         url_berita  \\\n",
       "0            Keluar  https://connect.detik.com/oauth/signout?redire...   \n",
       "1            DAFTAR  https://connect.detik.com/accounts/register?cl...   \n",
       "2           Politik      https://www.cnnindonesia.com/nasional/politik   \n",
       "3  Hukum & Kriminal  https://www.cnnindonesia.com/nasional/hukum-kr...   \n",
       "4         Peristiwa    https://www.cnnindonesia.com/nasional/peristiwa   \n",
       "5            Pemilu       https://www.cnnindonesia.com/nasional/pemilu   \n",
       "6      Info Politik  https://www.cnnindonesia.com/nasional/info-pol...   \n",
       "7      Asia Pasifik  https://www.cnnindonesia.com/internasional/asi...   \n",
       "8      Timur Tengah  https://www.cnnindonesia.com/internasional/tim...   \n",
       "9     Eropa Amerika  https://www.cnnindonesia.com/internasional/ero...   \n",
       "\n",
       "   keyword  page published_at                       scraped_at  \n",
       "0  purbaya     1          NaT 2025-09-26 07:45:15.469785+07:00  \n",
       "1  purbaya     1          NaT 2025-09-26 07:45:15.509931+07:00  \n",
       "2  purbaya     1          NaT 2025-09-26 07:45:15.656043+07:00  \n",
       "3  purbaya     1          NaT 2025-09-26 07:45:15.745989+07:00  \n",
       "4  purbaya     1          NaT 2025-09-26 07:45:15.910123+07:00  \n",
       "5  purbaya     1          NaT 2025-09-26 07:45:16.041971+07:00  \n",
       "6  purbaya     1          NaT 2025-09-26 07:45:16.176766+07:00  \n",
       "7  purbaya     1          NaT 2025-09-26 07:45:16.280720+07:00  \n",
       "8  purbaya     1          NaT 2025-09-26 07:45:16.348669+07:00  \n",
       "9  purbaya     1          NaT 2025-09-26 07:45:16.480125+07:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Excel: /Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Pekerjaan BMN/05. 2025/98_monitoring_berita/daftar_berita/cnn_links.csv\n"
     ]
    }
   ],
   "source": [
    "# CNN Indonesia - Scraper (Tag-pages first, Search via Selenium fallback)\n",
    "# - Reads keywords from config.json['keywords']\n",
    "# - For each keyword: try TAG mode (recommended). If you insist on search, set FORCE_SEARCH=True.\n",
    "# - Saves Excel to .../daftar_berita/cnn (auto-detected project root)\n",
    "\n",
    "# %%\n",
    "import os, re, json, time, random, logging\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# --- Auto-detect project root containing config.json ---\n",
    "def locate_project_root(target_file: str = \"config.json\", max_up: int = 4) -> Optional[str]:\n",
    "    # Priority 1: Colab path\n",
    "    colab_root = \"/content/drive/MyDrive/Monitoring Berita\"\n",
    "    if os.path.exists(os.path.join(colab_root, target_file)):\n",
    "        return colab_root\n",
    "    # Priority 2: current working directory or its parents\n",
    "    cwd = os.getcwd()\n",
    "    cur = cwd\n",
    "    for _ in range(max_up):\n",
    "        if os.path.exists(os.path.join(cur, target_file)):\n",
    "            return cur\n",
    "        parent = os.path.dirname(cur)\n",
    "        if parent == cur:\n",
    "            break\n",
    "        cur = parent\n",
    "    return None\n",
    "\n",
    "PROJECT_ROOT = locate_project_root() or os.getcwd()\n",
    "CONFIG_PATH = os.path.join(PROJECT_ROOT, \"config.json\")\n",
    "OUTPUT_DIR  = os.path.join(PROJECT_ROOT, \"daftar_berita\")\n",
    "\n",
    "MAX_PAGES   = 3      # batas iterasi halaman\n",
    "FORCE_SEARCH = False # True = pakai search ?query=... via Selenium fallback\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
    "logger = logging.getLogger(\"cnn-fix\")\n",
    "\n",
    "UA_POOL = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/18.6 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Linux; Android 13; SM-S908E) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Mobile Safari/537.36\",\n",
    "]\n",
    "\n",
    "def pick_ua(): return random.choice(UA_POOL)\n",
    "\n",
    "def sess():\n",
    "    s = requests.Session()\n",
    "    s.headers.update({\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"id,en-US;q=0.9,en;q=0.8\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"DNT\": \"1\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    })\n",
    "    return s\n",
    "\n",
    "def http_get(s: requests.Session, url: str, tries=3, backoff=1.5, timeout=25):\n",
    "    last = None\n",
    "    for i in range(1, tries+1):\n",
    "        try:\n",
    "            s.headers[\"User-Agent\"] = pick_ua()\n",
    "            r = s.get(url, timeout=timeout)\n",
    "            if r.status_code == 200 and len(r.text) > 1000:\n",
    "                return r\n",
    "            last = f\"HTTP {r.status_code}, len={len(r.text)}\"\n",
    "        except Exception as e:\n",
    "            last = str(e)\n",
    "        time.sleep(backoff ** i)\n",
    "    logger.warning(f\"GET fail {url} -> {last}\")\n",
    "    return None\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def load_keywords(path: str) -> List[str]:\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            cfg = json.load(f)\n",
    "        ks = cfg.get(\"keywords\") or []\n",
    "        ks = [str(k).strip() for k in ks if str(k).strip()]\n",
    "        if ks: return ks\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"load_keywords: {e}\")\n",
    "    return [\"menkeu\"]\n",
    "\n",
    "def slugify_for_tag(term: str) -> str:\n",
    "    # cnn tag uses lowercase, spaces -> hyphen, strip non-word except hyphen\n",
    "    t = term.strip().lower()\n",
    "    t = re.sub(r\"\\s+\", \"-\", t)\n",
    "    t = re.sub(r\"[^a-z0-9\\-]+\", \"\", t)\n",
    "    return t\n",
    "\n",
    "# ---------- Published date extraction ----------\n",
    "def extract_published_at_from_html(html: str) -> Optional[pd.Timestamp]:\n",
    "    \"\"\"\n",
    "    Try multiple strategies to get article publish datetime:\n",
    "    1) JSON-LD (NewsArticle) datePublished\n",
    "    2) <meta property=\"article:published_time\" content=\"...\">\n",
    "    3) <meta itemprop=\"datePublished\" content=\"...\"> or <time datetime=\"...\">\n",
    "    Returns timezone-aware Asia/Jakarta timestamp when possible.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    # 1) JSON-LD\n",
    "    for sc in soup.find_all(\"script\", attrs={\"type\": \"application/ld+json\"}):\n",
    "        txt = sc.string or sc.get_text(strip=True) or \"\"\n",
    "        if not txt: continue\n",
    "        try:\n",
    "            data = json.loads(txt)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        def find_date(obj):\n",
    "            if isinstance(obj, dict):\n",
    "                at = obj.get(\"@type\")\n",
    "                if at in (\"NewsArticle\", \"Article\", \"BlogPosting\"):\n",
    "                    dp = obj.get(\"datePublished\") or obj.get(\"dateCreated\")\n",
    "                    if dp:\n",
    "                        return dp\n",
    "                # search nested\n",
    "                for v in obj.values():\n",
    "                    found = find_date(v)\n",
    "                    if found:\n",
    "                        return found\n",
    "            elif isinstance(obj, list):\n",
    "                for v in obj:\n",
    "                    found = find_date(v)\n",
    "                    if found:\n",
    "                        return found\n",
    "            return None\n",
    "\n",
    "        ds = find_date(data)\n",
    "        if ds:\n",
    "            ts = _to_wib_timestamp(ds)\n",
    "            if ts is not None:\n",
    "                return ts\n",
    "\n",
    "    # 2) Meta tags\n",
    "    meta_candidates = [\n",
    "        (\"meta\", {\"property\": \"article:published_time\"}, \"content\"),\n",
    "        (\"meta\", {\"name\": \"pubdate\"}, \"content\"),\n",
    "        (\"meta\", {\"itemprop\": \"datePublished\"}, \"content\"),\n",
    "        (\"time\", {\"itemprop\": \"datePublished\"}, \"datetime\"),\n",
    "        (\"time\", {}, \"datetime\"),\n",
    "    ]\n",
    "    for tag, attrs, attrname in meta_candidates:\n",
    "        el = soup.find(tag, attrs=attrs)\n",
    "        if el and el.get(attrname):\n",
    "            ts = _to_wib_timestamp(el.get(attrname).strip())\n",
    "            if ts is not None:\n",
    "                return ts\n",
    "\n",
    "    # 3) Fallback: common date containers (best-effort)\n",
    "    for sel in [\".media__date\", \".date\", \".article__date\", \"time\"]:\n",
    "        for e in soup.select(sel):\n",
    "            raw = (e.get(\"datetime\") or e.get_text(\" \", strip=True) or \"\").strip()\n",
    "            if not raw:\n",
    "                continue\n",
    "            # handle WIB hint\n",
    "            if \"WIB\" in raw.upper():\n",
    "                raw = raw.replace(\"WIB\", \"+07:00\")\n",
    "            ts = _to_wib_timestamp(raw)\n",
    "            if ts is not None:\n",
    "                return ts\n",
    "\n",
    "    return None\n",
    "\n",
    "def _to_wib_timestamp(val: str) -> Optional[pd.Timestamp]:\n",
    "    try:\n",
    "        ts = pd.to_datetime(val, utc=True, errors=\"raise\")\n",
    "        # If parsed with timezone, convert to Asia/Jakarta\n",
    "        return ts.tz_convert(\"Asia/Jakarta\")\n",
    "    except Exception:\n",
    "        # Try non-UTC parse\n",
    "        ts2 = pd.to_datetime(val, errors=\"coerce\", dayfirst=True)\n",
    "        if ts2 is pd.NaT:\n",
    "            return None\n",
    "        # Assume WIB if no tz info\n",
    "        try:\n",
    "            return ts2.tz_localize(\"Asia/Jakarta\")\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def fetch_published_at(s: requests.Session, url: str) -> Optional[pd.Timestamp]:\n",
    "    r = http_get(s, url, tries=3)\n",
    "    if not r:\n",
    "        return None\n",
    "    return extract_published_at_from_html(r.text)\n",
    "\n",
    "def parse_tag_page(html: str) -> List[Dict]:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    items = []\n",
    "    # Grab all anchors that look like article links beneath the tag listing.\n",
    "    # Tag pages list cards; keep it heuristic but strict on domain and path depth\n",
    "    for a in soup.select(\"a[href]\"):\n",
    "        href = a.get(\"href\", \"\").strip()\n",
    "        if not href: continue\n",
    "        # absolute-ize\n",
    "        if href.startswith(\"//\"): href = \"https:\" + href\n",
    "        elif href.startswith(\"/\"): href = urljoin(\"https://www.cnnindonesia.com\", href)\n",
    "        # Filter to article-like paths, ruling out pure tag/section/search\n",
    "        p = urlparse(href).path\n",
    "        if \"/tag/\" in p or \"/search\" in p or p == \"/\" or p.count(\"/\") < 2:\n",
    "            continue\n",
    "        title = a.get(\"title\") or a.get_text(\" \", strip=True)\n",
    "        if title and len(title) > 5:\n",
    "            items.append({\"judul_berita\": title, \"url_berita\": href})\n",
    "    # dedup by url\n",
    "    seen, uniq = set(), []\n",
    "    for it in items:\n",
    "        if it[\"url_berita\"] in seen: continue\n",
    "        seen.add(it[\"url_berita\"])\n",
    "        uniq.append(it)\n",
    "    return uniq\n",
    "\n",
    "def crawl_tag(term: str, max_pages=3) -> pd.DataFrame:\n",
    "    s = sess()\n",
    "    slug = slugify_for_tag(term)\n",
    "    rows = []\n",
    "    for page in range(1, max_pages+1):\n",
    "        url = f\"https://www.cnnindonesia.com/tag/{slug}\"\n",
    "        # some tag pages support ?page=2, some infinite-scroll; we try param\n",
    "        if page > 1:\n",
    "            url = f\"{url}?page={page}\"\n",
    "        r = http_get(s, url)\n",
    "        if not r:\n",
    "            logger.warning(f\"[tag:{term}] no response page {page}\")\n",
    "            continue\n",
    "        items = parse_tag_page(r.text)\n",
    "        for it in items:\n",
    "            it[\"keyword\"] = term\n",
    "            it[\"page\"] = page\n",
    "            # Fetch published_at from article page (instead of scrape time)\n",
    "            pub = fetch_published_at(s, it[\"url_berita\"]) or pd.NaT\n",
    "            it[\"published_at\"] = pub\n",
    "            # Keep actual scrape timestamp separately\n",
    "            it[\"scraped_at\"] = pd.Timestamp.now(tz=\"Asia/Jakarta\")\n",
    "            rows.append(it)\n",
    "        logger.info(f\"[tag:{term}] page {page} -> {len(items)} items\")\n",
    "        time.sleep(random.uniform(0.8, 1.6))\n",
    "    cols = [\"judul_berita\",\"url_berita\",\"keyword\",\"page\",\"published_at\",\"scraped_at\"]\n",
    "    df = pd.DataFrame(rows) if rows else pd.DataFrame(columns=cols)\n",
    "    if not df.empty:\n",
    "        df = df.drop_duplicates(subset=[\"url_berita\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# ---------- Optional Search Fallback (Selenium) ----------\n",
    "def crawl_search_selenium(term: str, max_pages=3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Only use if you truly need the search ?query=â€¦ endpoint.\n",
    "    Uses undetected-chromedriver to load pages like a real browser.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import undetected_chromedriver as uc\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "    except Exception:\n",
    "        # Install on the fly (Colab-friendly)\n",
    "        import sys, subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"undetected-chromedriver\", \"selenium\"])\n",
    "        import undetected_chromedriver as uc\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "    opts = uc.ChromeOptions()\n",
    "    opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--disable-gpu\")\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    opts.add_argument(\"--window-size=1366,768\")\n",
    "    opts.add_argument(f\"--user-agent={pick_ua()}\")\n",
    "\n",
    "    driver = uc.Chrome(options=opts)\n",
    "    rows = []\n",
    "    try:\n",
    "        for page in range(1, max_pages+1):\n",
    "            url = f\"https://www.cnnindonesia.com/search?query={term}&result_type=latest&page={page}\"\n",
    "            driver.get(url)\n",
    "            # wait minimal content\n",
    "            WebDriverWait(driver, 15).until(lambda d: len(d.page_source) > 20000)\n",
    "            html = driver.page_source\n",
    "            items = parse_tag_page(html)  # reuse same robust anchor collector\n",
    "            # We'll fetch published_at with requests session for each item\n",
    "            s = sess()\n",
    "            for it in items:\n",
    "                it[\"keyword\"] = term\n",
    "                it[\"page\"] = page\n",
    "                pub = fetch_published_at(s, it[\"url_berita\"]) or pd.NaT\n",
    "                it[\"published_at\"] = pub\n",
    "                it[\"scraped_at\"] = pd.Timestamp.now(tz=\"Asia/Jakarta\")\n",
    "                rows.append(it)\n",
    "            logger.info(f\"[search:{term}] page {page} -> {len(items)} items\")\n",
    "            time.sleep(random.uniform(1.0, 2.0))\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    cols = [\"judul_berita\",\"url_berita\",\"keyword\",\"page\",\"published_at\",\"scraped_at\"]\n",
    "    df = pd.DataFrame(rows) if rows else pd.DataFrame(columns=cols)\n",
    "    if not df.empty:\n",
    "        df = df.drop_duplicates(subset=[\"url_berita\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# ---------- Run ----------\n",
    "def save_excel(df: pd.DataFrame, out_dir: str) -> str:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    fpath = os.path.join(out_dir, f\"cnn_links.csv\")\n",
    "    df.to_csv(fpath, index=False)\n",
    "    return fpath\n",
    "\n",
    "def main():\n",
    "    keywords = load_keywords(CONFIG_PATH)\n",
    "    logger.info(f\"Project root: {PROJECT_ROOT}\")\n",
    "    logger.info(f\"Keywords: {keywords}\")\n",
    "    dfs = []\n",
    "    for kw in keywords:\n",
    "        if FORCE_SEARCH:\n",
    "            logger.info(f\"Using SEARCH mode for: {kw}\")\n",
    "            df = crawl_search_selenium(kw, MAX_PAGES)\n",
    "        else:\n",
    "            logger.info(f\"Using TAG mode for: {kw}\")\n",
    "            df = crawl_tag(kw, MAX_PAGES)\n",
    "        dfs.append(df)\n",
    "    default_cols = [\"judul_berita\",\"url_berita\",\"keyword\",\"page\",\"published_at\",\"scraped_at\"]\n",
    "    df_all = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame(columns=default_cols)\n",
    "    df_all = df_all.drop_duplicates(subset=[\"url_berita\"]).reset_index(drop=True)\n",
    "    logger.info(f\"Total unique links: {len(df_all)}\")\n",
    "    out = save_excel(df_all, OUTPUT_DIR)\n",
    "    logger.info(f\"Saved: {out}\")\n",
    "    try:\n",
    "        display(df_all.head(10))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return out\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path = main()\n",
    "    print(\"Output Excel:\", path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1758691662400,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "0vmhjXqUWeRA"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(path)\n",
    "# Prefer published_at (from article page). If missing, fallback to scraped_at.\n",
    "df[\"tanggal_berita\"] = pd.to_datetime(df.get(\"published_at\", pd.NaT), errors=\"coerce\").fillna(\n",
    "    pd.to_datetime(df.get(\"scraped_at\", pd.NaT), errors=\"coerce\")\n",
    ")\n",
    "\n",
    "# Ensure timezone-naive for Excel (Excel cannot handle tz-aware datetimes)\n",
    "if pd.api.types.is_datetime64_any_dtype(df[\"tanggal_berita\"]):\n",
    "    # If any tz-aware, strip tz info after converting to Asia/Jakarta\n",
    "    try:\n",
    "        # Convert any timezone-aware values to Asia/Jakarta then drop tz\n",
    "        s = pd.to_datetime(df[\"tanggal_berita\"], errors=\"coerce\")\n",
    "        # If some are tz-aware, convert; otherwise this is a no-op\n",
    "        if getattr(s.dt.tz, \"zone\", None) is not None or s.dt.tz is not None:\n",
    "            s = s.dt.tz_convert(\"Asia/Jakarta\")\n",
    "        # Drop tz info\n",
    "        df[\"tanggal_berita\"] = s.dt.tz_localize(None)\n",
    "    except Exception:\n",
    "        # Fallback: force naive by localizing-none where possible\n",
    "        try:\n",
    "            df[\"tanggal_berita\"] = pd.to_datetime(df[\"tanggal_berita\"], errors=\"coerce\").dt.tz_localize(None)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# Keep columns tidy\n",
    "cols_order = [\n",
    "    c for c in [\"judul_berita\",\"url_berita\",\"keyword\",\"page\",\"tanggal_berita\",\"published_at\",\"scraped_at\"]\n",
    "    if c in df.columns\n",
    "]\n",
    "cols_order += [c for c in df.columns if c not in cols_order]\n",
    "df = df[cols_order]\n",
    "\n",
    "df.to_csv(path, index=False)\n",
    "df.to_excel(path.replace(\".csv\", \".xlsx\"), index=False)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPHIm4fcnEVwwZG0i6nwxj6",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
