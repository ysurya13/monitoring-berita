{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21274,
     "status": "ok",
     "timestamp": 1758171710284,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "uDfmt5xeu_h_",
    "outputId": "0bab6e86-56be-42bb-c0a9-49bb415c2d8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "\"\"\"# mount the colab with google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ghza0pyZvQEL"
   },
   "outputs": [],
   "source": [
    "# set folder tempat kerja (current working directory)\n",
    "import os\n",
    "\n",
    "# cwd = '/content/drive/MyDrive/Monitoring Berita'\n",
    "\n",
    "cwd = '/Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Pekerjaan BMN/05. 2025/98_monitoring_berita'\n",
    "\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wBfGQGrivRWt"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import unicodedata\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# =========================\n",
    "# Konfigurasi dasar\n",
    "# =========================\n",
    "TAG_BASE = \"https://www.detik.com/tag/\"\n",
    "ARTICLE_HOSTS = {\n",
    "    \"detik.com\",\"www.detik.com\",\n",
    "    \"news.detik.com\",\"finance.detik.com\",\"inet.detik.com\",\"hot.detik.com\",\n",
    "    \"health.detik.com\",\"food.detik.com\",\"travel.detik.com\",\"oto.detik.com\",\n",
    "    \"sport.detik.com\",\"sepakbola.detik.com\",\"wolipop.detik.com\",\"english.detik.com\"\n",
    "}\n",
    "\n",
    "UAS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/18.0 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\",\n",
    "]\n",
    "\n",
    "def headers():\n",
    "    return {\n",
    "        \"User-Agent\": random.choice(UAS),\n",
    "        \"Accept-Language\": \"id,en;q=0.8\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "    }\n",
    "\n",
    "def make_session():\n",
    "    s = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=0.5,\n",
    "        status_forcelist=(429,500,502,503,504),\n",
    "        allowed_methods=frozenset([\"GET\"])\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retries, pool_connections=10, pool_maxsize=10)\n",
    "    s.mount(\"http://\", adapter)\n",
    "    s.mount(\"https://\", adapter)\n",
    "    return s\n",
    "\n",
    "def slugify_tag(s: str) -> str:\n",
    "    # \"Kementerian Keuangan\" -> \"kementerian-keuangan\"\n",
    "    s = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\",\"ignore\").decode(\"ascii\")\n",
    "    s = re.sub(r\"[^a-zA-Z0-9]+\", \"-\", s.strip().lower())\n",
    "    s = re.sub(r\"-{2,}\", \"-\", s).strip(\"-\")\n",
    "    return s or \"berita\"\n",
    "\n",
    "def fetch_html(session, url, timeout=15):\n",
    "    try:\n",
    "        r = session.get(url, headers=headers(), timeout=timeout)\n",
    "        r.raise_for_status()\n",
    "        return r.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise RuntimeError(f\"Gagal fetch {url}: {e}\")\n",
    "\n",
    "# =========================\n",
    "# Ekstraksi detail artikel\n",
    "# =========================\n",
    "def parse_ld_json(soup):\n",
    "    best = {}\n",
    "    for tag in soup.find_all(\"script\", type=\"application/ld+json\"):\n",
    "        try:\n",
    "            text = tag.string or tag.text or \"\"\n",
    "            data = json.loads(text)\n",
    "            blocks = data if isinstance(data, list) else [data]\n",
    "            for b in blocks:\n",
    "                typ = (b.get(\"@type\") or \"\").lower()\n",
    "                if \"newsarticle\" in typ or \"article\" in typ:\n",
    "                    best.update(b)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return best\n",
    "\n",
    "def extract_author_from_ld(ld):\n",
    "    a = ld.get(\"author\")\n",
    "    if isinstance(a, str):\n",
    "        return a.strip()\n",
    "    if isinstance(a, dict):\n",
    "        return (a.get(\"name\") or \"\").strip() or None\n",
    "    if isinstance(a, list) and a:\n",
    "        first = a[0]\n",
    "        if isinstance(first, str):\n",
    "            return first.strip()\n",
    "        if isinstance(first, dict):\n",
    "            return (first.get(\"name\") or \"\").strip() or None\n",
    "    return None\n",
    "\n",
    "def extract_date_from_ld(ld):\n",
    "    for k in (\"datePublished\",\"dateCreated\",\"dateModified\"):\n",
    "        if ld.get(k):\n",
    "            return str(ld[k]).strip()\n",
    "    return None\n",
    "\n",
    "def extract_author_meta(soup):\n",
    "    for key in (\"author\",\"dc.creator\",\"og:article:author\",\"article:author\"):\n",
    "        tag = soup.find(\"meta\", attrs={\"name\": key}) or soup.find(\"meta\", attrs={\"property\": key})\n",
    "        if tag and tag.get(\"content\"):\n",
    "            return tag[\"content\"].strip()\n",
    "    # byline detik\n",
    "    by = soup.select_one(\".author, .author__name, .detail__author, .author-name\")\n",
    "    if by:\n",
    "        t = by.get_text(\" \", strip=True)\n",
    "        return re.sub(r\"^[Bb]y\\s+\", \"\", t).strip() or None\n",
    "    return None\n",
    "\n",
    "def extract_date_meta(soup):\n",
    "    for key in (\"article:published_time\",\"og:published_time\",\"pubdate\",\"date\"):\n",
    "        tag = soup.find(\"meta\", attrs={\"property\": key}) or soup.find(\"meta\", attrs={\"name\": key})\n",
    "        if tag and tag.get(\"content\"):\n",
    "            return tag[\"content\"].strip()\n",
    "    t = soup.find(\"time\")\n",
    "    if t:\n",
    "        return (t.get(\"datetime\") or t.get_text(\" \", strip=True) or \"\").strip() or None\n",
    "    cand = soup.select_one(\".date, .detail__date, .date-time, .author__date, .media__date\")\n",
    "    if cand:\n",
    "        return cand.get_text(\" \", strip=True)\n",
    "    return None\n",
    "\n",
    "def parse_article_detail(session, url):\n",
    "    try:\n",
    "        html = fetch_html(session, url)\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        ld = parse_ld_json(soup)\n",
    "        author = extract_author_from_ld(ld) or extract_author_meta(soup) or \"\"\n",
    "        date = extract_date_from_ld(ld) or extract_date_meta(soup) or \"\"\n",
    "        author = re.sub(r\"\\s+\",\" \",author).strip()\n",
    "        date = re.sub(r\"\\s+\",\" \",date).strip()\n",
    "        return author, date\n",
    "    except Exception:\n",
    "        return \"\", \"\"\n",
    "\n",
    "# =========================\n",
    "# Deteksi URL artikel Detik\n",
    "# =========================\n",
    "def looks_like_article(url):\n",
    "    try:\n",
    "        p = urlparse(url)\n",
    "        if p.netloc.lower() not in ARTICLE_HOSTS:\n",
    "            return False\n",
    "        # Pola umum artikel Detik:\n",
    "        # - /d-<digits> (sangat umum)\n",
    "        # - /<yyyy>/<mm>/<dd>/...\n",
    "        return bool(re.search(r\"/d-\\d+\", url)) or bool(re.search(r\"/\\d{4}/\\d{2}/\\d{2}/\", url))\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# =========================\n",
    "# Parser halaman TAG Detik\n",
    "# =========================\n",
    "def parse_tag_list(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    items = []\n",
    "    seen = set()\n",
    "\n",
    "    # Selector paling stabil di Detik untuk list: link judul ada di a.media__link\n",
    "    for a in soup.select(\"a.media__link[href]\"):\n",
    "        title = a.get_text(\" \", strip=True)\n",
    "        href = a[\"href\"].strip()\n",
    "        if not title or not href:\n",
    "            continue\n",
    "        if href.startswith(\"/\"):\n",
    "            href = urljoin(\"https://www.detik.com/\", href)\n",
    "        key = (title, href)\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        if looks_like_article(href):\n",
    "            items.append({\"judul_berita\": title, \"url_berita\": href})\n",
    "\n",
    "    # Fallback: beberapa layout lama pakai .list-content__item\n",
    "    if not items:\n",
    "        for card in soup.select(\"article, .list-content__item\"):\n",
    "            a = card.find(\"a\", href=True)\n",
    "            if not a:\n",
    "                continue\n",
    "            title = (card.find([\"h2\",\"h3\"]).get_text(\" \", strip=True)\n",
    "                     if card.find([\"h2\",\"h3\"]) else a.get_text(\" \", strip=True))\n",
    "            href = a[\"href\"].strip()\n",
    "            if href.startswith(\"/\"):\n",
    "                href = urljoin(\"https://www.detik.com/\", href)\n",
    "            if title and href and looks_like_article(href):\n",
    "                key = (title, href)\n",
    "                if key not in seen:\n",
    "                    seen.add(key)\n",
    "                    items.append({\"judul_berita\": title, \"url_berita\": href})\n",
    "\n",
    "    return items\n",
    "\n",
    "# =========================\n",
    "# Main\n",
    "# =========================\n",
    "def scrape_detik_by_tag(topic, max_pages=3, delay_range=(0.8, 1.6)):\n",
    "    \"\"\"\n",
    "    Ambil artikel dari halaman TAG Detik berdasarkan 'topic'.\n",
    "    Contoh topic: \"Kementerian Keuangan\" -> /tag/kementerian-keuangan\n",
    "    \"\"\"\n",
    "    session = make_session()\n",
    "    slug = slugify_tag(topic)\n",
    "    results = []\n",
    "\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{TAG_BASE}{slug}\"\n",
    "        if page > 1:\n",
    "            url = f\"{url}?page={page}\"\n",
    "\n",
    "        try:\n",
    "            html = fetch_html(session, url)\n",
    "        except RuntimeError as e:\n",
    "            print(f\"[PERINGATAN] Melewati page={page}: {e}\")\n",
    "            continue\n",
    "\n",
    "        items = parse_tag_list(html)\n",
    "        if not items:\n",
    "            print(f\"[INFO] Tidak ada item pada tag page={page} ({url}).\")\n",
    "            # Kalau halaman benar-benar kosong, tidak usah lanjut halaman berikut\n",
    "            # tapi kita tetap lanjut iterasi untuk berjaga jika page berikut ada.\n",
    "        for it in items:\n",
    "            judul = it[\"judul_berita\"]\n",
    "            href = it[\"url_berita\"]\n",
    "            penulis, tanggal = parse_article_detail(session, href)\n",
    "            results.append({\n",
    "                \"judul_berita\": judul,\n",
    "                \"tanggal_berita\": tanggal,\n",
    "                \"penulis_berita\": penulis,\n",
    "                \"url_berita\": href,\n",
    "            })\n",
    "            time.sleep(random.uniform(*delay_range))\n",
    "        time.sleep(random.uniform(*delay_range))\n",
    "\n",
    "    df = pd.DataFrame(results, columns=[\"judul_berita\",\"tanggal_berita\",\"penulis_berita\",\"url_berita\"])\n",
    "    if not df.empty:\n",
    "        df = df.drop_duplicates(subset=[\"url_berita\"]).reset_index(drop=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vYEPSzM0kPDD"
   },
   "outputs": [],
   "source": [
    "# Baca parameter search\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"config.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Kata kunci topik untuk analisis relevansi judul\n",
    "topic_keywords = config[\"keywords\"]\n",
    "\n",
    "# Daftar tanggal (YYYY-MM-DD). Akan di-convert ke DD-MM-YYYY untuk pencocokan di halaman.\n",
    "dates = config[\"search_date\"]\n",
    "\n",
    "# Maksimum halaman per tanggal (akan berhenti lebih awal jika halaman kosong)\n",
    "max_pages_per_date = config[\"max_page_length\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 258733,
     "status": "ok",
     "timestamp": 1758171970445,
     "user": {
      "displayName": "Monitoring Berita",
      "userId": "16755502473357078001"
     },
     "user_tz": -420
    },
    "id": "0rp_Om0fvXKz",
    "outputId": "e9dc0c36-347a-4a1a-a190-405c4978497c"
   },
   "outputs": [],
   "source": [
    "# ===== Contoh pakai =====\n",
    "if __name__ == \"__main__\":\n",
    "  df_list = []\n",
    "  for topic in topic_keywords:\n",
    "      df = scrape_detik_by_tag(topic, max_pages=1)\n",
    "      df_list.append(df)\n",
    "  df = pd.concat(df_list, ignore_index=True)\n",
    "  df.to_excel(cwd + f\"/daftar_berita/detik_.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMQDMUA72K8ePLbSbVBnTCe",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
