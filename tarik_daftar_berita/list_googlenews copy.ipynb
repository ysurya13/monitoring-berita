{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c4cc9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set folder tempat kerja (current working directory)\n",
    "import os\n",
    "cwd = '/Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Pekerjaan BMN/05. 2025/98_monitoring_berita/monitoring-berita'\n",
    "# cwd = '/content/drive/MyDrive/Monitoring Berita'\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e522ef7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mengecek dependencies...\n",
      "‚úì googlenewsdecoder sudah terinstall\n",
      "‚úì tqdm sudah terinstall\n",
      "\n",
      "üéâ Semua dependencies siap!\n"
     ]
    }
   ],
   "source": [
    "# Check and install required libraries only if not already installed\n",
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def check_and_install_package(package_name, import_name=None):\n",
    "    \"\"\"Check if package is installed, if not install it\"\"\"\n",
    "    if import_name is None:\n",
    "        import_name = package_name\n",
    "    \n",
    "    try:\n",
    "        importlib.import_module(import_name)\n",
    "        print(f\"‚úì {package_name} sudah terinstall\")\n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(f\"‚ö† {package_name} belum terinstall, menginstall...\")\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
    "            print(f\"‚úì {package_name} berhasil diinstall\")\n",
    "            return True\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"‚úó Gagal menginstall {package_name}\")\n",
    "            return False\n",
    "\n",
    "# Check required packages\n",
    "packages_to_check = [\n",
    "    (\"googlenewsdecoder\", \"googlenewsdecoder\"),\n",
    "    (\"tqdm\", \"tqdm\")\n",
    "]\n",
    "\n",
    "print(\"Mengecek dependencies...\")\n",
    "all_installed = True\n",
    "for package, import_name in packages_to_check:\n",
    "    if not check_and_install_package(package, import_name):\n",
    "        all_installed = False\n",
    "\n",
    "if all_installed:\n",
    "    print(\"\\nüéâ Semua dependencies siap!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Ada masalah dengan instalasi dependencies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb78487d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Google News: 2 queries, max 0 articles per query\n",
      "Date filter: 2025-09-28 to 2025-10-02 (5 days)\n",
      "Using individual date scraping: 2 √ó 5 = 10 tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:17<00:00,  1.79s/task, Articles=800]\n",
      "Scraping: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:17<00:00,  1.79s/task, Articles=800]\n",
      "Removing duplicates: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 221.55step/s, Complex]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 197 duplicates: 800 ‚Üí 603 articles\n",
      "Completed in 17.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding URLs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 603/603 [01:21<00:00,  7.43url/s, Success=603, Failed=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed! File saved: /Users/yusufpradana/Library/CloudStorage/OneDrive-Personal/Pekerjaan BMN/05. 2025/98_monitoring_berita/monitoring-berita/daftar_berita/google_news_rss.xlsx\n",
      "Total articles: 603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Tarik daftar berita via Google News RSS.\n",
    "\n",
    "Menghasilkan DataFrame dengan kolom sama seperti scraper lain:\n",
    "    query, judul_berita, tanggal_berita, penulis_berita, url_berita\n",
    "\n",
    "Sumber: Google News RSS (hl=id, gl=ID)\n",
    "Catatan:\n",
    " - Google News tidak selalu menyediakan penulis, hanya sumber (media). Itu kita mapping ke penulis_berita.\n",
    " - Tanggal di <pubDate> adalah GMT. Kita konversi ke zona Asia/Jakarta dan format \"%Y-%m-%d %H:%M:%S\".\n",
    " - Kembali ditambahkan filter tanggal: hanya tanggal (YYYY-MM-DD) yang ada di config['search_date'] yang diikutkan jika daftar itu tidak kosong.\n",
    " - Pembatas jumlah item per query diterapkan SETELAH filter tanggal (agar slot diisi item relevan tanggal target).\n",
    "\n",
    "Pemakaian:\n",
    "    python list_berita_google_news_rss.py  # hasil akan tersimpan ke daftar_berita/google_news_rss.xlsx\n",
    "\n",
    "Opsi lingkungan (opsional melalui variabel environment):\n",
    "    GNEWS_TIME_WINDOW_DAYS  (default 7)  -> batas pencarian relatif (when:7d) agar cakupan feed cukup.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from typing import List, Dict, Optional\n",
    "import datetime as dt\n",
    "import zoneinfo\n",
    "import re\n",
    "import html\n",
    "import urllib.parse as urlparse\n",
    "import xml.etree.ElementTree as ET\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "from googlenewsdecoder import gnewsdecoder\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Parameter umum\n",
    "# --------------------------------------------------\n",
    "JAKARTA_TZ = zoneinfo.ZoneInfo(\"Asia/Jakarta\")\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:125.0) Gecko/20100101 Firefox/125.0\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36\",\n",
    "]\n",
    "REQUEST_DELAY_RANGE = (0.8, 1.6)\n",
    "RETRY_TOTAL = 3\n",
    "TIME_WINDOW_DAYS = int(os.environ.get(\"GNEWS_TIME_WINDOW_DAYS\", \"30\"))  # Increase default to 30 days\n",
    "\n",
    "# Parallelization parameters\n",
    "MAX_WORKERS_RSS = 5  # Untuk RSS fetching - jangan terlalu tinggi untuk menghindari rate limiting\n",
    "MAX_WORKERS_DECODE = 10  # Untuk URL decoding - bisa lebih tinggi karena berbeda service\n",
    "\n",
    "# Debug options\n",
    "DEBUG_RSS_URLS = os.environ.get(\"DEBUG_RSS_URLS\", \"0\") == \"1\"  # Set ke \"1\" untuk debug URL yang digunakan\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Util: HTTP fetch dengan retry sederhana\n",
    "# --------------------------------------------------\n",
    "\n",
    "def fetch_url(url: str, timeout: float = 15.0) -> str:\n",
    "    last_err: Optional[Exception] = None\n",
    "    for attempt in range(1, RETRY_TOTAL + 1):\n",
    "        try:\n",
    "            headers = {\"User-Agent\": random.choice(USER_AGENTS)}\n",
    "            r = requests.get(url, headers=headers, timeout=timeout)\n",
    "            if r.status_code >= 400:\n",
    "                raise RuntimeError(f\"Status {r.status_code}\")\n",
    "            return r.text\n",
    "        except Exception as e:  # noqa: BLE001\n",
    "            last_err = e\n",
    "            time.sleep(0.5 * attempt)\n",
    "    raise RuntimeError(f\"Gagal fetch setelah {RETRY_TOTAL} percobaan: {last_err}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Build URL Google News RSS\n",
    "# --------------------------------------------------\n",
    "\n",
    "def build_google_news_rss_url(query: str, date_filters: List[str] = None, time_window_days: int = TIME_WINDOW_DAYS) -> str:\n",
    "    \"\"\"\n",
    "    Build Google News RSS URL dengan support untuk filter tanggal absolut dan relatif\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        date_filters: List tanggal dalam format YYYY-MM-DD (jika ada, akan digunakan after/before)\n",
    "        time_window_days: Fallback untuk when:Nd jika date_filters kosong\n",
    "    \"\"\"\n",
    "    q = query.strip()\n",
    "    \n",
    "    # Jika ada date_filters, gunakan format after/before untuk rentang yang lebih presisi\n",
    "    if date_filters and len(date_filters) > 0:\n",
    "        # Sort tanggal untuk mendapatkan min dan max\n",
    "        sorted_dates = sorted(date_filters)\n",
    "        start_date = sorted_dates[0]\n",
    "        end_date = sorted_dates[-1]\n",
    "        \n",
    "        # Tambah 1 hari ke end_date untuk inclusive range (before adalah exclusive)\n",
    "        from datetime import datetime, timedelta\n",
    "        end_dt = datetime.strptime(end_date, \"%Y-%m-%d\") + timedelta(days=1)\n",
    "        end_date_exclusive = end_dt.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        # Format yang BENAR untuk Google News RSS\n",
    "        query_with_date = f\"{q} after:{start_date} before:{end_date_exclusive}\"\n",
    "    else:\n",
    "        # Fallback ke format when: jika tidak ada date_filters\n",
    "        query_with_date = f\"{q} when:{time_window_days}d\"\n",
    "    \n",
    "    # Gunakan quote_plus yang benar untuk Google News\n",
    "    encoded = urlparse.quote_plus(query_with_date)\n",
    "    base = \"https://news.google.com/rss/search\"\n",
    "    url = f\"{base}?q={encoded}&hl=id&gl=ID&ceid=ID:id\"\n",
    "    \n",
    "    return url\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Parse RSS XML ke list item\n",
    "# --------------------------------------------------\n",
    "\n",
    "def parse_rss_items(xml_text: str) -> List[Dict[str, str]]:\n",
    "    cleaned = re.sub(r\"[\\x00-\\x08\\x0b-\\x0c\\x0e-\\x1f]\", \"\", xml_text)\n",
    "    root = ET.fromstring(cleaned)\n",
    "    channel = root.find(\"channel\")\n",
    "    if channel is None:\n",
    "        return []\n",
    "    items_out: List[Dict[str, str]] = []\n",
    "    for item in channel.findall(\"item\"):\n",
    "        title_el = item.find(\"title\")\n",
    "        link_el = item.find(\"link\")\n",
    "        pub_el = item.find(\"pubDate\")\n",
    "        source_el = item.find(\"source\")\n",
    "\n",
    "        title = html.unescape(title_el.text.strip()) if title_el is not None and title_el.text else \"\"\n",
    "        link = link_el.text.strip() if link_el is not None and link_el.text else \"\"\n",
    "        pub_raw = pub_el.text.strip() if pub_el is not None and pub_el.text else \"\"\n",
    "        source = source_el.text.strip() if source_el is not None and source_el.text else \"\"\n",
    "\n",
    "        final_url = resolve_final_article_url(link)\n",
    "\n",
    "        items_out.append({\n",
    "            \"judul_berita\": title,\n",
    "            \"url_berita\": final_url,\n",
    "            \"pub_raw\": pub_raw,\n",
    "            \"penulis_berita\": source,\n",
    "        })\n",
    "    return items_out\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Resolve final URL dari link news.google.com jika ada parameter url=...\n",
    "# --------------------------------------------------\n",
    "\n",
    "def resolve_final_article_url(link: str) -> str:\n",
    "    if not link:\n",
    "        return link\n",
    "    try:\n",
    "        if \"news.google.com\" in link and \"url=\" in link:\n",
    "            parsed = urlparse.urlparse(link)\n",
    "            qs = urlparse.parse_qs(parsed.query)\n",
    "            if \"url\" in qs and qs[\"url\"]:\n",
    "                return qs[\"url\"][0]\n",
    "    except Exception:  # noqa: BLE001\n",
    "        return link\n",
    "    return link\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Convert pubDate -> datetime lokal & format string\n",
    "# --------------------------------------------------\n",
    "RFC_PARSE_FORMATS = [\n",
    "    \"%a, %d %b %Y %H:%M:%S %Z\",\n",
    "    \"%a, %d %b %Y %H:%M:%S %z\",\n",
    "]\n",
    "\n",
    "\n",
    "def parse_pubdate(pub_raw: str) -> Optional[dt.datetime]:\n",
    "    if not pub_raw:\n",
    "        return None\n",
    "    for fmt in RFC_PARSE_FORMATS:\n",
    "        try:\n",
    "            dt_obj = dt.datetime.strptime(pub_raw, fmt)\n",
    "            if dt_obj.tzinfo is None:\n",
    "                dt_obj = dt_obj.replace(tzinfo=dt.timezone.utc)\n",
    "            return dt_obj.astimezone(JAKARTA_TZ)\n",
    "        except Exception:  # noqa: BLE001\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Ambil berita untuk satu query (filter tanggal + batasi max item)\n",
    "# --------------------------------------------------\n",
    "\n",
    "def scrape_google_news_query(query: str, max_items: int, date_filters: List[str], delay_range=REQUEST_DELAY_RANGE, pbar=None) -> pd.DataFrame:\n",
    "    # Gunakan date_filters dalam URL building untuk hasil yang lebih akurat\n",
    "    url = build_google_news_rss_url(query, date_filters)\n",
    "    if pbar:\n",
    "        pbar.set_description(f\"Fetching: {query}\")\n",
    "    \n",
    "    try:\n",
    "        xml_text = fetch_url(url)\n",
    "    except Exception as e:  # noqa: BLE001\n",
    "        return pd.DataFrame(columns=[\"query\", \"judul_berita\", \"tanggal_berita\", \"penulis_berita\", \"url_berita\"])\n",
    "\n",
    "    raw_items = parse_rss_items(xml_text)\n",
    "\n",
    "    # Transform semua items dulu, kemudian filter\n",
    "    all_items: List[Dict[str, str]] = []\n",
    "    \n",
    "    for it in raw_items:\n",
    "        pub_dt = parse_pubdate(it.get(\"pub_raw\", \"\"))\n",
    "        tanggal_fmt = pub_dt.strftime(\"%Y-%m-%d %H:%M:%S\") if pub_dt else \"\"\n",
    "        date_only = tanggal_fmt[:10] if tanggal_fmt else None\n",
    "        \n",
    "        item_data = {\n",
    "            \"query\": query,\n",
    "            \"judul_berita\": it.get(\"judul_berita\", \"\"),\n",
    "            \"tanggal_berita\": tanggal_fmt,\n",
    "            \"penulis_berita\": it.get(\"penulis_berita\", \"\"),\n",
    "            \"url_berita\": it.get(\"url_berita\", \"\"),\n",
    "            \"date_only\": date_only\n",
    "        }\n",
    "        all_items.append(item_data)\n",
    "    \n",
    "    # PERBAIKAN: Tidak perlu filter tanggal ketat karena Google News RSS URL sudah handle filtering\n",
    "    # URL after:before: sudah membatasi rentang tanggal yang benar\n",
    "    # Filter tanggal ketat malah membuang artikel yang valid dari RSS\n",
    "    filtered_items = all_items\n",
    "    \n",
    "    # Hapus kolom bantuan dan batasi jumlah\n",
    "    out_items = []\n",
    "    for item in filtered_items:\n",
    "        out_items.append({\n",
    "            \"query\": item[\"query\"],\n",
    "            \"judul_berita\": item[\"judul_berita\"],\n",
    "            \"tanggal_berita\": item[\"tanggal_berita\"], \n",
    "            \"penulis_berita\": item[\"penulis_berita\"],\n",
    "            \"url_berita\": item[\"url_berita\"]\n",
    "        })\n",
    "\n",
    "    # Batasi setelah filter\n",
    "    if max_items > 0 and len(out_items) > max_items:\n",
    "        out_items = out_items[:max_items]\n",
    "\n",
    "    df = pd.DataFrame(out_items, columns=[\"query\", \"judul_berita\", \"tanggal_berita\", \"penulis_berita\", \"url_berita\"])\n",
    "    # Hapus duplikat dasar per query (hanya URL yang sama)\n",
    "    if not df.empty:\n",
    "        df = df.drop_duplicates(subset=[\"url_berita\"]).reset_index(drop=True)\n",
    "    \n",
    "    time.sleep(random.uniform(*delay_range))\n",
    "    return df\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Ambil berita untuk banyak query (PARALLEL VERSION)\n",
    "# --------------------------------------------------\n",
    "\n",
    "def scrape_google_news_queries_parallel(queries: List[str], max_items: int, date_filters: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Versi paralel untuk scraping multiple queries secara bersamaan\"\"\"\n",
    "    all_df: List[pd.DataFrame] = []\n",
    "    \n",
    "    def scrape_single_query(query: str) -> pd.DataFrame:\n",
    "        \"\"\"Wrapper untuk scraping single query tanpa progress bar (untuk parallel)\"\"\"\n",
    "        return scrape_google_news_query(query, max_items=max_items, date_filters=date_filters, pbar=None)\n",
    "    \n",
    "    # Parallel execution dengan ThreadPoolExecutor\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS_RSS) as executor:\n",
    "        # Submit semua tasks\n",
    "        future_to_query = {executor.submit(scrape_single_query, query): query for query in queries}\n",
    "        \n",
    "        # Progress bar untuk parallel processing\n",
    "        with tqdm(total=len(queries), desc=\"Scraping queries\", unit=\"query\") as pbar:\n",
    "            for future in as_completed(future_to_query):\n",
    "                query = future_to_query[future]\n",
    "                try:\n",
    "                    df_q = future.result()\n",
    "                    all_df.append(df_q)\n",
    "                    pbar.set_postfix({'Articles': sum(len(df) for df in all_df)})\n",
    "                except Exception as e:\n",
    "                    # Tambahkan DataFrame kosong untuk query yang gagal\n",
    "                    all_df.append(pd.DataFrame(columns=[\"query\", \"judul_berita\", \"tanggal_berita\", \"penulis_berita\", \"url_berita\"]))\n",
    "                \n",
    "                pbar.update(1)\n",
    "    \n",
    "    if not all_df:\n",
    "        return pd.DataFrame(columns=[\"query\", \"judul_berita\", \"tanggal_berita\", \"penulis_berita\", \"url_berita\"])\n",
    "    \n",
    "    df = pd.concat(all_df, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def scrape_google_news_queries(queries: List[str], max_items: int, date_filters: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Wrapper function yang memilih antara sequential atau parallel\"\"\"\n",
    "    if len(queries) <= 2:\n",
    "        # Untuk query sedikit, gunakan sequential (overhead parallel tidak worth it)\n",
    "        all_df: List[pd.DataFrame] = []\n",
    "        \n",
    "        with tqdm(total=len(queries), desc=\"Scraping queries\", unit=\"query\") as pbar:\n",
    "            for q in queries:\n",
    "                df_q = scrape_google_news_query(q, max_items=max_items, date_filters=date_filters, pbar=pbar)\n",
    "                all_df.append(df_q)\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({'Articles': sum(len(df) for df in all_df)})\n",
    "        \n",
    "        if not all_df:\n",
    "            return pd.DataFrame(columns=[\"query\", \"judul_berita\", \"tanggal_berita\", \"penulis_berita\", \"url_berita\"])\n",
    "        df = pd.concat(all_df, ignore_index=True)\n",
    "        return df\n",
    "    else:\n",
    "        # Untuk banyak query, gunakan parallel\n",
    "        return scrape_google_news_queries_parallel(queries, max_items, date_filters)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Fungsi untuk menghapus duplikat menyeluruh\n",
    "# --------------------------------------------------\n",
    "\n",
    "def remove_duplicates_comprehensive(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Menghapus duplikat secara menyeluruh berdasarkan beberapa kriteria:\n",
    "    1. URL berita yang sama\n",
    "    2. Judul berita yang sangat mirip (untuk menangani judul dengan sedikit variasi)\n",
    "    3. Kombinasi penulis dan tanggal yang sama dengan judul mirip\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame dengan kolom [query, judul_berita, tanggal_berita, penulis_berita, url_berita]\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame yang sudah dibersihkan dari duplikat\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    \n",
    "    initial_count = len(df)\n",
    "    \n",
    "    # Progress bar untuk deduplication - lebih efisien tanpa terlalu banyak step\n",
    "    with tqdm(total=3, desc=\"Removing duplicates\", unit=\"step\") as pbar:\n",
    "        # 1. Hapus duplikat berdasarkan URL yang sama (paling cepat dan efektif)\n",
    "        pbar.set_postfix_str(\"URLs\")\n",
    "        df_cleaned = df.drop_duplicates(subset=[\"url_berita\"], keep='first').reset_index(drop=True)\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # 2. Normalisasi judul dalam satu operasi yang lebih efisien\n",
    "        pbar.set_postfix_str(\"Titles\")\n",
    "        df_cleaned = df_cleaned.copy()\n",
    "        df_cleaned['judul_normalized'] = (\n",
    "            df_cleaned['judul_berita']\n",
    "            .str.lower()\n",
    "            .str.strip()\n",
    "            .str.replace(r'[^\\w\\s]', ' ', regex=True)\n",
    "            .str.replace(r'\\s+', ' ', regex=True)\n",
    "            .str.strip()\n",
    "        )\n",
    "        df_cleaned['tanggal_only'] = df_cleaned['tanggal_berita'].str[:10]  # YYYY-MM-DD\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # 3. Hapus duplikat berdasarkan kombinasi kriteria yang lebih ketat\n",
    "        pbar.set_postfix_str(\"Complex\")\n",
    "        # Pertama hapus judul yang benar-benar sama\n",
    "        df_cleaned = df_cleaned.drop_duplicates(subset=[\"judul_normalized\"], keep='first').reset_index(drop=True)\n",
    "        \n",
    "        # Kemudian hapus kombinasi penulis + tanggal + judul mirip\n",
    "        df_cleaned = df_cleaned.drop_duplicates(subset=[\"penulis_berita\", \"tanggal_only\", \"judul_normalized\"], keep='first').reset_index(drop=True)\n",
    "        \n",
    "        # Hapus kolom bantuan\n",
    "        df_cleaned = df_cleaned.drop(columns=[\"judul_normalized\", \"tanggal_only\"])\n",
    "        pbar.update(1)\n",
    "    \n",
    "    final_count = len(df_cleaned)\n",
    "    total_removed = initial_count - final_count\n",
    "    \n",
    "    if total_removed > 0:\n",
    "        print(f\"Removed {total_removed} duplicates: {initial_count} ‚Üí {final_count} articles\")\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Main CLI (aman untuk notebook & script)\n",
    "# --------------------------------------------------\n",
    "\n",
    "def main():  # noqa: D401\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if \"__file__\" in globals():\n",
    "        base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\n",
    "    else:\n",
    "        base_dir = os.getcwd()\n",
    "\n",
    "    config_path = os.path.join(base_dir, \"config.json\")\n",
    "    if not os.path.exists(config_path):\n",
    "        raise SystemExit(f\"config.json tidak ditemukan di {base_dir}\")\n",
    "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    queries = config.get(\"keywords\", [])\n",
    "    date_filters = config.get(\"search_date\", [])  # daftar tanggal (YYYY-MM-DD)\n",
    "    # ubah untuk hasil akhir\n",
    "    max_items = 0\n",
    "    if not queries:\n",
    "        raise SystemExit(\"keywords kosong di config.json\")\n",
    "\n",
    "    print(f\"Scraping Google News: {len(queries)} queries, max {max_items} articles per query\")\n",
    "    if date_filters:\n",
    "        # Show date range instead of all individual dates\n",
    "        sorted_dates = sorted(date_filters)\n",
    "        if len(sorted_dates) == 1:\n",
    "            print(f\"Date filter: {sorted_dates[0]}\")\n",
    "        elif len(sorted_dates) <= 2:\n",
    "            print(f\"Date filter: {', '.join(sorted_dates)}\")\n",
    "        else:\n",
    "            print(f\"Date filter: {sorted_dates[0]} to {sorted_dates[-1]} ({len(sorted_dates)} days)\")\n",
    "        \n",
    "        # PERBAIKAN: Gunakan individual date scraping untuk capture semua artikel\n",
    "        print(f\"Using individual date scraping: {len(queries)} √ó {len(date_filters)} = {len(queries) * len(date_filters)} tasks\")\n",
    "        \n",
    "        all_results = []\n",
    "        with tqdm(total=len(queries) * len(date_filters), desc=\"Scraping\", unit=\"task\") as pbar:\n",
    "            for query in queries:\n",
    "                for single_date in date_filters:\n",
    "                    try:\n",
    "                        df_single = scrape_google_news_query(\n",
    "                            query=query, \n",
    "                            max_items=max_items, \n",
    "                            date_filters=[single_date],  # Single date only\n",
    "                            pbar=None\n",
    "                        )\n",
    "                        \n",
    "                        if len(df_single) > 0:\n",
    "                            all_results.append(df_single)\n",
    "                        \n",
    "                        pbar.set_postfix({\n",
    "                            'Articles': sum(len(df) for df in all_results)\n",
    "                        })\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        pass  # Continue with other dates\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "        \n",
    "        # Combine results\n",
    "        if all_results:\n",
    "            df = pd.concat(all_results, ignore_index=True)\n",
    "        else:\n",
    "            df = pd.DataFrame(columns=[\"query\", \"judul_berita\", \"tanggal_berita\", \"penulis_berita\", \"url_berita\"])\n",
    "    else:\n",
    "        # Fallback to original method if no date filters\n",
    "        df = scrape_google_news_queries(queries, max_items=max_items, date_filters=date_filters)\n",
    "    \n",
    "    # Deduplication\n",
    "    df = remove_duplicates_comprehensive(df)\n",
    "    \n",
    "    total_duration = time.time() - start_time\n",
    "    print(f\"Completed in {total_duration:.1f}s\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def decode_single_url(url_with_index):\n",
    "    \"\"\"Decode single URL - untuk parallel processing\"\"\"\n",
    "    index, source_url = url_with_index\n",
    "    interval_time = 0.1  # Kurangi interval untuk parallel processing\n",
    "    \n",
    "    try:\n",
    "        decoded_url = gnewsdecoder(source_url, interval=interval_time)\n",
    "        \n",
    "        if decoded_url.get(\"status\"):\n",
    "            return index, decoded_url[\"decoded_url\"], True\n",
    "        else:\n",
    "            return index, source_url, False  # Gunakan URL asli jika decode gagal\n",
    "                    \n",
    "    except Exception as e:\n",
    "        return index, source_url, False  # Gunakan URL asli jika ada exception\n",
    "\n",
    "def convert_link_parallel(df):\n",
    "    \"\"\"Versi paralel untuk decode URLs\"\"\"\n",
    "    daftar_berita = df['url_berita'].tolist()\n",
    "    \n",
    "    if len(daftar_berita) == 0:\n",
    "        return df\n",
    "    \n",
    "    # Buat list dengan index untuk maintain order\n",
    "    indexed_urls = list(enumerate(daftar_berita))\n",
    "    \n",
    "    # Array untuk menyimpan hasil dengan urutan yang benar\n",
    "    decoded_results = [None] * len(daftar_berita)\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    # Parallel execution dengan ThreadPoolExecutor\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS_DECODE) as executor:\n",
    "        # Submit semua tasks\n",
    "        future_to_index = {executor.submit(decode_single_url, url_data): url_data[0] \n",
    "                          for url_data in indexed_urls}\n",
    "        \n",
    "        # Progress bar untuk parallel processing\n",
    "        with tqdm(total=len(daftar_berita), desc=\"Decoding URLs\", unit=\"url\") as pbar:\n",
    "            for future in as_completed(future_to_index):\n",
    "                try:\n",
    "                    index, decoded_url, success = future.result()\n",
    "                    decoded_results[index] = decoded_url\n",
    "                    \n",
    "                    if success:\n",
    "                        success_count += 1\n",
    "                    else:\n",
    "                        error_count += 1\n",
    "                    \n",
    "                    pbar.set_postfix({'Success': success_count, 'Failed': error_count})\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    index = future_to_index[future]\n",
    "                    decoded_results[index] = daftar_berita[index]  # Gunakan URL asli\n",
    "                    error_count += 1\n",
    "                    pbar.set_postfix({'Success': success_count, 'Failed': error_count})\n",
    "                \n",
    "                pbar.update(1)\n",
    "    \n",
    "    # Update DataFrame dengan URL yang sudah di-decode\n",
    "    df['url_berita'] = decoded_results\n",
    "    \n",
    "    return df\n",
    "\n",
    "def convert_link():\n",
    "    if \"__file__\" in globals():\n",
    "        base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\n",
    "    else:\n",
    "        base_dir = os.getcwd()\n",
    "\n",
    "    df = main()\n",
    "    \n",
    "    # Gunakan parallel processing untuk decode URLs\n",
    "    if len(df) <= 10:\n",
    "        # Untuk URL sedikit, gunakan sequential\n",
    "        interval_time = 1\n",
    "        daftar_berita = df['url_berita']\n",
    "        \n",
    "        print(f\"\\nMemulai decode {len(daftar_berita)} Google News URLs...\")\n",
    "        \n",
    "        decoded = []\n",
    "        success_count = 0\n",
    "        error_count = 0\n",
    "        \n",
    "        with tqdm(total=len(daftar_berita), desc=\"Decode URLs\", unit=\"url\") as pbar:\n",
    "            for i, source_url in enumerate(daftar_berita):\n",
    "                try:\n",
    "                    decoded_url = gnewsdecoder(source_url, interval=interval_time)\n",
    "                    \n",
    "                    if decoded_url.get(\"status\"):\n",
    "                        decoded.append(decoded_url[\"decoded_url\"])\n",
    "                        success_count += 1\n",
    "                    else:\n",
    "                        decoded.append(source_url)\n",
    "                        error_count += 1\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    decoded.append(source_url)\n",
    "                    error_count += 1\n",
    "                \n",
    "                pbar.set_postfix({'‚úì': success_count, '‚úó': error_count})\n",
    "                pbar.update(1)\n",
    "        \n",
    "        df['url_berita'] = decoded\n",
    "    else:\n",
    "        # Untuk banyak URL, gunakan parallel\n",
    "        df = convert_link_parallel(df)\n",
    "    \n",
    "    # Simpan hasil\n",
    "    out_dir = os.path.join(base_dir, \"daftar_berita\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_xlsx = os.path.join(out_dir, \"google_news_rss.xlsx\")\n",
    "    df.to_excel(out_xlsx, index=False)\n",
    "    \n",
    "    print(f\"\\nCompleted! File saved: {out_xlsx}\")\n",
    "    print(f\"Total articles: {len(df)}\")\n",
    "\n",
    "\n",
    "# Jalankan hanya jika belum pernah dieksekusi\n",
    "if not hasattr(sys.modules[__name__], '_conversion_done'):\n",
    "    if __name__ == \"__main__\":\n",
    "        convert_link()\n",
    "        sys.modules[__name__]._conversion_done = True\n",
    "else:\n",
    "    print(\"Sudah dijalankan sebelumnya. Restart kernel jika ingin menjalankan ulang.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
